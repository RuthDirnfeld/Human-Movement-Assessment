{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46HEnLeeDbNt"
   },
   "source": [
    "## Sprint 8\n",
    "### Pose detection using a PoseNet model in Python.\n",
    "\n",
    "https://github.com/ViktorShLnu/pose-video\n",
    "Check 'development' branch for the latest code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLh0xb5rF8xS"
   },
   "source": [
    "## Installation\n",
    "The code for this sprint was tested with the following Python version:\n",
    "\n",
    "*   Python < 3.8 and >= 3.6\n",
    "\n",
    "\n",
    "The following libraries are required to be installed to be able to run the code:\n",
    "*   opencv-python - 3.4.5.20\n",
    "*   tensorflow - 1.15.0\n",
    "*   scipy - 1.4.1\n",
    "*   pyyaml - 3.13\n",
    "\n",
    "The code is located in the group's repository. Clone the repository to be able to run the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJ2Zw1l0F-b4"
   },
   "source": [
    "## Usage\n",
    "\n",
    "To run the code on the entire directory of videos or pictures, run the command:\n",
    "\n",
    "\n",
    "```\n",
    "python posenet_processing.py --input_dir \"./videos\"\n",
    "```\n",
    "\n",
    "If you'd like to run the code on a single video or image file, change the flag `input_dir` to `input_file` and provide the path to the file.\n",
    "\n",
    "\n",
    "The command can use other optional flags:\n",
    "* model - PoseNet model\n",
    "* video_width - the width of the target video\n",
    "* video_height - the height of the target video\n",
    "* scale_factor - scale factor for PoseNet joint location precision\n",
    "* output_dir - output directory\n",
    "\n",
    "After the code finishes executing, you may find the generated .csv file with the pose data as well as a video or image file with the predicted joint location overlayed on the original media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCJpoQUFH9Rm"
   },
   "source": [
    "## Architecture\n",
    "\n",
    "![Architecture of the code](https://i.imgur.com/p3Ue6lX.jpg)\n",
    "\n",
    "The diagram shows the flow of image and video processing. The image or video files are passed to the posenet library to be processed. After the processing is finished the output is either a processed image with drawn and joined key points and a .csv file with the stored joint coordinates, or a processed video with drawn and joined key points and a .csv file with the stored joint coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gstVOQKIXmR"
   },
   "source": [
    "## Code output\n",
    "After the code is finished with the processing of an image or a video, for each file two outputs are produced. First output is an image or a video containing the original picture together with overlayed joint locations. This is an example of processed video and image:\n",
    "\n",
    "Image: ![Processed image](https://i.postimg.cc/RhjJzj5L/frisbee.jpg)\n",
    "\n",
    "Video: https://youtu.be/0Q_YhoYRp7Y\n",
    "\n",
    "The second output is a .csv file. The columns consists of X and Y locations of all the identified joints as well as their score. On top of that, frame, pose and pose scores are included for each record. The following are the columns of the generated .csv file grouped for each joint:\n",
    "\n",
    "* Frame\n",
    "* Pose, Pose_Score\n",
    "* Nose_score, Nose_X_Coord, Nose_Y_Coord\n",
    "* LeftEye_score, LeftEye_X_Coord, LeftEye_Y_Coord\n",
    "* RightEye_score, RightEye_X_Coord, RightEye_Y_Coord\n",
    "* LeftEar_score, LeftEar_X_Coord, LeftEar_Y_Coord\n",
    "* RightEar_score, RightEar_X_Coord, RightEar_Y_Coord\n",
    "* LeftShoulder_score,LeftShoulder_X_Coord, LeftShoulder_Y_Coord\n",
    "* RightShoulder_score, RightShoulder_X_Coord, RightShoulder_Y_Coord\n",
    "* LeftElbow_score, LeftElbow_X_Coord, LeftElbow_Y_Coord\n",
    "* RightElbow_score, RightElbow_X_Coord, RightElbow_Y_Coord\n",
    "* LeftWrist_score, LeftWrist_X_Coord, LeftWrist_Y_Coord\n",
    "* RightWrist_score, RightWrist_X_Coord, RightWrist_Y_Coord\n",
    "* LeftHip_score, LeftHip_X_Coord, LeftHip_Y_Coord\n",
    "* RightHip_score, RightHip_X_Coord, RightHip_Y_Coord\n",
    "* LeftKnee_score, LeftKnee_X_Coord, LeftKnee_Y_Coord\n",
    "* RightKnee_score, RightKnee_X_Coord, RightKnee_Y_Coord\n",
    "* LeftAnkle_score, LeftAnkle_X_Coord, LeftAnkle_Y_Coord\n",
    "* RightAnkle_score, RightAnkle_X_Coord, RightAnkle_Y_Coord\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
