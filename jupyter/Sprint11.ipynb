{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVonn0cbK6-O"
   },
   "source": [
    "# Kinect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KKPSZaXJl0g"
   },
   "source": [
    "Similarly as in previous Sprints the following parameters were applied on all tests:  \n",
    "  \n",
    "single hidden layer (16 neurons)    \n",
    "batch sizes: 16, 64, 512, 1024, 2048  \n",
    "epochs: 100, 500, 1000  \n",
    "activation functions: ReLU, SELU, tanh  \n",
    "learning rate for optimizer: varies between 1e-3 and 3e-4  \n",
    "metrics for training loss function: loss='categorical_crossentropy'\n",
    "  \n",
    "Looking at the results, regardless of which architecture was selected, the Adam optimizer with 100 epochs, 64 batches and selu activation function resulted in having the best accuracy and lowest loss. The lowest loss, (0.057) resulted from an architecture of 1 hidden layer with 16 neurons. \n",
    "\n",
    "'categorical_crossentropy' was chosen since in our data we one hot encoded the Classifier labels, and the categorical_crossentropy is tuned to output the one hot vector.\n",
    "\n",
    "The following result tables are of same structure as in previous Sprints where the best result for each optimizer is highlighted in the tables below.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tntsh6pJOL1"
   },
   "source": [
    "##Adam optimizer\n",
    "Adam(lr=1e-3)\n",
    "\n",
    "| Activation Function  |\tEpochs  |  Batch Size    |\t  Test set               |\n",
    "|----------------------|------------|----------------|---------------------------|\n",
    "| relu                 | 100        | 64             | loss: 0.073  - accuracy: 0.989  - precision: 0.984 - recall: 0.984  |\n",
    "| relu                 | 100        | 512            | loss: 0.074  - accuracy: 0.989  - precision: 0.984 - recall: 0.984  | \n",
    "| relu                 | 100        | 1024           | loss: 0.076  - accuracy: 0.989  - precision: 0.984 - recall: 0.984  | \n",
    "| relu                 | 100        | 2048           | loss: 0.079  - accuracy: 0.989  - precision: 0.984 - recall: 0.984  | \n",
    "| relu                 | 500        | 16             | loss: 0.063  - accuracy: 0.989  - precision: 0.984 - recall: 0.984 |\n",
    "| relu                 | 500        | 64             | loss: 0.070  - accuracy: 0.989  - precision: 0.984 - recall: 0.984 | \n",
    "| relu                 | 500        | 512            | loss: 0.071  - accuracy: 0.989  - precision: 0.984 - recall: 0.984 | \n",
    "| relu                 | 500        | 1024           | loss: 0.082  - accuracy: 0.989  - precision: 0.984 - recall: 0.984 | \n",
    "| relu                 | 500        | 2048           | loss: 0.075  - accuracy: 0.989  - precision: 0.984 - recall: 0.984 |\n",
    "| relu                 | 1000       | 16             | loss: 0.064  - accuracy: 0.991  - precision: 0.986 - recall: 0.986 |\n",
    "| relu                 | 1000       | 64             | loss: 0.062  - accuracy: 0.991  - precision: 0.986 - recall: 0.986 |\n",
    "| relu                 | 1000       | 512            | loss: 0.074  - accuracy: 0.989  - precision: 0.984 - recall: 0.984 | \n",
    "| relu                 | 1000       | 1024           | loss: 0.075  - accuracy: 0.989  - precision: 0.984 - recall: 0.984 | \n",
    "| relu                 | 1000       | 2048           | loss: 0.076  - accuracy: 0.989  - precision: 0.984 - recall: 0.984 |\n",
    "| tanh                 | 100        | 16             | loss: 0.064 - accuracy: 0.991 - precision: 0.986 - recall: 0.986 |\n",
    "| tanh                 | 100        | 64             | loss: 0.063 - accuracy: 0.991 - precision: 0.986 - recall: 0.986 | \n",
    "| tanh                 | 100        | 512            | loss: 0.063 - accuracy: 0.991 - precision: 0.986 - recall: 0.986 | \n",
    "| tanh                 | 100        | 1024           | loss: 0.065 - accuracy: 0.991 - precision: 0.986 - recall: 0.986 | \n",
    "| tanh                 | 100        | 2048           | loss: 0.066 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 500        | 16             | loss: 0.062 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| tanh                 | 500        | 64             | loss: 0.060 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| tanh                 | 500        | 512            | loss: 0.063 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 500        | 1024           | loss: 0.068 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 500        | 2048           | loss: 0.064 - accuracy: 0.991 - precision: 0.986 - recall: 0.986 | \n",
    "| tanh                 | 1000        | 16             | loss: 0.060 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| tanh                 | 1000        | 64             | loss: 0.062 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| tanh                 | 1000        | 512            | loss: 0.063 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 1000        | 1024           | loss: 0.065 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 1000        | 2048           | loss: 0.065 - accuracy: 0.991 - precision: 0.986 - recall: 0.986 | \n",
    "| selu                 | 100        | 16             | loss: 0.061 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| selu                 | 100        | 32             | loss: 0.060 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| **selu**                 | 100        | 64             | **loss: 0.057 - accuracy: 0.992 - precision: 0.988 - recall: 0.988**  |\n",
    "| selu                 | 100        | 512            | loss: 0.062 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 100        | 1024           | loss: 0.064 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 100        | 2048           | loss: 0.068 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 500        | 16             | loss: 0.062 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 500        | 64             | loss: 0.060 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 500        | 512            | loss: 0.066 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 500        | 1024           | loss: 0.065 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 500        | 2048           | loss: 0.066 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 1000        | 16             | loss: 0.061 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| selu                 | 1000        | 64             | loss: 0.060 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| selu                 | 1000        | 512            | loss: 0.064 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 1000        | 1024           | loss: 0.064 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOa2-r0IJO1X"
   },
   "source": [
    "##Stochastic gradient descent optimizer \n",
    "\n",
    "SGD(lr=3e-4, decay=1e-7, momentum=0.9, nesterov=True)\n",
    "\n",
    "| Activation Function  |\tEpochs  |  Batch Size    |\t  Test set               |\n",
    "|----------------------|------------|----------------|---------------------------|\n",
    "| relu                 | 100        | 16            | loss: 0.065 - accuracy: 0.991 - precision: 0.986 - recall: 0.986 | \n",
    "| relu                 | 100        | 64            | loss: 0.065 - accuracy: 0.991 - precision: 0.986 - recall: 0.986 | \n",
    "| relu                 | 100        | 512            | loss: 0.094 - accuracy: 0.989  - precision: 0.984 - recall: 0.984 | \n",
    "| relu                 | 100        | 1024           | loss: 0.101 - accuracy: 0.989  - precision: 0.984 - recall: 0.984 | \n",
    "| relu                 | 100        | 2048           | loss: 0.163 - accuracy: 0.989  - precision: 0.984 - recall: 0.984 | \n",
    "| **relu**                 | 500        | 16            | **loss: 0.062 - accuracy: 0.991 - precision: 0.986 - recall: 0.986** | \n",
    "| relu                 | 500        | 64            | loss: 0.064 - accuracy: 0.991 - precision: 0.986 - recall: 0.986 | \n",
    "| relu                 | 500        | 256            | loss: 0.078 - accuracy: 0.989  - precision: 0.984 - recall: 0.984  |\n",
    "| relu                 | 500        | 512            | loss: 0.084 - accuracy: 0.989  - precision: 0.984 - recall: 0.984  | \n",
    "| relu                 | 500        | 1024           | loss: 0.082 - accuracy: 0.989  - precision: 0.984 - recall: 0.984  | \n",
    "| relu                 | 500        | 2048           | loss: 0.098 - accuracy: 0.989  - precision: 0.984 - recall: 0.984  |\n",
    "| relu                 | 1000       | 16            | loss: 0.064 - accuracy: 0.991 - precision: 0.986 - recall: 0.986 | \n",
    "| relu                 | 1000       | 64            | loss: 0.065 - accuracy: 0.991 - precision: 0.986 - recall: 0.986 | \n",
    "| relu                 | 1000       | 256            | loss: 0.078 - accuracy: 0.989  - precision: 0.984 - recall: 0.984  |\n",
    "| relu                 | 1000       | 512            | loss: 0.078 - accuracy: 0.989  - precision: 0.984 - recall: 0.984  | \n",
    "| relu                 | 1000       | 1024           | loss: 0.097 - accuracy: 0.989  - precision: 0.984 - recall: 0.984  | \n",
    "| relu                 | 1000       | 2048           | loss: 0.103 - accuracy: 0.989  - precision: 0.984 - recall: 0.984  |\n",
    "| tanh                 | 100        | 16            | loss: 0.063  - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 100        | 64            | loss: 0.066 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 100        | 512           | loss: 0.074 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 500        | 16            | loss: 0.064 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 500        | 64            | loss: 0.065 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 500        | 512           | loss: 0.071 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 1000        | 16            | loss: 0.063 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 1000        | 64            | loss: 0.066 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 1000        | 512           | loss: 0.073 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 100        | 16            | loss: 0.064 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 100        | 64            | loss: 0.065 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 100        | 512           | loss: 0.072 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 500        | 16            | loss: 0.065 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 500        | 64            | loss: 0.064 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 500        | 512           | loss: 0.069 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 1000        | 16            | loss: 0.064 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 1000        | 64            | loss: 0.064 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 1000        | 512           | loss: 0.068 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmLawJbPJPEE"
   },
   "source": [
    "##Nesterov Adam optimizer \n",
    "\n",
    "Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "| Activation Function  |\tEpochs  |  Batch Size    |\t  Test set               |\n",
    "|----------------------|------------|----------------|---------------------------|\n",
    "| relu                 | 100        | 16             | loss: 0.063 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| relu                 | 100        | 64             | loss: 0.076 - accuracy: 0.989  - precision: 0.984 - recall: 0.984  | \n",
    "| relu                 | 100        | 512            | loss: 0.074 - accuracy: 0.989  - precision: 0.984 - recall: 0.984  | \n",
    "| relu                 | 100        | 1024           | loss: 0.075 - accuracy: 0.989  - precision: 0.984 - recall: 0.984  | \n",
    "| relu                 | 100        | 2048           | loss: 0.080 - accuracy: 0.989  - precision: 0.984 - recall: 0.984  | \n",
    "| relu                 | 500        | 16             | loss: 0.062 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| relu                 | 500        | 64             | loss: 0.060 - accuracy: 0.991  - precision: 0.986 - recall: 0.986  |\n",
    "| relu                 | 500        | 512            | loss: 0.060 - accuracy: 0.991  - precision: 0.986 - recall: 0.986  | \n",
    "| relu                 | 500        | 1024           | loss: 0.064 - accuracy: 0.991  - precision: 0.986 - recall: 0.986  | \n",
    "| relu                 | 500        | 2048           | loss: 0.064 - accuracy: 0.991  - precision: 0.986 - recall: 0.986  |\n",
    "| relu                 | 1000        | 16             | loss: 0.062 - accuracy: 0.991  - precision: 0.986 - recall: 0.986  |\n",
    "| relu                 | 1000        | 64             | loss: 0.061 - accuracy: 0.991  - precision: 0.986 - recall: 0.986  |\n",
    "| relu                 | 1000        | 512            | loss: 0.065 - accuracy: 0.991  - precision: 0.986 - recall: 0.986  | \n",
    "| relu                 | 1000        | 1024           | loss: 0.066 - accuracy: 0.991  - precision: 0.986 - recall: 0.986  | \n",
    "| relu                 | 1000        | 2048           | loss: 0.068 - accuracy: 0.991  - precision: 0.986 - recall: 0.986  |\n",
    "| tanh                 | 100        | 16            | loss: 0.065  - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 100        | 64            | loss: 0.061 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 100        | 512           | loss: 0.063 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 500        | 16            | loss: 0.063 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 500        | 64            | loss: 0.060 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 500        | 512           | loss: 0.065 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| tanh                 | 1000        | 16            | loss: 0.061 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| **tanh**                 | 1000        | 64            | **loss: 0.059 - accuracy: 0.991 - precision: 0.986 - recall: 0.986**  | \n",
    "| tanh                 | 1000        | 512           | loss: 0.067 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 100        | 16            | loss: 0.062 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 100        | 64            | loss: 0.061 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 100        | 512           | loss: 0.064 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 500        | 16            | loss: 0.062 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 500        | 64            | loss: 0.061 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 500        | 512           | loss: 0.065 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 1000        | 16            | loss: 0.061 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 1000        | 64            | loss: 0.061 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | \n",
    "| selu                 | 1000        | 512           | loss: 0.064 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pi-LCPQzTAEs"
   },
   "source": [
    "## Best results with more Neurons or multiple hidden layers\n",
    "\n",
    "In the table below the following tests were performed:  \n",
    "The two architectures with the two lowest losses were taken, thus, Adam optimizer, selu activation function, 100 epochs and a batch size 64 was taken and a higher neuron number or two hidden layers were tested if it would improve the result. The same was done with the other architecture, Nadam optimizer, tanh activation funcion, 1000 epochs and a batch size 64. In both cases it can be seen that the neuron number and multiple layers don't improve the result.\n",
    "\n",
    "| Optimizer | Activation Function  |\tEpochs  |  Batch Size    | Hidden layer neurons (in sequence) |\t  Test set               |\n",
    "|-----------|----------------------|------------|----------------|------------------------------------|---------------------------|\n",
    "| Adam     | selu                 | 100        | 64             |   64                               |loss: 0.062 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| Adam     | selu                 | 100        | 64             |   128                             |loss: 0.061 - accuracy: 0.991  - precision: 0.986 - recall: 0.986  | \n",
    "| Adam     | selu                 | 100        | 64             |   16, 16                               |loss: 0.061 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| Adam     | selu                 | 100        | 64             |   64, 64                               |loss: 0.060 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| Adam     | selu                 | 100        | 64             |   64, 128                               |loss: 0.063 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| Nadam     | tanh                 | 1000        | 64             |   64                               |loss: 0.061 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| Nadam     | tanh                 | 1000        | 64             |   128                             |loss: 0.063 - accuracy: 0.991  - precision: 0.986 - recall: 0.986  | \n",
    "| Nadam     | tanh                 | 1000        | 64             |   16, 16                               |loss: 0.061 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| Nadam     | tanh                 | 1000        | 64             |   64, 64                               |loss: 0.061 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n",
    "| Nadam     | tanh                 | 1000        | 64             |   64, 128                               |loss: 0.063 - accuracy: 0.991 - precision: 0.986 - recall: 0.986  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeuNxaNsNTRZ"
   },
   "source": [
    "#Posenet Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_JT5HgRNV6T"
   },
   "source": [
    "With the posenet data we had used the following parameters for all of the tests: \n",
    "\n",
    "neurons: 16 (1 layer)  \n",
    "batch sizes: 16, 32, 64, 128, 512, 1024, 2048  \n",
    "epochs: 100, 500, 1000  \n",
    "activation functions: ReLU, SELU, tanh  \n",
    "learning rate for optimizer: varies between 1e-3 and 3e-4  \n",
    "\n",
    "\n",
    "Looking at the results, the Adam optimizer with 500 epochs, 16 batches and relu activation function resulted in having lowest loss and highest accuracy. 0.06890, 0.99069 respectively \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQLfhPqVN38O"
   },
   "source": [
    "##Adam Optimizer\n",
    "Adam(lr=1e-3)\n",
    "\n",
    "| Activation Function  |\tEpochs  |  Batch Size    |\t  Test set               |\n",
    "|----------------------|------------|----------------|---------------------------|\n",
    "| relu       | 100        | 16     |loss: 0.06975698393659698  - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964728  |\n",
    "| **relu**       | **500**        | **16**     |loss: **0.06890640106275771**  - accuracy: **0.9906909** - precision: **0.9860358** - recall: **0.9860358** - auc: **0.9966222**  |\n",
    "| relu       | 1000        | 16     |loss: 0.06970801546419658 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9966733  |\n",
    "| relu       | 100        | 32     |loss: 0.0702720716216754 -  accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99660724  |\n",
    "| relu       | 500        | 32     |loss: 0.06986380416609536 -  accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99654406  |\n",
    "| relu       | 1000        | 32     |loss: 0.07047145659660686 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964394 |\n",
    "| relu       | 100        | 64     |loss: 0.07112582822350871 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99632925  |\n",
    "| relu       | 500        | 64     |loss: 0.07080243368256137 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963492  |\n",
    "| relu       | 1000        | 64     |loss: 0.06897399569238509 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99653554  |\n",
    "| relu       | 100        | 128     |loss: 0.0702009958779131 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964928 |\n",
    "| relu       | 500        | 128     |loss: 0.07094893652034233 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9961885  |\n",
    "| relu       | 1000        | 128     |loss: 0.0695408051550115 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99643314  |\n",
    "| relu       | 100        | 256     |loss: 0.07118140431233573 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9961325  |\n",
    "| relu       | 500        | 256     |loss: 0.0698403393839637 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.996481  |\n",
    "| relu       | 1000        | 256     |loss: 0.07031176740201923 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964421 |\n",
    "| relu       | 100        | 512     |loss: 0.07082077225405457 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99626267  |\n",
    "| relu       | 500        | 512     |loss: 0.0719890115458849 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9960338  |\n",
    "| relu       | 1000        | 512     |loss: 0.07054032644986014 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.996221  |\n",
    "| relu       | 100        | 1024     |loss: 0.07296657629960177 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99566776  |\n",
    "| relu       | 500        | 1024     |loss: 0.24278499488534408 - accuracy: 0.9905516 - precision: 0.98603004 - recall: 0.985619 - auc: 0.9914136  |\n",
    "| relu       | 1000        | 1024     |loss: 0.07155526075985791 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99602056  |\n",
    "| relu       | 100        | 2048     |loss: 0.3609679417840736 - accuracy: 0.98464644 - precision: 0.98598427 - recall: 0.9676949 - auc: 0.9920751  |\n",
    "| relu       | 500        | 2048     |loss: 0.3633546093793251 - accuracy: 0.9845769 - precision: 0.9859813 - recall: 0.96748644 - auc: 0.9921268  |\n",
    "| relu       | 1000        | 2048     |loss: 0.36243273566891226 - accuracy: 0.9843685 - precision: 0.98597234 - recall: 0.9668612 - auc: 0.99207926  |\n",
    "| tanh       | 100        | 16     |loss: 0.07133566724270793 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9965749  |\n",
    "| tanh       | 500        | 16     |loss: 0.0710177682941875 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964454  |\n",
    "| tanh       | 1000        | 16     |loss: 0.07150271645249824 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99654293  |\n",
    "| tanh       | 100        | 32     |loss: 0.07150837403565657 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99628806  |\n",
    "| tanh       | 500        | 32     |loss: 0.0702630467014096 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9965591  |\n",
    "| tanh       | 1000        | 32     |loss: 0.07136905528744741 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964809  |\n",
    "| tanh       | 100        | 64     |loss: 0.07091160826058922 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964644  |\n",
    "| tanh       | 500        | 64     |loss: 0.07193385505611671 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99618345  |\n",
    "| tanh       | 1000        | 64     |loss: 0.07045593704964628 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9965884  |\n",
    "| tanh       | 100        | 128     |loss: 0.07051285290752862 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99654996  |\n",
    "| tanh       | 500        | 128     |loss: 0.07223335883054896 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9962426  |\n",
    "| tanh       | 1000        | 128     |loss: 0.07130959903314343 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964542  |\n",
    "| tanh       | 100        | 256     |loss: 0.07126390198560097  - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963513  |\n",
    "| tanh       | 500        | 256     |loss: 0.07239582850789965 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99621934  |\n",
    "| tanh       | 1000        | 256     |loss: 0.0709493971084644 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963656  |\n",
    "| tanh       | 100        | 512     |loss: 0.07202180485866526 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9962348  |\n",
    "| tanh       | 500        | 512     |loss: 0.07163250436381331 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99638945  |\n",
    "| tanh       | 1000        | 512     |loss: 0.07083735497930041 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964897  |\n",
    "| tanh       | 100        | 1024     |loss: 0.07205541200839564 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99631757  |\n",
    "| tanh       | 500        | 1024     |loss: 0.07115409905871235 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99631864  |\n",
    "| tanh       | 1000        | 1024     |loss: 0.07118670772504886 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9962969  |\n",
    "| tanh       | 100        | 2048     |loss: 0.07179537376062033 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964979  |\n",
    "| tanh       | 500        | 2048     |loss: 0.07178820126774213 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.996251  |\n",
    "| tanh       | 1000        | 2048     |loss: 0.07188690132016985 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99648964  |\n",
    "| selu       | 100        | 16     |loss: 0.07369579715019914 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963644  |\n",
    "| selu       | 500        | 16     |loss: 0.07167896571429 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964264  |\n",
    "| selu       | 1000        | 16     |loss: 0.07070836155873729 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964772 |\n",
    "| selu       | 100        | 32     |loss: 0.07169489467476944 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9962395  |\n",
    "| selu       | 500        | 32     |loss: 0.0705661845112801 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.996539  |\n",
    "| selu       | 1000        | 32     |loss: 0.07143723723999208 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99630135  |\n",
    "| selu       | 100        | 64     |loss: 0.07151524502692395 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99615425  |\n",
    "| selu       | 500        | 64     |loss: 0.07158268796200054 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99621665  |\n",
    "| selu       | 1000        | 64     |loss: 0.07233492289671753 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9959615  |\n",
    "| selu       | 100        | 128     |loss: 0.07029049972436387 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9965442  |\n",
    "| selu       | 500        | 128     |loss: 0.07033100063326757 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99658406  |\n",
    "| selu       | 1000        | 128     |loss: 0.07158817330366773 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99626195  |\n",
    "| selu       | 100        | 256     |loss: 0.07219806470042121 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99616295  |\n",
    "| selu       | 500        | 256     |loss: 0.07111933840444158 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99650437  |\n",
    "| selu       | 1000        | 256     |loss: 0.07209559830341204 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9961074  |\n",
    "| selu       | 100        | 512     |loss: 0.07219582524037252 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9959591  |\n",
    "| selu       | 500        | 512     |loss: 0.0715693308940377 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9962348  |\n",
    "| selu       | 1000        | 512     |loss: 0.07107484541917254 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963598  |\n",
    "| selu       | 100        | 1024     |loss: 0.07082512708269789 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963225  |\n",
    "| selu       | 500        | 1024     |loss: 0.0713500861868902 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963529  |\n",
    "| selu       | 1000        | 1024     |loss: 0.07236526163916829 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9965269  |\n",
    "| selu       | 100        | 2048     |loss: 0.0714292965018784 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963325  |\n",
    "| selu       | 500        | 2048     |loss: 0.07311980751789128 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99655026 |\n",
    "| selu       | 1000        | 2048     |loss: 0.07208716219641259 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963572  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4-JDIM1kqD7"
   },
   "source": [
    "##Stochastic gradient descent optimizer \n",
    "\n",
    "SGD(lr=3e-4, decay=1e-7, momentum=0.9, nesterov=True)\n",
    "\n",
    "| Activation Function  |\tEpochs  |  Batch Size    |\t  Test set               |\n",
    "|----------------------|------------|----------------|---------------------------|\n",
    "| relu       | 100        | 16     |loss: 0.07182161242317947 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9962744  |\n",
    "| relu       | 500        | 16     |loss: 0.07127043842669947 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964661  |\n",
    "| relu       | 1000        | 16     |loss: 0.07086763242411025 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9965439  |\n",
    "| relu       | 100        | 32     |loss: 0.07367962863406523 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9958296  |\n",
    "| relu       | 500        | 32     |loss: 0.0720076304194628 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99625045  |\n",
    "| relu       | 1000        | 32     |loss: 0.07194113939466353 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99625677  |\n",
    "| relu       | 100        | 64     |loss: 0.07662186929711505 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9946554  |\n",
    "| relu       | 500        | 64     |loss: 0.07384969384136574 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99576867|\n",
    "| relu       | 1000        | 64     |loss: 0.0733955133552599 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99583626|\n",
    "| relu       | 100        | 128     |loss: 0.38776981182126213  - accuracy: 0.9856883 - precision: 0.9858231 - recall: 0.9710296 - auc: 0.99029493|\n",
    "| relu       | 500        | 128     |loss: 0.38708477504256367  - accuracy: 0.9855495 - precision: 0.9858171 - recall: 0.97061276 - auc: 0.9903094|\n",
    "| relu       | 1000        | 128     |loss: 0.3881507859397998 - accuracy: 0.9854106 - precision: 0.9858111 - recall: 0.9701959 - auc: 0.99034524|\n",
    "| relu       | 100        | 256     |loss: 0.4646490133395042 - accuracy: 0.97262746 - precision: 0.9841689 - recall: 0.9328887 - auc: 0.99086183|\n",
    "| relu       | 500        | 256     |loss: 0.46484863191607 - accuracy: 0.97262746 - precision: 0.9841689 - recall: 0.9328887 - auc: 0.9908469|\n",
    "| relu       | 1000        | 256     |loss: 0.46512611572471146  - accuracy: 0.97255796 - precision: 0.9841654 - recall: 0.9326803 - auc: 0.990865|\n",
    "| relu       | 100        | 512     |loss: 0.46599492200864956 - accuracy: 0.9726276 - precision: 0.9841689 - recall: 0.9328887 - auc: 0.99091446|\n",
    "| relu       | 500        | 512     |loss: 0.46526334241262424  - accuracy: 0.9726971 - precision: 0.98417234 - recall: 0.9330971 - auc: 0.9908946|\n",
    "| relu       | 1000        | 512     |loss: 0.46511716756188604  - accuracy: 0.9726971 - precision: 0.98417234 - recall: 0.9330971 - auc: 0.99085677|\n",
    "| relu       | 100        | 1024     |loss: 0.46515593527753735  - accuracy: 0.97269696 - precision: 0.98417234 - recall: 0.9330971 - auc: 0.99086654|\n",
    "| relu       | 500        | 1024     |loss: 0.4651425136109599 - accuracy: 0.97269696 - precision: 0.98417234 - recall: 0.9330971 - auc: 0.99086154|\n",
    "| relu       | 1000        | 1024     |loss: 0.46549380462882217  - accuracy: 0.97262746 - precision: 0.9841689 - recall: 0.9328887 - auc: 0.99089503|\n",
    "| relu       | 100        | 2048     |loss: 0.47154042688743825  - accuracy: 0.971377 - precision: 0.98410594 - recall: 0.9291371 - auc: 0.9908669|\n",
    "| relu       | 500        | 2048     |loss: 0.47161514856458553  - accuracy: 0.9714465 - precision: 0.98410946 - recall: 0.92934555 - auc: 0.99087155|\n",
    "| relu       | 1000        | 2048     |loss: 0.47179431812670786 - accuracy: 0.9713075 - precision: 0.9841024 - recall: 0.92892873 - auc: 0.99087846|\n",
    "| tanh       | 100        | 16     |loss: 0.07275809640574947 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9960907|\n",
    "| tanh       | 500        | 16     |loss: 0.07241489345742767 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99619025|\n",
    "| tanh       | 1000        | 16     |loss: 0.07281722454218292 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9960491|\n",
    "| tanh       | 100        | 32     |loss: 0.07530274009545578 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9956674|\n",
    "| tanh       | 500        | 32     |loss: 0.07615763086883065 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99545807|\n",
    "| tanh       | 1000        | 32     |loss: 0.07586117731054011 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99557734|\n",
    "| tanh       | 100        | 64     |loss: 0.0757970268798898 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99558663|\n",
    "| tanh       | 500        | 64     |loss: 0.07437619053234205 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9958535|\n",
    "| tanh       | 1000        | 64     |loss: 0.07596468821745607 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9955305|\n",
    "| tanh       | 100        | 128     |loss: 0.07565879080756698 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9956281|\n",
    "| tanh       | 500        | 128     |loss: 0.0758157347256365 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.995572|\n",
    "| tanh       | 1000        | 128     |loss: 0.07551804811023284 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9956202|\n",
    "| tanh       | 100        | 256     |loss: 0.07832191234804681 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9951885|\n",
    "| tanh       | 500        | 256     |loss: 0.07859574468272981 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99508584|\n",
    "| tanh       | 1000        | 256     |loss: 0.07851199872198578 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9951082|\n",
    "| tanh       | 100        | 512     |loss: 0.0900287935638388 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99388397|\n",
    "| tanh       | 500        | 512     |loss: 0.08663046640150246 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9940183|\n",
    "| tanh       | 1000        | 512     |loss: 0.08684887578334347 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9940363|\n",
    "| tanh       | 100        | 1024     |loss: 0.12942515121071774 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9921981|\n",
    "| tanh       | 500        | 1024     |loss: 0.09115296221490003 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9938577|\n",
    "| tanh       | 1000        | 1024     |loss: 0.0911505195580457 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9938517|\n",
    "| tanh       | 100        | 2048     |loss: 0.22756585032803756 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99046737|\n",
    "| tanh       | 500        | 2048     |loss: 0.13592258230528767 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99181914|\n",
    "| tanh       | 1000        | 2048     |loss: 0.09150917760999862 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9938272|\n",
    "| selu       | 100        | 16     |loss: 0.07111394043744741 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99638534|\n",
    "| selu       | 500        | 16     |loss: 0.07112183425809976 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.996354|\n",
    "| selu       | 1000        | 16     |loss: 0.071925295439206 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9961482|\n",
    "| selu       | 100        | 32     |loss: 0.07121284915388201 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99630153|\n",
    "| selu       | 500        | 32     |loss: 0.07246043421902623 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99598086|\n",
    "| selu       | 1000        | 32     |loss: 0.0717982003102704 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9962057|\n",
    "| selu       | 100        | 64     |loss: 0.07234029063163772 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99600756|\n",
    "| selu       | 500        | 64     |loss: 0.07291762948831254 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99586654|\n",
    "| selu       | 1000        | 64     |loss: 0.07291360093236417 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9958182|\n",
    "| selu       | 100        | 128     |loss: 0.07369918365958533 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9957389|\n",
    "| selu       | 500        | 128     |loss: 0.07360371552243139 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9957219|\n",
    "| selu       | 1000        | 128     |loss: 0.07388248409813471 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9956728|\n",
    "| selu       | 100        | 256     |loss: 0.0768974812304591 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9950211|\n",
    "| selu       | 500        | 256     |loss: 0.07689209697941235 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99500966|\n",
    "| selu       | 1000        | 256     |loss: 0.08019662810867655 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9943528|\n",
    "| selu       | 100        | 512     |loss: 0.08575658946173446 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9938622|\n",
    "| selu       | 500        | 512     |loss: 0.08573419624639084 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9938442|\n",
    "| selu       | 1000        | 512     |loss: 0.08573187144163401 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.993818|\n",
    "| selu       | 100        | 1024     |loss: 0.09948940680095582 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99293375|\n",
    "| selu       | 500        | 1024     |loss: 0.0859058911618266 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9938106|\n",
    "| selu       | 1000        | 1024     |loss: 0.08575108390194319 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99383855|\n",
    "| selu       | 100        | 2048     |loss: 0.1571868380105411  - accuracy: 0.9906211 - precision: 0.98603296 - recall: 0.98582745 - auc: 0.991889|\n",
    "| selu       | 500        | 2048     |loss: 0.11362268022568636 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9925525|\n",
    "| selu       | 1000        | 2048     |loss: 0.11362351909459954 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99254274|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfL4pojsAE0d"
   },
   "source": [
    "##Nesterov Adam optimizer \n",
    "\n",
    "Nadam(lr=1e-3, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "| Activation Function  |\tEpochs  |  Batch Size    |\t  Test set               |\n",
    "|----------------------|------------|----------------|---------------------------|\n",
    "| relu       | 100        | 16     |loss: 0.07164896070792581 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99601 |\n",
    "| relu       | 500        | 16     |loss: 0.07122050559442458 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99608207 |\n",
    "| relu       | 1000        | 16     |loss: 0.07104109938940861 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9962269 |\n",
    "| relu       | 100        | 32     |loss: 0.07006925996664715 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99661934 |\n",
    "| relu       | 500        | 32     |loss: 0.07044738982060691 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964132 |\n",
    "| relu       | 1000        | 32     |loss: 0.06961376471549782 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99654496 |\n",
    "| relu       | 100        | 64     |loss: 0.0695644635466448 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9965575 |\n",
    "| relu       | 500        | 64     |loss: 0.07072196201440342 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9961453 |\n",
    "| relu       | 1000        | 64     |loss: 0.06999923726726244 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964903 |\n",
    "| relu       | 100        | 128     |loss: 0.07001982457243239 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99653023 |\n",
    "| relu       | 500        | 128     |loss: 0.06959591922709325 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99658066 |\n",
    "| relu       | 1000        | 128     |loss: 0.06993737817431749 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99654233 |\n",
    "| relu       | 100        | 256     |loss: 0.07155259006989405 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9960382 |\n",
    "| relu       | 500        | 256     |loss: 0.07039167586342898 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99642795 |\n",
    "| relu       | 1000        | 256     |loss: 0.0709625377027429 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9961753 |\n",
    "| relu       | 100        | 512     |loss: 0.0705625896853375 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963412 |\n",
    "| relu       | 500        | 512     |loss: 0.07053183663283949 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963374 |\n",
    "| relu       | 1000        | 512     |loss: 0.07250470382663397 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9957928 |\n",
    "| relu       | 100        | 1024     |loss: 0.07248042469200565 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9958869 |\n",
    "| relu       | 500        | 1024     |loss: 0.07018404885064368 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9965976 |\n",
    "| relu       | 1000        | 1024     |loss: 0.07173951247822696 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99601233 |\n",
    "| relu       | 100        | 2048     |loss: 0.3094682095571576  - accuracy: 0.98832846 - precision: 0.98593616 - recall: 0.97894955 - auc: 0.9913198 |\n",
    "| relu       | 500        | 2048     |loss: 0.3114879767489264  - accuracy: 0.98825896 - precision: 0.98593324 - recall: 0.97874117 - auc: 0.99147666 |\n",
    "| relu       | 1000        | 2048     |loss: 0.30658252329168445  - accuracy: 0.9885369 - precision: 0.98594505 - recall: 0.9795748 - auc: 0.9912952 |\n",
    "| tanh       | 100        | 16     |loss: 0.07042203632444641 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.996406 |\n",
    "| tanh       | 500        | 16     |loss: 0.07129776769400376 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964037 |\n",
    "| tanh       | 1000        | 16     |loss: 0.0727423545766642 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9962574 |\n",
    "| tanh       | 100        | 32     |loss: 0.07120386181572369 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99649256 |\n",
    "| tanh       | 500        | 32     |loss: 0.07066099074245642 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99647945 |\n",
    "| tanh       | 1000        | 32     |loss: 0.07122917040070677 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963501 |\n",
    "| tanh       | 100        | 64     |loss: 0.07185336297728112 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963841 |\n",
    "| tanh       | 500        | 64     |loss: 0.07117049901025302 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964058 |\n",
    "| tanh       | 1000        | 64     |loss: 0.07118274228629494 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9965002 |\n",
    "| tanh       | 100        | 128     |loss: 0.07170620602883612 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99622095 |\n",
    "| tanh       | 500        | 128     |loss: 0.07205803348229 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99647933 |\n",
    "| tanh       | 1000        | 128     |loss: 0.07093791995956879 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99661183 |\n",
    "| tanh       | 100        | 256     |loss: 0.07090089669223625 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964153 |\n",
    "| tanh       | 500        | 256     |loss: 0.07102593247867614 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99641734 |\n",
    "| tanh       | 1000        | 256     |loss: 0.07206551165394905 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9961578 |\n",
    "| tanh       | 100        | 512     |loss: 0.0716215403701624 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963225 |\n",
    "| tanh       | 500        | 512     |loss: 0.07122228521637838 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99627566 |\n",
    "| tanh       | 1000        | 512     |loss: 0.07155760209626981 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9962968 |\n",
    "| tanh       | 100        | 1024     |loss: 0.07109751216306344 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99629676 |\n",
    "| tanh       | 500        | 1024     |loss: 0.07088055590842356 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964089 |\n",
    "| tanh       | 1000        | 1024     |loss: 0.07156396332682545 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99629873 |\n",
    "| tanh       | 100        | 2048     |loss: 0.07222146528457393 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99617237 |\n",
    "| tanh       | 500        | 2048     |loss: 0.07298891821805513 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9962585 |\n",
    "| tanh       | 1000        | 2048     |loss: 0.07289537475003754 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99640846 |\n",
    "| selu       | 100        | 16     |loss: 0.07103276425229542 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99660045 |\n",
    "| selu       | 500        | 16     |loss: 0.07089148061183057 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99659115 |\n",
    "| selu       | 1000        | 16     |loss: 0.07132643791208321 - accuracy: 0.9906909 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99639004 |\n",
    "| selu       | 100        | 32     |loss: 0.07070211873930561 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.996443 |\n",
    "| selu       | 500        | 32     |loss: 0.07114471860071578 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99640965 |\n",
    "| selu       | 1000        | 32     |loss: 0.0714368270418951 - accuracy: 0.9906907 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963383 |\n",
    "| selu       | 100        | 64     |loss: 0.07111030907718377 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964836 |\n",
    "| selu       | 500        | 64     |loss: 0.07049430281905246 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99656785 |\n",
    "| selu       | 1000        | 64     |loss: 0.07119460974126222 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99650234 |\n",
    "| selu       | 100        | 128     |loss: 0.07088074040889938 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963551 |\n",
    "| selu       | 500        | 128     |loss: 0.07047252925349654 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99644876 |\n",
    "| selu       | 1000        | 128     |loss: 0.07152765122191417 - accuracy: 0.9906904 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99640584 |\n",
    "| selu       | 100        | 256     |loss: 0.07090988812992204 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963636 |\n",
    "| selu       | 500        | 256     |loss: 0.07077077851041053 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9964687 |\n",
    "| selu       | 1000        | 256     |loss: 0.07150993635004488 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99629194 |\n",
    "| selu       | 100        | 512     |loss: 0.07244093353398495 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99624455 |\n",
    "| selu       | 500        | 512     |loss: 0.0711112518939638 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963131 |\n",
    "| selu       | 1000        | 512     |loss: 0.07097744977737576 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963812 |\n",
    "| selu       | 100        | 1024     |loss: 0.07100986198317562 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963707 |\n",
    "| selu       | 500        | 1024     |loss: 0.07117354662692164 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.99626577 |\n",
    "| selu       | 1000        | 1024     |loss: 0.07107184169850482 - accuracy: 0.9906906 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9962101 |\n",
    "| selu       | 100        | 2048     |loss: 0.07162835403947643 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963836 |\n",
    "| selu       | 500        | 2048     |loss: 0.07150490047808437 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9962001 |\n",
    "| selu       | 1000        | 2048     |loss: 0.07151379683779001 - accuracy: 0.99069047 - precision: 0.9860358 - recall: 0.9860358 - auc: 0.9963153 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoF8YImqaPaZ"
   },
   "source": [
    "#Implementation\n",
    "\n",
    "It seems the the best architecture and the best parameters for the model (Kinect data) is:  \n",
    "\n",
    "Adam optimizer (Adam(lr=1e-3))  \n",
    "fitted with batch size of 64, 100 epochs  \n",
    "1 hidden layer, using 16 neurons and SELU activation function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEwTi_ZWU96B"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder \n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "96TDZzmYV4It",
    "outputId": "40bc4633-8a1a-4f7d-a5d1-17e045d105b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23987, 41)"
      ]
     },
     "execution_count": 124,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "for files in glob.glob('*.csv'):\n",
    "    d = pd.read_csv(files)\n",
    "    data = pd.concat([data,d],axis=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "nXo_X4ALV4Qw",
    "outputId": "b0af4e0d-7f89-44ea-f88b-72e395c5d201"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head_x</th>\n",
       "      <th>head_y</th>\n",
       "      <th>head_z</th>\n",
       "      <th>left_shoulder_x</th>\n",
       "      <th>left_shoulder_y</th>\n",
       "      <th>left_shoulder_z</th>\n",
       "      <th>left_elbow_x</th>\n",
       "      <th>left_elbow_y</th>\n",
       "      <th>left_elbow_z</th>\n",
       "      <th>right_shoulder_x</th>\n",
       "      <th>right_shoulder_y</th>\n",
       "      <th>right_shoulder_z</th>\n",
       "      <th>right_elbow_x</th>\n",
       "      <th>right_elbow_y</th>\n",
       "      <th>right_elbow_z</th>\n",
       "      <th>left_hand_x</th>\n",
       "      <th>left_hand_y</th>\n",
       "      <th>left_hand_z</th>\n",
       "      <th>right_hand_x</th>\n",
       "      <th>right_hand_y</th>\n",
       "      <th>right_hand_z</th>\n",
       "      <th>left_hip_x</th>\n",
       "      <th>left_hip_y</th>\n",
       "      <th>left_hip_z</th>\n",
       "      <th>right_hip_x</th>\n",
       "      <th>right_hip_y</th>\n",
       "      <th>right_hip_z</th>\n",
       "      <th>left_knee_x</th>\n",
       "      <th>left_knee_y</th>\n",
       "      <th>left_knee_z</th>\n",
       "      <th>right_knee_x</th>\n",
       "      <th>right_knee_y</th>\n",
       "      <th>right_knee_z</th>\n",
       "      <th>left_foot_x</th>\n",
       "      <th>left_foot_y</th>\n",
       "      <th>left_foot_z</th>\n",
       "      <th>right_foot_x</th>\n",
       "      <th>right_foot_y</th>\n",
       "      <th>right_foot_z</th>\n",
       "      <th>Timeline</th>\n",
       "      <th>Timeline_OneHot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.020009</td>\n",
       "      <td>0.76927</td>\n",
       "      <td>0.023969</td>\n",
       "      <td>-0.16908</td>\n",
       "      <td>0.57342</td>\n",
       "      <td>-0.018576</td>\n",
       "      <td>-0.29740</td>\n",
       "      <td>0.77810</td>\n",
       "      <td>-0.10540</td>\n",
       "      <td>0.15735</td>\n",
       "      <td>0.57379</td>\n",
       "      <td>0.015735</td>\n",
       "      <td>0.27095</td>\n",
       "      <td>0.75681</td>\n",
       "      <td>-0.053982</td>\n",
       "      <td>-0.37288</td>\n",
       "      <td>1.00270</td>\n",
       "      <td>-0.14412</td>\n",
       "      <td>0.31612</td>\n",
       "      <td>0.93853</td>\n",
       "      <td>-0.093399</td>\n",
       "      <td>-0.076151</td>\n",
       "      <td>0.058349</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>0.073160</td>\n",
       "      <td>0.060424</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>-0.10336</td>\n",
       "      <td>-0.32016</td>\n",
       "      <td>-0.048768</td>\n",
       "      <td>0.10958</td>\n",
       "      <td>-0.31008</td>\n",
       "      <td>-0.016868</td>\n",
       "      <td>-0.10207</td>\n",
       "      <td>-0.66552</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>0.10029</td>\n",
       "      <td>-0.64834</td>\n",
       "      <td>0.027033</td>\n",
       "      <td>is_start</td>\n",
       "      <td>[0 0 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.019519</td>\n",
       "      <td>0.76013</td>\n",
       "      <td>0.025003</td>\n",
       "      <td>-0.16845</td>\n",
       "      <td>0.56497</td>\n",
       "      <td>-0.016398</td>\n",
       "      <td>-0.29606</td>\n",
       "      <td>0.76894</td>\n",
       "      <td>-0.10561</td>\n",
       "      <td>0.15637</td>\n",
       "      <td>0.56500</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>0.27116</td>\n",
       "      <td>0.74990</td>\n",
       "      <td>-0.054809</td>\n",
       "      <td>-0.37098</td>\n",
       "      <td>0.99408</td>\n",
       "      <td>-0.14651</td>\n",
       "      <td>0.31525</td>\n",
       "      <td>0.93142</td>\n",
       "      <td>-0.095501</td>\n",
       "      <td>-0.076356</td>\n",
       "      <td>0.050935</td>\n",
       "      <td>0.009502</td>\n",
       "      <td>0.073918</td>\n",
       "      <td>0.053273</td>\n",
       "      <td>0.014935</td>\n",
       "      <td>-0.10436</td>\n",
       "      <td>-0.32181</td>\n",
       "      <td>-0.051818</td>\n",
       "      <td>0.11207</td>\n",
       "      <td>-0.31196</td>\n",
       "      <td>-0.020435</td>\n",
       "      <td>-0.10229</td>\n",
       "      <td>-0.66758</td>\n",
       "      <td>0.010119</td>\n",
       "      <td>0.10063</td>\n",
       "      <td>-0.65011</td>\n",
       "      <td>0.025793</td>\n",
       "      <td>None</td>\n",
       "      <td>[1 0 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.019019</td>\n",
       "      <td>0.74988</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>-0.16782</td>\n",
       "      <td>0.55588</td>\n",
       "      <td>-0.014187</td>\n",
       "      <td>-0.29467</td>\n",
       "      <td>0.75900</td>\n",
       "      <td>-0.10560</td>\n",
       "      <td>0.15529</td>\n",
       "      <td>0.55475</td>\n",
       "      <td>0.017851</td>\n",
       "      <td>0.27135</td>\n",
       "      <td>0.74106</td>\n",
       "      <td>-0.055607</td>\n",
       "      <td>-0.36899</td>\n",
       "      <td>0.98490</td>\n",
       "      <td>-0.14864</td>\n",
       "      <td>0.31416</td>\n",
       "      <td>0.92268</td>\n",
       "      <td>-0.097869</td>\n",
       "      <td>-0.076507</td>\n",
       "      <td>0.042520</td>\n",
       "      <td>0.015682</td>\n",
       "      <td>0.074680</td>\n",
       "      <td>0.044947</td>\n",
       "      <td>0.021409</td>\n",
       "      <td>-0.10543</td>\n",
       "      <td>-0.32386</td>\n",
       "      <td>-0.054754</td>\n",
       "      <td>0.11460</td>\n",
       "      <td>-0.31438</td>\n",
       "      <td>-0.024284</td>\n",
       "      <td>-0.10260</td>\n",
       "      <td>-0.66982</td>\n",
       "      <td>0.009284</td>\n",
       "      <td>0.10096</td>\n",
       "      <td>-0.65205</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>None</td>\n",
       "      <td>[1 0 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.018472</td>\n",
       "      <td>0.73970</td>\n",
       "      <td>0.026980</td>\n",
       "      <td>-0.16710</td>\n",
       "      <td>0.54389</td>\n",
       "      <td>-0.011680</td>\n",
       "      <td>-0.29287</td>\n",
       "      <td>0.74755</td>\n",
       "      <td>-0.10561</td>\n",
       "      <td>0.15416</td>\n",
       "      <td>0.54368</td>\n",
       "      <td>0.018758</td>\n",
       "      <td>0.27139</td>\n",
       "      <td>0.73108</td>\n",
       "      <td>-0.056380</td>\n",
       "      <td>-0.36694</td>\n",
       "      <td>0.97486</td>\n",
       "      <td>-0.15114</td>\n",
       "      <td>0.31293</td>\n",
       "      <td>0.91307</td>\n",
       "      <td>-0.100320</td>\n",
       "      <td>-0.076622</td>\n",
       "      <td>0.032538</td>\n",
       "      <td>0.021972</td>\n",
       "      <td>0.075539</td>\n",
       "      <td>0.035402</td>\n",
       "      <td>0.027774</td>\n",
       "      <td>-0.10664</td>\n",
       "      <td>-0.32656</td>\n",
       "      <td>-0.057932</td>\n",
       "      <td>0.11723</td>\n",
       "      <td>-0.31730</td>\n",
       "      <td>-0.028394</td>\n",
       "      <td>-0.10296</td>\n",
       "      <td>-0.67195</td>\n",
       "      <td>0.008131</td>\n",
       "      <td>0.10127</td>\n",
       "      <td>-0.65425</td>\n",
       "      <td>0.023483</td>\n",
       "      <td>None</td>\n",
       "      <td>[1 0 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.017865</td>\n",
       "      <td>0.72686</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>-0.16637</td>\n",
       "      <td>0.53079</td>\n",
       "      <td>-0.009048</td>\n",
       "      <td>-0.29109</td>\n",
       "      <td>0.73538</td>\n",
       "      <td>-0.10572</td>\n",
       "      <td>0.15303</td>\n",
       "      <td>0.53158</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>0.27144</td>\n",
       "      <td>0.71975</td>\n",
       "      <td>-0.057104</td>\n",
       "      <td>-0.36452</td>\n",
       "      <td>0.96358</td>\n",
       "      <td>-0.15350</td>\n",
       "      <td>0.31126</td>\n",
       "      <td>0.90116</td>\n",
       "      <td>-0.102830</td>\n",
       "      <td>-0.076611</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>0.028149</td>\n",
       "      <td>0.076518</td>\n",
       "      <td>0.024865</td>\n",
       "      <td>0.034112</td>\n",
       "      <td>-0.10797</td>\n",
       "      <td>-0.32987</td>\n",
       "      <td>-0.061696</td>\n",
       "      <td>0.11996</td>\n",
       "      <td>-0.32085</td>\n",
       "      <td>-0.032998</td>\n",
       "      <td>-0.10331</td>\n",
       "      <td>-0.67381</td>\n",
       "      <td>0.006866</td>\n",
       "      <td>0.10139</td>\n",
       "      <td>-0.65648</td>\n",
       "      <td>0.022521</td>\n",
       "      <td>None</td>\n",
       "      <td>[1 0 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     head_x   head_y    head_z  ...  right_foot_z  Timeline  Timeline_OneHot\n",
       "0 -0.020009  0.76927  0.023969  ...      0.027033  is_start          [0 0 1]\n",
       "1 -0.019519  0.76013  0.025003  ...      0.025793      None          [1 0 0]\n",
       "2 -0.019019  0.74988  0.026000  ...      0.024610      None          [1 0 0]\n",
       "3 -0.018472  0.73970  0.026980  ...      0.023483      None          [1 0 0]\n",
       "4 -0.017865  0.72686  0.027886  ...      0.022521      None          [1 0 0]\n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-hHrirwV4YI"
   },
   "outputs": [],
   "source": [
    "data = data.drop('Timeline', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bP00wwznWEbV"
   },
   "outputs": [],
   "source": [
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, test_df = train_test_split(data, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('Timeline_OneHot'))\n",
    "train_labels = np.array([np.fromstring(v[1:-1], dtype=int, sep=' ') for v in train_labels])\n",
    "\n",
    "bool_train_labels = train_labels != 0\n",
    "val_labels = np.array(val_df.pop('Timeline_OneHot'))\n",
    "val_labels = np.array([np.fromstring(v[1:-1], dtype=int, sep=' ') for v in val_labels])\n",
    "\n",
    "test_labels = np.array(test_df.pop('Timeline_OneHot'))\n",
    "test_labels = np.array([np.fromstring(v[1:-1], dtype=int, sep=' ') for v in test_labels])\n",
    "\n",
    "train_features = np.array(train_df)\n",
    "val_features = np.array(val_df)\n",
    "test_features = np.array(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "id": "Z9UYMQJgWEwM",
    "outputId": "4348a1b9-fab6-486c-e0a0-0ebf2b8cc37c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (15351, 3)\n",
      "Validation labels shape: (3838, 3)\n",
      "Test labels shape: (4798, 3)\n",
      "Training features shape: (15351, 39)\n",
      "Validation features shape: (3838, 39)\n",
      "Test features shape: (4798, 39)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "train_features = np.clip(train_features, -5, 5)\n",
    "val_features = np.clip(val_features, -5, 5)\n",
    "test_features = np.clip(test_features, -5, 5)\n",
    "\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-mxzeQXWE2p"
   },
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "def make_model(metrics = METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            16, activation='selu',\n",
    "            input_shape=(train_features.shape[-1],)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(3, activation='softmax',\n",
    "                              bias_initializer=output_bias),\n",
    "        ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YI8vQU3WWw7-"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "4vioSA_nW53W",
    "outputId": "5412451c-8b03-43fd-bc4f-cc6aab90e8c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 16)                640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 755\n",
      "Trainable params: 723\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "2ZAMcqsrXGIB",
    "outputId": "eadcdb08-b29c-433b-93ff-38e4248ffc22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8109027 , 0.13016947, 0.05892779],\n",
       "       [0.00334784, 0.39224637, 0.60440576],\n",
       "       [0.11021544, 0.02186085, 0.86792374],\n",
       "       [0.00510376, 0.11271313, 0.88218313],\n",
       "       [0.53529906, 0.09239782, 0.37230322],\n",
       "       [0.00183692, 0.12860669, 0.8695564 ],\n",
       "       [0.00189142, 0.44100356, 0.55710506],\n",
       "       [0.2683725 , 0.38019627, 0.35143125],\n",
       "       [0.0058562 , 0.25508308, 0.7390607 ],\n",
       "       [0.5901395 , 0.09798177, 0.31187874]], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(train_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "dCj3P4a4XGOB",
    "outputId": "f3f510c6-d6c1-48bb-b69c-2d5bbfc528a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3722\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tM72UnteXGSY"
   },
   "outputs": [],
   "source": [
    "initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\n",
    "model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gHq87fFGXbKL",
    "outputId": "751d454c-e2bf-4313-cfe0-2230afb08d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 1.0062 - tp: 12651.0000 - fp: 12276.0000 - tn: 49128.0000 - fn: 18051.0000 - accuracy: 0.6707 - precision: 0.5075 - recall: 0.4121 - auc: 0.5775 - val_loss: 0.3259 - val_tp: 3750.0000 - val_fp: 69.0000 - val_tn: 7607.0000 - val_fn: 88.0000 - val_accuracy: 0.9864 - val_precision: 0.9819 - val_recall: 0.9771 - val_auc: 0.9895\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.2487 - tp: 14220.0000 - fp: 442.0000 - tn: 30260.0000 - fn: 1131.0000 - accuracy: 0.9658 - precision: 0.9699 - recall: 0.9263 - auc: 0.9860 - val_loss: 0.1003 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9936\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.1237 - tp: 14977.0000 - fp: 271.0000 - tn: 30431.0000 - fn: 374.0000 - accuracy: 0.9860 - precision: 0.9822 - recall: 0.9756 - auc: 0.9913 - val_loss: 0.0811 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9962\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.1023 - tp: 15067.0000 - fp: 244.0000 - tn: 30458.0000 - fn: 284.0000 - accuracy: 0.9885 - precision: 0.9841 - recall: 0.9815 - auc: 0.9924 - val_loss: 0.0783 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9964\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0922 - tp: 15074.0000 - fp: 246.0000 - tn: 30456.0000 - fn: 277.0000 - accuracy: 0.9886 - precision: 0.9839 - recall: 0.9820 - auc: 0.9943 - val_loss: 0.0774 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9963\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0908 - tp: 15088.0000 - fp: 242.0000 - tn: 30460.0000 - fn: 263.0000 - accuracy: 0.9890 - precision: 0.9842 - recall: 0.9829 - auc: 0.9938 - val_loss: 0.0761 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9970\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0864 - tp: 15095.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 256.0000 - accuracy: 0.9892 - precision: 0.9843 - recall: 0.9833 - auc: 0.9946 - val_loss: 0.0759 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9970\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0841 - tp: 15098.0000 - fp: 242.0000 - tn: 30460.0000 - fn: 253.0000 - accuracy: 0.9893 - precision: 0.9842 - recall: 0.9835 - auc: 0.9952 - val_loss: 0.0762 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9966\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0856 - tp: 15103.0000 - fp: 242.0000 - tn: 30460.0000 - fn: 248.0000 - accuracy: 0.9894 - precision: 0.9842 - recall: 0.9838 - auc: 0.9945 - val_loss: 0.0739 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9971\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0824 - tp: 15106.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 245.0000 - accuracy: 0.9894 - precision: 0.9843 - recall: 0.9840 - auc: 0.9955 - val_loss: 0.0742 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9971\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0836 - tp: 15108.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 243.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9842 - auc: 0.9951 - val_loss: 0.0739 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9972\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0811 - tp: 15109.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 242.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9842 - auc: 0.9955 - val_loss: 0.0728 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9975\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0797 - tp: 15109.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 242.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9842 - auc: 0.9959 - val_loss: 0.0729 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9973\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0799 - tp: 15109.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 242.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9842 - auc: 0.9957 - val_loss: 0.0723 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9976\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0795 - tp: 15107.0000 - fp: 242.0000 - tn: 30460.0000 - fn: 244.0000 - accuracy: 0.9894 - precision: 0.9842 - recall: 0.9841 - auc: 0.9957 - val_loss: 0.0721 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9976\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0787 - tp: 15108.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 243.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9842 - auc: 0.9959 - val_loss: 0.0716 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9976\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0768 - tp: 15109.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 242.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9842 - auc: 0.9963 - val_loss: 0.0733 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9973\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0781 - tp: 15109.0000 - fp: 242.0000 - tn: 30460.0000 - fn: 242.0000 - accuracy: 0.9895 - precision: 0.9842 - recall: 0.9842 - auc: 0.9961 - val_loss: 0.0720 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9976\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0790 - tp: 15110.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 241.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9843 - auc: 0.9958 - val_loss: 0.0718 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9975\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0785 - tp: 15110.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 241.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9843 - auc: 0.9959 - val_loss: 0.0740 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9971\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0776 - tp: 15109.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 242.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9842 - auc: 0.9961 - val_loss: 0.0720 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9975\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0777 - tp: 15110.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 241.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9843 - auc: 0.9960 - val_loss: 0.0717 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9977\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0768 - tp: 15109.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 242.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9842 - auc: 0.9963 - val_loss: 0.0714 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9976\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0776 - tp: 15110.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 241.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9843 - auc: 0.9961 - val_loss: 0.0714 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9976\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0776 - tp: 15110.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 241.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9843 - auc: 0.9962 - val_loss: 0.0725 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9974\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0766 - tp: 15110.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 241.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9843 - auc: 0.9965 - val_loss: 0.0723 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9974\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0758 - tp: 15110.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 241.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9843 - auc: 0.9965 - val_loss: 0.0707 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9976\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0768 - tp: 15110.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 241.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9843 - auc: 0.9962 - val_loss: 0.0720 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9973\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0748 - tp: 15110.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 241.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9843 - auc: 0.9967 - val_loss: 0.0716 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9974\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0743 - tp: 15110.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 241.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9843 - auc: 0.9968 - val_loss: 0.0708 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9976\n",
      "Epoch 31/100\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0754 - tp: 15110.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 241.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9843 - auc: 0.9965 - val_loss: 0.0705 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9976\n",
      "Epoch 32/100\n",
      "224/240 [===========================>..] - ETA: 0s - loss: 0.0746 - tp: 14110.0000 - fp: 225.0000 - tn: 28447.0000 - fn: 226.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9842 - auc: 0.9967Restoring model weights from the end of the best epoch.\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0749 - tp: 15109.0000 - fp: 241.0000 - tn: 30461.0000 - fn: 242.0000 - accuracy: 0.9895 - precision: 0.9843 - recall: 0.9842 - auc: 0.9967 - val_loss: 0.0713 - val_tp: 3778.0000 - val_fp: 60.0000 - val_tn: 7616.0000 - val_fn: 60.0000 - val_accuracy: 0.9896 - val_precision: 0.9844 - val_recall: 0.9844 - val_auc: 0.9976\n",
      "Epoch 00032: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = make_model()\n",
    "model.load_weights(initial_weights)\n",
    "baseline_history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sF_t9_jXd5v"
   },
   "outputs": [],
   "source": [
    "train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "tmg_MwujXgMH",
    "outputId": "fab5fc3b-e717-4850-b37d-34de146656de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  0.05793583393096924\n",
      "tp :  4741.0\n",
      "fp :  57.0\n",
      "tn :  9539.0\n",
      "fn :  57.0\n",
      "accuracy :  0.9920799136161804\n",
      "precision :  0.9881200790405273\n",
      "recall :  0.9881200790405273\n",
      "auc :  0.9980856776237488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_results = model.evaluate(test_features, test_labels,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "    print(name, ': ', value)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sbosp1mNNGY"
   },
   "source": [
    "# Software pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "et4JHXbKNo73"
   },
   "source": [
    "1. Designed pipeline for `KeyFrameDetector` based on DL achitecture defined previously.\n",
    "\n",
    "Deep learning pipeline to simplify model training, testing and deployment for conducting the inferance. The pipeline consists of the following modules:\n",
    "* Data creator - Reads the data from the given folder. Processes it into separate datasets in order to train and test the model.\n",
    "* Model - Serves as a wrapper for the model architecture to simplify the model compilation, saving, serving.\n",
    "* Trainer - Defines the pipeline for the model training. Integrates Model, Data creator with the configurate hyperparameters to start the training. Includes early stopping callback to stop the traininng session if no loss decrease was made in the given N epochs. Aslo contains the Tensorboard callback for interactive visualization of optimization process and logs saving.\n",
    "* Estimator - the module to serve the model and conduct the inferance on a given set of data\n",
    "* Config - Includes multi-level fancy dictionary for project configuration."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
