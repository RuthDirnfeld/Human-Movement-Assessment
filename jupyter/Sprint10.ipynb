{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akzwNYcxmKxl"
   },
   "source": [
    "## Sprint 10 - updated version\n",
    "\n",
    "After getting a feedback, we applied suggested fixes for sprint 10 deep learning part:\n",
    "\n",
    "- Fit two separate scalers:  \n",
    "one over the entire input data and  \n",
    "one over the entire output data\n",
    "\n",
    "\n",
    "After these changes were applied, we executed experiments again. The following parameters and options were tested:\n",
    "\n",
    "* batch_sizes  - 16, 32, 64, 128\n",
    "* epochs  -100, 500, 1000\n",
    "* activations - 'relu', 'selu', 'tanh'\n",
    "* architecture (neurons in each layer) - 64; 128; 256; (64, 32); (128, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYK1O2eTZwhV"
   },
   "source": [
    "# Machine learning research section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Df_RV7nZF8rQ"
   },
   "source": [
    "## Adam optimizer\n",
    "\n",
    "The best result with automated tests and Adam optimizer was a model with 2 hidden layers, first layer having 128 neurons and second - 64, the activation function ReLU and a learning rate of 1e-4. The batch size in the best result is 128 and 1000 epochs. This resulted in the mean absolute error being 0.0397 and mean squared error being 0.0031.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFszlhmyXVXs"
   },
   "source": [
    "## Stochastic gradient descent optimizer\n",
    "\n",
    "The best result with automated tests and SGD optimizer was a model with 2 hidden layers, first layer having 128 neurons and second - 64, the activation function Tanh and a learning rate of 1e-4. The batch size in the best result is 32 and 1000 epochs. This resulted in the mean absolute error being 0.0398 and mean squared error being 0.0032.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmJqR1H8XWQi"
   },
   "source": [
    "## Nesterov Adam optimizer\n",
    "\n",
    "The best result with automated tests and Nadam optimizer was a model with 2 hidden layers, first layer having 64 neurons and second - 32, the activation function ReLu and a learning rate of 1e-4. The batch size in the best result is 128 and 100 epochs. This resulted in the mean absolute error being 0.0422 and mean squared error being 0.0035.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkDGxdUWEQnq"
   },
   "source": [
    "From the automated tests it seems the the best architecture and the best parameters for the model is:\n",
    "- Adam optimizer (*Adam(learning_rate = 1e-4)*)\n",
    "- fitted with batch size of 128, 1000 epochs\n",
    "- 2 hidden layers, 128 and 64 neurons with the ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d0JnV5K_DJV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCy4KXMLqPNP"
   },
   "outputs": [],
   "source": [
    "#constants\n",
    "num_of_inputs = 26\n",
    "num_of_outputs = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "CKFi8XZwsM_H",
    "outputId": "d38cd527-6c37-4c12-9be1-75481bddee7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23987, 96)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "for files in glob.glob('*_merged.csv'):\n",
    "  d = pd.read_csv(files)\n",
    "  data = pd.concat([data,d],axis=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "9N3iCaALolXo",
    "outputId": "28899f76-2074-41d0-9826-4ce6fc7dca4c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Frame</th>\n",
       "      <th>Pose</th>\n",
       "      <th>Pose_Score</th>\n",
       "      <th>Nose_score</th>\n",
       "      <th>Nose_X_Coord</th>\n",
       "      <th>Nose_Y_Coord</th>\n",
       "      <th>LeftEye_score</th>\n",
       "      <th>LeftEye_X_Coord</th>\n",
       "      <th>LeftEye_Y_Coord</th>\n",
       "      <th>RightEye_score</th>\n",
       "      <th>RightEye_X_Coord</th>\n",
       "      <th>RightEye_Y_Coord</th>\n",
       "      <th>LeftEar_score</th>\n",
       "      <th>LeftEar_X_Coord</th>\n",
       "      <th>LeftEar_Y_Coord</th>\n",
       "      <th>RightEar_score</th>\n",
       "      <th>RightEar_X_Coord</th>\n",
       "      <th>RightEar_Y_Coord</th>\n",
       "      <th>LeftShoulder_score</th>\n",
       "      <th>LeftShoulder_X_Coord</th>\n",
       "      <th>LeftShoulder_Y_Coord</th>\n",
       "      <th>RightShoulder_score</th>\n",
       "      <th>RightShoulder_X_Coord</th>\n",
       "      <th>RightShoulder_Y_Coord</th>\n",
       "      <th>LeftElbow_score</th>\n",
       "      <th>LeftElbow_X_Coord</th>\n",
       "      <th>LeftElbow_Y_Coord</th>\n",
       "      <th>RightElbow_score</th>\n",
       "      <th>RightElbow_X_Coord</th>\n",
       "      <th>RightElbow_Y_Coord</th>\n",
       "      <th>LeftWrist_score</th>\n",
       "      <th>LeftWrist_X_Coord</th>\n",
       "      <th>LeftWrist_Y_Coord</th>\n",
       "      <th>RightWrist_score</th>\n",
       "      <th>RightWrist_X_Coord</th>\n",
       "      <th>RightWrist_Y_Coord</th>\n",
       "      <th>LeftHip_score</th>\n",
       "      <th>LeftHip_X_Coord</th>\n",
       "      <th>...</th>\n",
       "      <th>FrameNo</th>\n",
       "      <th>head_x</th>\n",
       "      <th>head_y</th>\n",
       "      <th>head_z</th>\n",
       "      <th>left_shoulder_x</th>\n",
       "      <th>left_shoulder_y</th>\n",
       "      <th>left_shoulder_z</th>\n",
       "      <th>left_elbow_x</th>\n",
       "      <th>left_elbow_y</th>\n",
       "      <th>left_elbow_z</th>\n",
       "      <th>right_shoulder_x</th>\n",
       "      <th>right_shoulder_y</th>\n",
       "      <th>right_shoulder_z</th>\n",
       "      <th>right_elbow_x</th>\n",
       "      <th>right_elbow_y</th>\n",
       "      <th>right_elbow_z</th>\n",
       "      <th>left_hand_x</th>\n",
       "      <th>left_hand_y</th>\n",
       "      <th>left_hand_z</th>\n",
       "      <th>right_hand_x</th>\n",
       "      <th>right_hand_y</th>\n",
       "      <th>right_hand_z</th>\n",
       "      <th>left_hip_x</th>\n",
       "      <th>left_hip_y</th>\n",
       "      <th>left_hip_z</th>\n",
       "      <th>right_hip_x</th>\n",
       "      <th>right_hip_y</th>\n",
       "      <th>right_hip_z</th>\n",
       "      <th>left_knee_x</th>\n",
       "      <th>left_knee_y</th>\n",
       "      <th>left_knee_z</th>\n",
       "      <th>right_knee_x</th>\n",
       "      <th>right_knee_y</th>\n",
       "      <th>right_knee_z</th>\n",
       "      <th>left_foot_x</th>\n",
       "      <th>left_foot_y</th>\n",
       "      <th>left_foot_z</th>\n",
       "      <th>right_foot_x</th>\n",
       "      <th>right_foot_y</th>\n",
       "      <th>right_foot_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.655831</td>\n",
       "      <td>0.993477</td>\n",
       "      <td>273.007834</td>\n",
       "      <td>1009.915685</td>\n",
       "      <td>0.988301</td>\n",
       "      <td>254.240474</td>\n",
       "      <td>1024.839597</td>\n",
       "      <td>0.981556</td>\n",
       "      <td>255.661543</td>\n",
       "      <td>1000.727941</td>\n",
       "      <td>0.731087</td>\n",
       "      <td>271.439195</td>\n",
       "      <td>1041.418666</td>\n",
       "      <td>0.385952</td>\n",
       "      <td>276.141888</td>\n",
       "      <td>976.865513</td>\n",
       "      <td>0.779818</td>\n",
       "      <td>338.016955</td>\n",
       "      <td>1079.338461</td>\n",
       "      <td>0.567354</td>\n",
       "      <td>349.800080</td>\n",
       "      <td>962.226168</td>\n",
       "      <td>0.575574</td>\n",
       "      <td>240.126619</td>\n",
       "      <td>1137.927764</td>\n",
       "      <td>0.560392</td>\n",
       "      <td>243.300596</td>\n",
       "      <td>901.982512</td>\n",
       "      <td>0.131350</td>\n",
       "      <td>105.446721</td>\n",
       "      <td>1139.219205</td>\n",
       "      <td>0.517373</td>\n",
       "      <td>103.555565</td>\n",
       "      <td>874.089284</td>\n",
       "      <td>0.231592</td>\n",
       "      <td>587.716132</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.013125</td>\n",
       "      <td>0.76769</td>\n",
       "      <td>0.016975</td>\n",
       "      <td>-0.13440</td>\n",
       "      <td>0.55792</td>\n",
       "      <td>0.026116</td>\n",
       "      <td>-0.23058</td>\n",
       "      <td>0.80156</td>\n",
       "      <td>-0.039267</td>\n",
       "      <td>0.15317</td>\n",
       "      <td>0.54221</td>\n",
       "      <td>0.019385</td>\n",
       "      <td>0.25349</td>\n",
       "      <td>0.78255</td>\n",
       "      <td>-0.034044</td>\n",
       "      <td>-0.25871</td>\n",
       "      <td>1.0100</td>\n",
       "      <td>-0.098156</td>\n",
       "      <td>0.27352</td>\n",
       "      <td>1.00080</td>\n",
       "      <td>-0.10205</td>\n",
       "      <td>-0.064062</td>\n",
       "      <td>0.048522</td>\n",
       "      <td>-0.035460</td>\n",
       "      <td>0.083012</td>\n",
       "      <td>0.045385</td>\n",
       "      <td>-0.040149</td>\n",
       "      <td>-0.11567</td>\n",
       "      <td>-0.36270</td>\n",
       "      <td>-0.049812</td>\n",
       "      <td>0.11445</td>\n",
       "      <td>-0.38705</td>\n",
       "      <td>-0.032298</td>\n",
       "      <td>-0.12462</td>\n",
       "      <td>-0.73390</td>\n",
       "      <td>-0.049147</td>\n",
       "      <td>0.11816</td>\n",
       "      <td>-0.73437</td>\n",
       "      <td>-0.058490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.613827</td>\n",
       "      <td>0.993368</td>\n",
       "      <td>272.870526</td>\n",
       "      <td>1010.081804</td>\n",
       "      <td>0.988391</td>\n",
       "      <td>254.171676</td>\n",
       "      <td>1024.759945</td>\n",
       "      <td>0.982182</td>\n",
       "      <td>255.689709</td>\n",
       "      <td>1000.377692</td>\n",
       "      <td>0.734645</td>\n",
       "      <td>270.953928</td>\n",
       "      <td>1041.859123</td>\n",
       "      <td>0.380509</td>\n",
       "      <td>276.037587</td>\n",
       "      <td>976.845164</td>\n",
       "      <td>0.777726</td>\n",
       "      <td>338.242424</td>\n",
       "      <td>1079.142920</td>\n",
       "      <td>0.554936</td>\n",
       "      <td>350.733095</td>\n",
       "      <td>961.744201</td>\n",
       "      <td>0.572586</td>\n",
       "      <td>240.081118</td>\n",
       "      <td>1137.495570</td>\n",
       "      <td>0.543105</td>\n",
       "      <td>242.883200</td>\n",
       "      <td>901.567692</td>\n",
       "      <td>0.136428</td>\n",
       "      <td>104.504332</td>\n",
       "      <td>1139.397962</td>\n",
       "      <td>0.504939</td>\n",
       "      <td>103.515364</td>\n",
       "      <td>874.223371</td>\n",
       "      <td>0.206954</td>\n",
       "      <td>589.144867</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>0.013139</td>\n",
       "      <td>0.76703</td>\n",
       "      <td>0.016671</td>\n",
       "      <td>-0.13439</td>\n",
       "      <td>0.55792</td>\n",
       "      <td>0.025870</td>\n",
       "      <td>-0.23022</td>\n",
       "      <td>0.80156</td>\n",
       "      <td>-0.039252</td>\n",
       "      <td>0.15321</td>\n",
       "      <td>0.54231</td>\n",
       "      <td>0.019179</td>\n",
       "      <td>0.25362</td>\n",
       "      <td>0.78132</td>\n",
       "      <td>-0.034038</td>\n",
       "      <td>-0.25727</td>\n",
       "      <td>1.0100</td>\n",
       "      <td>-0.098092</td>\n",
       "      <td>0.27432</td>\n",
       "      <td>1.00080</td>\n",
       "      <td>-0.10202</td>\n",
       "      <td>-0.063768</td>\n",
       "      <td>0.048384</td>\n",
       "      <td>-0.035447</td>\n",
       "      <td>0.083212</td>\n",
       "      <td>0.045353</td>\n",
       "      <td>-0.040140</td>\n",
       "      <td>-0.11520</td>\n",
       "      <td>-0.36424</td>\n",
       "      <td>-0.051272</td>\n",
       "      <td>0.11472</td>\n",
       "      <td>-0.38644</td>\n",
       "      <td>-0.033540</td>\n",
       "      <td>-0.12461</td>\n",
       "      <td>-0.73518</td>\n",
       "      <td>-0.049845</td>\n",
       "      <td>0.11794</td>\n",
       "      <td>-0.73534</td>\n",
       "      <td>-0.058871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0.616458</td>\n",
       "      <td>0.992321</td>\n",
       "      <td>272.446694</td>\n",
       "      <td>1009.098878</td>\n",
       "      <td>0.988987</td>\n",
       "      <td>253.733421</td>\n",
       "      <td>1023.635307</td>\n",
       "      <td>0.982231</td>\n",
       "      <td>255.291728</td>\n",
       "      <td>1000.092430</td>\n",
       "      <td>0.757978</td>\n",
       "      <td>270.887374</td>\n",
       "      <td>1040.814260</td>\n",
       "      <td>0.395599</td>\n",
       "      <td>274.858078</td>\n",
       "      <td>977.857769</td>\n",
       "      <td>0.767602</td>\n",
       "      <td>337.309908</td>\n",
       "      <td>1079.511514</td>\n",
       "      <td>0.573403</td>\n",
       "      <td>350.596995</td>\n",
       "      <td>962.638706</td>\n",
       "      <td>0.586219</td>\n",
       "      <td>238.813969</td>\n",
       "      <td>1138.983025</td>\n",
       "      <td>0.544667</td>\n",
       "      <td>243.702899</td>\n",
       "      <td>902.284827</td>\n",
       "      <td>0.039230</td>\n",
       "      <td>93.306244</td>\n",
       "      <td>1137.526624</td>\n",
       "      <td>0.456074</td>\n",
       "      <td>101.548887</td>\n",
       "      <td>875.156868</td>\n",
       "      <td>0.224446</td>\n",
       "      <td>588.449288</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.013177</td>\n",
       "      <td>0.76616</td>\n",
       "      <td>0.015813</td>\n",
       "      <td>-0.13397</td>\n",
       "      <td>0.55757</td>\n",
       "      <td>0.025667</td>\n",
       "      <td>-0.22984</td>\n",
       "      <td>0.80156</td>\n",
       "      <td>-0.039234</td>\n",
       "      <td>0.15334</td>\n",
       "      <td>0.54155</td>\n",
       "      <td>0.018876</td>\n",
       "      <td>0.25476</td>\n",
       "      <td>0.78061</td>\n",
       "      <td>-0.034037</td>\n",
       "      <td>-0.25589</td>\n",
       "      <td>1.0091</td>\n",
       "      <td>-0.098032</td>\n",
       "      <td>0.27542</td>\n",
       "      <td>1.00080</td>\n",
       "      <td>-0.10197</td>\n",
       "      <td>-0.063421</td>\n",
       "      <td>0.047696</td>\n",
       "      <td>-0.035423</td>\n",
       "      <td>0.083461</td>\n",
       "      <td>0.044645</td>\n",
       "      <td>-0.040129</td>\n",
       "      <td>-0.11481</td>\n",
       "      <td>-0.36576</td>\n",
       "      <td>-0.053177</td>\n",
       "      <td>0.11521</td>\n",
       "      <td>-0.38635</td>\n",
       "      <td>-0.035453</td>\n",
       "      <td>-0.12471</td>\n",
       "      <td>-0.73625</td>\n",
       "      <td>-0.051154</td>\n",
       "      <td>0.11778</td>\n",
       "      <td>-0.73722</td>\n",
       "      <td>-0.059684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.626826</td>\n",
       "      <td>0.992370</td>\n",
       "      <td>272.878498</td>\n",
       "      <td>1008.953404</td>\n",
       "      <td>0.988711</td>\n",
       "      <td>254.000736</td>\n",
       "      <td>1023.688530</td>\n",
       "      <td>0.981702</td>\n",
       "      <td>255.377566</td>\n",
       "      <td>999.837624</td>\n",
       "      <td>0.774546</td>\n",
       "      <td>270.621712</td>\n",
       "      <td>1040.393927</td>\n",
       "      <td>0.391449</td>\n",
       "      <td>273.927825</td>\n",
       "      <td>978.350099</td>\n",
       "      <td>0.768187</td>\n",
       "      <td>336.862481</td>\n",
       "      <td>1079.437516</td>\n",
       "      <td>0.562235</td>\n",
       "      <td>350.707741</td>\n",
       "      <td>962.317466</td>\n",
       "      <td>0.566379</td>\n",
       "      <td>241.677129</td>\n",
       "      <td>1137.541196</td>\n",
       "      <td>0.561406</td>\n",
       "      <td>243.707463</td>\n",
       "      <td>901.252746</td>\n",
       "      <td>0.106802</td>\n",
       "      <td>102.554181</td>\n",
       "      <td>1137.392365</td>\n",
       "      <td>0.471692</td>\n",
       "      <td>101.467682</td>\n",
       "      <td>873.330147</td>\n",
       "      <td>0.237681</td>\n",
       "      <td>589.192264</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.013234</td>\n",
       "      <td>0.76524</td>\n",
       "      <td>0.014532</td>\n",
       "      <td>-0.13355</td>\n",
       "      <td>0.55695</td>\n",
       "      <td>0.025575</td>\n",
       "      <td>-0.22928</td>\n",
       "      <td>0.80127</td>\n",
       "      <td>-0.039421</td>\n",
       "      <td>0.15351</td>\n",
       "      <td>0.54063</td>\n",
       "      <td>0.018614</td>\n",
       "      <td>0.25610</td>\n",
       "      <td>0.77736</td>\n",
       "      <td>-0.033977</td>\n",
       "      <td>-0.25448</td>\n",
       "      <td>1.0076</td>\n",
       "      <td>-0.097969</td>\n",
       "      <td>0.27725</td>\n",
       "      <td>1.00020</td>\n",
       "      <td>-0.10189</td>\n",
       "      <td>-0.062726</td>\n",
       "      <td>0.046813</td>\n",
       "      <td>-0.034977</td>\n",
       "      <td>0.084082</td>\n",
       "      <td>0.043731</td>\n",
       "      <td>-0.040102</td>\n",
       "      <td>-0.11462</td>\n",
       "      <td>-0.36651</td>\n",
       "      <td>-0.056456</td>\n",
       "      <td>0.11606</td>\n",
       "      <td>-0.38678</td>\n",
       "      <td>-0.039129</td>\n",
       "      <td>-0.12476</td>\n",
       "      <td>-0.73736</td>\n",
       "      <td>-0.053214</td>\n",
       "      <td>0.11782</td>\n",
       "      <td>-0.73823</td>\n",
       "      <td>-0.060639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.656187</td>\n",
       "      <td>0.992199</td>\n",
       "      <td>272.552117</td>\n",
       "      <td>1008.530946</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>253.512955</td>\n",
       "      <td>1023.707535</td>\n",
       "      <td>0.982186</td>\n",
       "      <td>255.120958</td>\n",
       "      <td>999.857743</td>\n",
       "      <td>0.786832</td>\n",
       "      <td>270.719317</td>\n",
       "      <td>1040.153683</td>\n",
       "      <td>0.376655</td>\n",
       "      <td>274.050199</td>\n",
       "      <td>977.895391</td>\n",
       "      <td>0.771492</td>\n",
       "      <td>337.537906</td>\n",
       "      <td>1079.643322</td>\n",
       "      <td>0.555928</td>\n",
       "      <td>350.230158</td>\n",
       "      <td>962.086911</td>\n",
       "      <td>0.592517</td>\n",
       "      <td>240.537102</td>\n",
       "      <td>1137.865899</td>\n",
       "      <td>0.563793</td>\n",
       "      <td>243.040456</td>\n",
       "      <td>901.359304</td>\n",
       "      <td>0.551228</td>\n",
       "      <td>114.411065</td>\n",
       "      <td>1135.575960</td>\n",
       "      <td>0.511387</td>\n",
       "      <td>114.932842</td>\n",
       "      <td>867.126963</td>\n",
       "      <td>0.219999</td>\n",
       "      <td>589.392992</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>0.013457</td>\n",
       "      <td>0.76193</td>\n",
       "      <td>0.012078</td>\n",
       "      <td>-0.13312</td>\n",
       "      <td>0.55481</td>\n",
       "      <td>0.025396</td>\n",
       "      <td>-0.22732</td>\n",
       "      <td>0.79641</td>\n",
       "      <td>-0.040943</td>\n",
       "      <td>0.15394</td>\n",
       "      <td>0.53958</td>\n",
       "      <td>0.018403</td>\n",
       "      <td>0.25797</td>\n",
       "      <td>0.77305</td>\n",
       "      <td>-0.033895</td>\n",
       "      <td>-0.25208</td>\n",
       "      <td>1.0054</td>\n",
       "      <td>-0.100710</td>\n",
       "      <td>0.28009</td>\n",
       "      <td>0.99628</td>\n",
       "      <td>-0.10214</td>\n",
       "      <td>-0.062056</td>\n",
       "      <td>0.045160</td>\n",
       "      <td>-0.033615</td>\n",
       "      <td>0.085121</td>\n",
       "      <td>0.042273</td>\n",
       "      <td>-0.039195</td>\n",
       "      <td>-0.11417</td>\n",
       "      <td>-0.36754</td>\n",
       "      <td>-0.064164</td>\n",
       "      <td>0.11940</td>\n",
       "      <td>-0.38693</td>\n",
       "      <td>-0.046703</td>\n",
       "      <td>-0.12462</td>\n",
       "      <td>-0.73756</td>\n",
       "      <td>-0.056391</td>\n",
       "      <td>0.11798</td>\n",
       "      <td>-0.73823</td>\n",
       "      <td>-0.061967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Frame  ...  right_foot_x  right_foot_y  right_foot_z\n",
       "0           0            26     27  ...       0.11816      -0.73437     -0.058490\n",
       "1           1            27     28  ...       0.11794      -0.73534     -0.058871\n",
       "2           2            28     29  ...       0.11778      -0.73722     -0.059684\n",
       "3           3            29     30  ...       0.11782      -0.73823     -0.060639\n",
       "4           4            30     31  ...       0.11798      -0.73823     -0.061967\n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-m_bubwoo5j"
   },
   "outputs": [],
   "source": [
    "# DATA PREPROCESSING\n",
    "# Remove frame, score, z and unnamed columns\n",
    "data = data.loc[:, ~data.columns.str.contains('^Unnamed|^Frame|^Pose|Eye|Ear|_z|_score|_Score')]\n",
    "# Randomize data\n",
    "shuffled_data = shuffle(data, random_state=42)\n",
    "# Split into inputs and labels\n",
    "raw_features = shuffled_data.iloc[:, 0:num_of_inputs]\n",
    "raw_labels = shuffled_data.drop(raw_features.columns, axis=1)\n",
    "# Scale data\n",
    "# Scale features and labels\n",
    "feature_scaler = MinMaxScaler()\n",
    "label_scaler = MinMaxScaler()\n",
    "features = feature_scaler.fit_transform(raw_features)\n",
    "labels = label_scaler.fit_transform(raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7JdNK1vnqZG"
   },
   "outputs": [],
   "source": [
    "# Split data into train, tes and validation sets\n",
    "X_train, Xval, y_train, yval = train_test_split(features, labels, test_size=0.3)\n",
    "X_val, X_test, y_val, y_test = train_test_split(Xval, yval, test_size=0.333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "f8GwMfkaw96l",
    "outputId": "df82b512-0f79-4f13-bca3-af96d364fc96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16790, 26)\n",
      "(4800, 26)\n",
      "(2397, 26)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SI_Ot1Czpfz3"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(26,)))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5ic7kp6uzqc"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-4), metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "_X5IBQhnvyHb",
    "outputId": "1c20a6e8-bdae-4152-f22b-87f0fcc319f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fec47104c50>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6)\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=1000, verbose=0, batch_size=128, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "JxF5mXszwVtz",
    "outputId": "758e7237-05b7-48b4-926e-6d8c045d57ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 - 0s - loss: 0.0493 - mse: 0.0047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04928087443113327, 0.0047427089884877205]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data evaluation\n",
    "model.evaluate(X_test, y_test, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaiwpCQCZgRB"
   },
   "source": [
    "# Software development section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NzK1NRaZ4b_"
   },
   "source": [
    "## PoseNet to Kinect Pipeline\n",
    "\n",
    "1. Designed pipeline based on the same software architecture as the one for Kinect.\n",
    "\n",
    "ML Pipelines description for `Kinect` and `PoseNet to Kinect` models:\n",
    "\n",
    "Deep learning pipeline to simplify model training, testing and deployment for conducting the inferance. The pipeline consists of the following modules:\n",
    "* Data creator - Reads the data from the given folder. Processes it into separate datasets in order to train and test the model.\n",
    "* Model - Serves as a wrapper for the model architecture to simplify the model compilation, saving, serving.\n",
    "* Trainer - Defines the pipeline for the model training. Integrates Model, Data creator with the configurate hyperparameters to start the training. Includes early stopping callback to stop the traininng session if no loss decrease was made in the given N epochs. Aslo contains the Tensorboard callback for interactive visualization of optimization process and logs saving.\n",
    "* Estimator - the module to serve the model and conduct the inferance on a given set of data\n",
    "* Config - Includes multi-level fancy dictionary for project configuration.\n",
    "\n",
    "2. Extended the pipeline architecture for both models:\n",
    "* Implemented scalers for input and output of the model data. Created save/load pipeline for scalers data creator and estimator.\n",
    "* Added test split option\n",
    "* Fixed the dataset being partly fixed\n",
    "* Added configs for both models individually\n",
    "3. Code clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZTVbn0Yx6Lp"
   },
   "source": [
    "### Adam optimizer results\n",
    "\n",
    "- Before Feedback:  \n",
    "\n",
    "All tests were executed varying the following parameters:\n",
    "\n",
    "- batch sizes: 16, 32, 64\n",
    "- epochs: 20, 100, 500, 1000\n",
    "- activation functions: ReLU, SELU, tanh\n",
    "- learning rate for optimizer: varies between 3e-4 and 3e-6\n",
    "\n",
    "Each parameter combination was tested 5 times and an average of the test results was taken.\n",
    "\n",
    "The following neural network architectures were tested:\n",
    "1. 1 hidden layer with 32 neurons, best result: mae= 0.052521, mse= 0.005415, batch size= 64, epochs= 100, activation= relu\n",
    "2. 1 hidden layer with 64 neurons, best result: mae= 0.055399, mse= 0.005902, batch size= 64, epochs= 100, activation= relu\n",
    "3. 1 hidden layer with 128 neurons, best result: mae= 0.06783, mse= 0.007983, batch size= 64, epochs= 100, activation= relu\n",
    "4. 2 hidden layers with 64 and 32 neurons, best result: mae= 0.050266, mse= 0.00486, batch size= 64, epochs= 100, activation= relu\n",
    "5. **2 hidden layers with 128 and 64 neurons, best result: mae= 0.049150, mse= 0.004465, batch size= 64, epochs= 100, activation= relu**  \n",
    "\n",
    "- After Feedback:  \n",
    "Adam(learning_rate = 1e-4)\n",
    "\n",
    "\n",
    "| Neurons | Activation function | Epochs | Batch size | Results on test set|\n",
    "|---------|---------------------|--------|------------|--------------------|\n",
    "64, None | relu | 100 | 16 |  mae (loss): 0.0563, mse: 0.0059 |\n",
    "128, None | relu | 100 | 16 |  mae (loss): 0.0850, mse: 0.0140 |\n",
    "256, None | relu | 100 | 16 |  mae (loss): 0.0790, mse: 0.0100 |\n",
    "64, 32 | relu | 100 | 16 |  mae (loss): 0.0476, mse: 0.0047 |\n",
    "128, 64 | relu | 100 | 16 |  mae (loss): 0.0466, mse: 0.0042 |\n",
    "64, None | selu | 100 | 16 |  mae (loss): 0.0595, mse: 0.0068 |\n",
    "128, None | selu | 100 | 16 |  mae (loss): 0.0788, mse: 0.0104 |\n",
    "256, None | selu | 100 | 16 |  mae (loss): 0.1132, mse: 0.0187 |\n",
    "64, 32 | selu | 100 | 16 |  mae (loss): 0.0516, mse: 0.0050 |\n",
    "128, 64 | selu | 100 | 16 |  mae (loss): 0.0521, mse: 0.0050 |\n",
    "64, None | tanh | 100 | 16 |  mae (loss): 0.0653, mse: 0.0073 |\n",
    "128, None | tanh | 100 | 16 |  mae (loss): 0.0704, mse: 0.0091 |\n",
    "256, None | tanh | 100 | 16 |  mae (loss): 0.1757, mse: 0.0466 |\n",
    "64, 32 | tanh | 100 | 16 |  mae (loss): 0.0547, mse: 0.0077 |\n",
    "128, 64 | tanh | 100 | 16 |  mae (loss): 0.0714, mse: 0.0087 |\n",
    "64, None | relu | 500 | 16 |  mae (loss): 0.0527, mse: 0.0052 |\n",
    "128, None |  relu | 500 | 16 |  mae (loss): 0.0656, mse: 0.0115 |\n",
    "256, None | relu | 500 | 16 |  mae (loss): 0.0963, mse: 0.0140 |\n",
    "64, 32 | relu | 500 | 16 |  mae (loss): 0.0494, mse: 0.0047 |\n",
    "128, 64 | relu | 500 | 16 |  mae (loss): 0.0417, mse: 0.0033 |\n",
    "64, None | selu | 500 | 16 |  mae (loss): 0.0682, mse: 0.0078 |\n",
    "128, None | selu | 500 | 16 |  mae (loss): 0.0670, mse: 0.0076 |\n",
    "256, None | selu | 500 | 16 |  mae (loss): 0.1296, mse: 0.0246 |\n",
    "64, 32 | selu | 500 | 16 |  mae (loss): 0.0484, mse: 0.0045 |\n",
    "128, 64 | selu | 500 | 16 |  mae (loss): 0.0578, mse: 0.0062 |\n",
    "64, None | tanh | 500 | 16 |  mae (loss): 0.0676, mse: 0.0099 |\n",
    "128, None | tanh | 500 | 16 |  mae (loss): 0.0817, mse: 0.0116 |\n",
    "256, None | tanh | 500 | 16 |  mae (loss): 0.1668, mse: 0.0413 |\n",
    "64, 32 | tanh | 500 | 16 |  mae (loss): 0.0507, mse: 0.0063 |\n",
    "128, 64 | tanh | 500 | 16 |  mae (loss): 0.0600, mse: 0.0071 |\n",
    "64, None | relu | 1000 | 16 |  mae (loss): 0.0567, mse: 0.0062 |\n",
    "128, None | relu | 1000 | 16 |  mae (loss): 0.0704, mse: 0.0118 |\n",
    "256, None | relu | 1000 | 16 |  mae (loss): 0.1019, mse: 0.0179 |\n",
    "64, 32 | relu | 1000 | 16 |  mae (loss): 0.0493, mse: 0.0047 |\n",
    "128, 64 | relu | 1000 | 16 |  mae (loss): 0.0543, mse: 0.0061 |\n",
    "64, None | selu | 1000 | 16 |  mae (loss): 0.0733, mse: 0.0088 |\n",
    "128, None | selu | 1000 | 16 |  mae (loss): 0.0830, mse: 0.0112 |\n",
    "256, None | selu | 1000 | 16 |  mae (loss): 0.1399, mse: 0.0275 |\n",
    "64, 32 | selu | 1000 | 16 |  mae (loss): 0.0500, mse: 0.0051 |\n",
    "128, 64 | selu | 1000 | 16 |  mae (loss): 0.0535, mse: 0.0054 |\n",
    "64, None | tanh | 1000 | 16 |  mae (loss): 0.0724, mse: 0.0119 |\n",
    "128, None | tanh | 1000 | 16 |  mae (loss): 0.0989, mse: 0.0167 |\n",
    "256, None | tanh | 1000 | 16 |  mae (loss): 0.1301, mse: 0.0275 |\n",
    "64, 32 | tanh | 1000 | 16 |  mae (loss): 0.0521, mse: 0.0052 |\n",
    "128, 64 | tanh | 1000 | 16 |  mae (loss): 0.0687, mse: 0.0087 |\n",
    "64, None | relu | 100 | 32 |  mae (loss): 0.0586, mse: 0.0086 |\n",
    "128, None | relu | 100 | 32 |  mae (loss): 0.0606, mse: 0.0075 |\n",
    "256, None | relu | 100 | 32 |  mae (loss): 0.0804, mse: 0.0111 |\n",
    "64, 32 | relu | 100 | 32 |  mae (loss): 0.0486, mse: 0.0046 |\n",
    "128, 64 | relu | 100 | 32 |  mae (loss): 0.0490, mse: 0.0045 |\n",
    "64, None | selu | 100 | 32 |  mae (loss): 0.0768, mse: 0.0097 |\n",
    "128, None | selu | 100 | 32 |  mae (loss): 0.0898, mse: 0.0125 |\n",
    "256, None | selu | 100 | 32 |  mae (loss): 0.1003, mse: 0.0157 |\n",
    "64, 32 | selu | 100 | 32 |  mae (loss): 0.0526, mse: 0.0053 |\n",
    "128, 64 | selu | 100 | 32 |  mae (loss): 0.0595, mse: 0.0060 |\n",
    "64, None | tanh | 100 | 32 |  mae (loss): 0.0697, mse: 0.0088 |\n",
    "128, None | tanh | 100 | 32 |  mae (loss): 0.0669, mse: 0.0079 |\n",
    "256, None | tanh | 100 | 32 |  mae (loss): 0.0670, mse: 0.0081 |\n",
    "64, 32 | tanh | 100 | 32 |  mae (loss): 0.0683, mse: 0.0080 |\n",
    "128, 64 | tanh | 100 | 32 |  mae (loss): 0.0704, mse: 0.0088 |\n",
    "64, None | relu | 500 | 32 |  mae (loss): 0.0534, mse: 0.0086 |\n",
    "128, None | relu | 500 | 32 |  mae (loss): 0.0588, mse: 0.0078 |\n",
    "256, None | relu | 500 | 32 |  mae (loss): 0.0846, mse: 0.0126 |\n",
    "64, 32 | relu | 500 | 32 |  mae (loss): 0.0511, mse: 0.0051 |\n",
    "128, 64 | relu | 500 | 32 |  mae (loss): 0.0452, mse: 0.0038 |\n",
    "64, None | selu | 500 | 32 |  mae (loss): 0.0790, mse: 0.0102 |\n",
    "128, None | selu | 500 | 32 |  mae (loss): 0.0841, mse: 0.0119 |\n",
    "256, None | selu | 500 | 32 |  mae (loss): 0.1100, mse: 0.0191 |\n",
    "64, 32 | selu | 500 | 32 |  mae (loss): 0.0497, mse: 0.0049 |\n",
    "128, 64 | selu | 500 | 32 |  mae (loss): 0.0549, mse: 0.0055 |\n",
    "64, None | tanh | 500 | 32 |  mae (loss): 0.0677, mse: 0.0085 |\n",
    "128, None | tanh | 500 | 32 |  mae (loss): 0.0732, mse: 0.0091 |\n",
    "256, None | tanh | 500 | 32 |  mae (loss): 0.0774, mse: 0.0104 |\n",
    "64, 32 | tanh | 500 | 32 |  mae (loss): 0.0660, mse: 0.0086 |\n",
    "128, 64 | tanh | 500 | 32 |  mae (loss): 0.0685, mse: 0.0080 |\n",
    "64, None | relu | 1000 | 32 |  mae (loss): 0.0620, mse: 0.0090 |\n",
    "128, None | relu | 1000 | 32 |  mae (loss): 0.0617, mse: 0.0070 |\n",
    "256, None | relu | 1000 | 32 |  mae (loss): 0.1011, mse: 0.0160 |\n",
    "64, 32 | relu | 1000 | 32 |  mae (loss): 0.0527, mse: 0.0057 |\n",
    "128, 64 | relu | 1000 | 32 |  mae (loss): 0.0434, mse: 0.0035 |\n",
    "64, None | selu | 1000 | 32 |  mae (loss): 0.0668, mse: 0.0076 |\n",
    "128, None | selu | 1000 | 32 |  mae (loss): 0.0802, mse: 0.0105 |\n",
    "256, None | selu | 1000 | 32 |  mae (loss): 0.0974, mse: 0.0150 |\n",
    "64, 32 | selu | 1000 | 32 |  mae (loss): 0.0533, mse: 0.0053 |\n",
    "128, 64 | selu | 1000 | 32 |  mae (loss): 0.0638, mse: 0.0068 |\n",
    "64, None | tanh | 1000 | 32 |  mae (loss): 0.0640, mse: 0.0071 |\n",
    "128, None | tanh | 1000 | 32 |  mae (loss): 0.0700, mse: 0.0084 |\n",
    "256, None | tanh | 1000 | 32 |  mae (loss): 0.0971, mse: 0.0149 |\n",
    "64, 32 | tanh | 1000 | 32 |  mae (loss): 0.0591, mse: 0.0063 |\n",
    "128, 64 | tanh | 1000 | 32 |  mae (loss): 0.0765, mse: 0.0103 |\n",
    "64, None | relu | 100 | 64 |  mae (loss): 0.0472, mse: 0.0045 |\n",
    "128, None | relu | 100 | 64 |  mae (loss): 0.0590, mse: 0.0064 |\n",
    "256, None | relu | 100 | 64 |  mae (loss): 0.0775, mse: 0.0095 |\n",
    "64, 32 | relu | 100 | 64 |  mae (loss): 0.0496, mse: 0.0048 |\n",
    "128, 64 | relu | 100 | 64 |  mae (loss): 0.0429, mse: 0.0037 |\n",
    "64, None | selu | 100 | 64 |  mae (loss): 0.0590, mse: 0.0065 |\n",
    "128, None | selu | 100 | 64 |  mae (loss): 0.0973, mse: 0.0145 |\n",
    "256, None | selu | 100 | 64 |  mae (loss): 0.0862, mse: 0.0114 |\n",
    "64, 32 | selu | 100 | 64 |  mae (loss): 0.0485, mse: 0.0048 |\n",
    "128, 64 | selu | 100 | 64 |  mae (loss): 0.0531, mse: 0.0051 |\n",
    "64, None | tanh | 100 | 64 |  mae (loss): 0.0475, mse: 0.0044 |\n",
    "128, None | tanh | 100 | 64 |  mae (loss): 0.0808, mse: 0.0101 |\n",
    "256, None | tanh | 100 | 64 |  mae (loss): 0.0959, mse: 0.0135 |\n",
    "64, 32 | tanh | 100 | 64 |  mae (loss): 0.0669, mse: 0.0076 |\n",
    "128, 64 | tanh | 100 | 64 |  mae (loss): 0.0640, mse: 0.0071 |\n",
    "64, None | relu | 500 | 64 |  mae (loss): 0.0478, mse: 0.0045 |\n",
    "128, None | relu | 500 | 64 |  mae (loss): 0.0562, mse: 0.0058 |\n",
    "256, None | relu | 500 | 64 |  mae (loss): 0.0829, mse: 0.0111 |\n",
    "64, 32 | relu | 500 | 64 |  mae (loss): 0.0468, mse: 0.0044 |\n",
    "128, 64 | relu | 500 | 64 |  mae (loss): 0.0456, mse: 0.0040 |\n",
    "64, None | selu | 500 | 64 |  mae (loss): 0.0484, mse: 0.0046 |\n",
    "128, None | selu | 500 | 64 |  mae (loss): 0.0816, mse: 0.0104 |\n",
    "256, None | selu | 500 | 64 |  mae (loss): 0.1330, mse: 0.0253 |\n",
    "64, 32 | selu | 500 | 64 |  mae (loss): 0.0455, mse: 0.0041 |\n",
    "128, 64 | selu | 500 | 64 |  mae (loss): 0.0596, mse: 0.0061 |\n",
    "64, None | tanh | 500 | 64 |  mae (loss): 0.0564, mse: 0.0060 |\n",
    "128, None | tanh | 500 | 64 |  mae (loss): 0.0708, mse: 0.0082 |\n",
    "256, None | tanh | 500 | 64 |  mae (loss): 0.0782, mse: 0.0110 |\n",
    "64, 32 | tanh | 500 | 64 |  mae (loss): 0.0567, mse: 0.0060 |\n",
    "128, 64 | tanh | 500 | 64 |  mae (loss): 0.0669, mse: 0.0077 |\n",
    "64, None | relu | 1000 | 64 |  mae (loss): 0.0518, mse: 0.0050 |\n",
    "128, None | relu | 1000 | 64 |  mae (loss): 0.0494, mse: 0.0047 |\n",
    "256, None | relu | 1000 | 64 |  mae (loss): 0.0664, mse: 0.0074 |\n",
    "64, 32 | relu | 1000 | 64 |  mae (loss): 0.0450, mse: 0.0040 |\n",
    "128, 64 | relu | 1000 | 64 |  mae (loss): 0.0413, mse: 0.0034 |\n",
    "64, None | selu | 1000 | 64 |  mae (loss): 0.0478, mse: 0.0045 |\n",
    "128, None | selu | 1000 | 64 |  mae (loss): 0.0844, mse: 0.0110 |\n",
    "256, None | selu | 1000 | 64 |  mae (loss): 0.0791, mse: 0.0099 |\n",
    "64, 32 | selu | 1000 | 64 |  mae (loss): 0.0488, mse: 0.0047 |\n",
    "128, 64 | selu | 1000 | 64 |  mae (loss): 0.0550, mse: 0.0054 |\n",
    "64, None | tanh | 1000 | 64 |  mae (loss): 0.0486, mse: 0.0047 |\n",
    "128, None | tanh | 1000 | 64 |  mae (loss): 0.0750, mse: 0.0088 |\n",
    "256, None | tanh | 1000 | 64 |  mae (loss): 0.0844, mse: 0.0113 |\n",
    "64, 32 | tanh | 1000 | 64 |  mae (loss): 0.0606, mse: 0.0066 |\n",
    "128, 64 | tanh | 1000 | 64 |  mae (loss): 0.0638, mse: 0.0072 |\n",
    "64, None | relu | 100 | 128 |  mae (loss): 0.0510, mse: 0.0050 |\n",
    "128, None | relu | 100 | 128 |  mae (loss): 0.0504, mse: 0.0048 |\n",
    "256, None | relu | 100 | 128 |  mae (loss): 0.0493, mse: 0.0046 |\n",
    "64, 32 | relu | 100 | 128 |  mae (loss): 0.0449, mse: 0.0041 |\n",
    "128, 64 | relu | 100 | 128 |  mae (loss): 0.0411, mse: 0.0034 |\n",
    "64, None | selu | 100 | 128 |  mae (loss): 0.0479, mse: 0.0045 |\n",
    "128, None | selu | 100 | 128 |  mae (loss): 0.0499, mse: 0.0047 |\n",
    "256, None | selu | 100 | 128 |  mae (loss): 0.0618, mse: 0.0067 |\n",
    "64, 32 | selu | 100 | 128 |  mae (loss): 0.0450, mse: 0.0039 |\n",
    "128, 64 | selu | 100 | 128 |  mae (loss): 0.0442, mse: 0.0038 |\n",
    "64, None | tanh | 100 | 128 |  mae (loss): 0.0487, mse: 0.0045 |\n",
    "128, None | tanh | 100 | 128 |  mae (loss): 0.0580, mse: 0.0060 |\n",
    "256, None | tanh | 100 | 128 |  mae (loss): 0.0904, mse: 0.0126 |\n",
    "64, 32 | tanh | 100 | 128 |  mae (loss): 0.0496, mse: 0.0046 |\n",
    "128, 64 | tanh | 100 | 128 |  mae (loss): 0.0689, mse: 0.0077 |\n",
    "64, None | relu | 500 | 128 |  mae (loss): 0.0501, mse: 0.0049 |\n",
    "128, None | relu | 500 | 128 |  mae (loss): 0.0461, mse: 0.0042 |\n",
    "256, None | relu | 500 | 128 |  mae (loss): 0.0513, mse: 0.0048 |\n",
    "64, 32 | relu | 500 | 128 |  mae (loss): 0.0423, mse: 0.0037 |\n",
    "128, 64 | relu | 500 | 128 |  mae (loss): 0.0443, mse: 0.0038 |\n",
    "64, None | selu | 500 | 128 |  mae (loss): 0.0485, mse: 0.0047 |\n",
    "128, None | selu | 500 | 128 |  mae (loss): 0.0514, mse: 0.0049 |\n",
    "256, None | selu | 500 | 128 |  mae (loss): 0.0640, mse: 0.0070 |\n",
    "64, 32 | selu | 500 | 128 |  mae (loss): 0.0467, mse: 0.0042 |\n",
    "128, 64 | selu | 500 | 128 |  mae (loss): 0.0466, mse: 0.0041 |\n",
    "64, None | tanh | 500 | 128 |  mae (loss): 0.0499, mse: 0.0047 |\n",
    "128, None | tanh | 500 | 128 |  mae (loss): 0.0591, mse: 0.0062 |\n",
    "256, None | tanh | 500 | 128 |  mae (loss): 0.0637, mse: 0.0070 |\n",
    "64, 32 | tanh | 500 | 128 |  mae (loss): 0.0487, mse: 0.0044 |\n",
    "128, 64 | tanh | 500 | 128 |  mae (loss): 0.0558, mse: 0.0055 |\n",
    "64, None | relu | 1000 | 128 |  mae (loss): 0.0487, mse: 0.0046 |\n",
    "128, None | relu | 1000 | 128 |  mae (loss): 0.0462, mse: 0.0042 |\n",
    "256, None | relu | 1000 | 128 |  mae (loss): 0.0562, mse: 0.0055 |\n",
    "64, 32 | relu | 1000 | 128 |  mae (loss): 0.0432, mse: 0.0037 |\n",
    "128, 64 | relu | 1000 | 128 |  mae (loss): 0.0397, mse: 0.0031 |\n",
    "64, None | selu | 1000 | 128 |  mae (loss): 0.0503, mse: 0.0048 |\n",
    "128, None | selu | 1000 | 128 |  mae (loss): 0.0524, mse: 0.0051 |\n",
    "256, None | selu | 1000 | 128 |  mae (loss): 0.0823, mse: 0.0105 |\n",
    "64, 32 | selu | 1000 | 128 |  mae (loss): 0.0487, mse: 0.0047 |\n",
    "128, 64 | selu | 1000 | 128 |  mae (loss): 0.0499, mse: 0.0047 |\n",
    "64, None | tanh | 1000 | 128 |  mae (loss): 0.0530, mse: 0.0051 |\n",
    "128, None | tanh | 1000 | 128 |  mae (loss): 0.0562, mse: 0.0057 |\n",
    "256, None | tanh | 1000 | 128 |  mae (loss): 0.0885, mse: 0.0118 |\n",
    "64, 32 | tanh | 1000 | 128 |  mae (loss): 0.0478, mse: 0.0044 |\n",
    "128, 64 | tanh | 1000 | 128 |  mae (loss): 0.0509, mse: 0.0047 |\n",
    "\n",
    "\n",
    "Best result: neurons=128, 64, activation=relu, batch_size=128, epochs=1000, MAE=0.0397, MSE=0.0031"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxUBcbE5yDxd"
   },
   "source": [
    "### SGD optimizer results\n",
    "\n",
    "- Before Feedback:  \n",
    "SGD(lr=3e-4, decay=1e-7, momentum=0.9, nesterov=True)\n",
    "\n",
    "| Activation Function  |\tEpochs  |  Batch Size  |\tTest set               |\n",
    "|----------------------|------------|--------------|---------------------------|\n",
    "| relu                 | 100        | 16             | mse: 0.0066 - mae: 0.0559              | \n",
    "| relu                 | 100        | 32             | mse: 0.0087 - mae: 0.0649              | \n",
    "| relu                 | 100        | 64             | mse: 0.0108 - mae: 0.0726              | \n",
    "| relu                 | 100        | 128            | mse: 0.0118 - mae: 0.0757              |\n",
    "| **relu**             | **1000**   | **16**         | **mse: 0.0038 - mae: 0.0417**          | \n",
    "| relu                 | 1000       | 32             | mse: 0.0043 - mae: 0.0444              | \n",
    "| relu                 | 1000       | 64             | mse: 0.0052 - mae: 0.0490              | \n",
    "| relu                 | 1000       | 128            | mse: 0.0064 - mae: 0.0545              |\n",
    "| tanh                 | 100        | 16             | mse: 0.0078 - mae: 0.0619              | \n",
    "| tanh                 | 100        | 32             | mse: 0.0095 - mae: 0.0670              | \n",
    "| tanh                 | 100        | 64             | mse: 0.0112 - mae: 0.0719              | \n",
    "| tanh                 | 100        | 128            | mse: 0.0127 - mae: 0.0745              |\n",
    "| tanh                 | 1000       | 16             | mse: 0.0042 - mae: 0.0453              | \n",
    "| tanh                 | 1000       | 32             | mse: 0.0047 - mae: 0.0475              | \n",
    "| tanh                 | 1000       | 64             | mse: 0.0055 - mae: 0.0513              | \n",
    "| tanh                 | 1000       | 128            | mse: 0.0071 - mae: 0.0593              | \n",
    "| selu                 | 100        | 16             | mse: 0.0061 - mae: 0.0539              |\n",
    "| selu                 | 100        | 32             | mse: 0.0076 - mae: 0.0606              | \n",
    "| selu                 | 100        | 64             | mse: 0.0094 - mae: 0.0658              | \n",
    "| selu                 | 100        | 128            | mse: 0.0112 - mae: 0.0711              | \n",
    "| selu                 | 1000       | 16             | mse: 0.0041 - mae: 0.0442              |\n",
    "| selu                 | 1000       | 32             | mse: 0.0042 - mae: 0.0447              | \n",
    "| selu                 | 1000       | 64             | mse: 0.0047 - mae: 0.0470              | \n",
    "| selu                 | 1000       | 128            | mse: 0.0058 - mae: 0.0521              | \n",
    "\n",
    "\n",
    "- After Feedback:  \n",
    "SGD(lr=1e-4, decay=1e-7, momentum=0.9, nesterov=True)\n",
    "\n",
    "| Neurons | Activation function | Epochs | Batch size | Results on test set|\n",
    "|---------|---------------------|--------|------------|--------------------|\n",
    "64, None | relu | 100 | 16 |  mae (loss): 0.0634, mse: 0.0096 |\n",
    "128, None | relu | 100 | 16 |  mae (loss): 0.0544, mse: 0.0097 |\n",
    "256, None | relu | 100 | 16 |  mae (loss): 0.0489, mse: 0.0062 |\n",
    "64, 32 | relu | 100 | 16 |  mae (loss): 0.0625, mse: 0.0088 |\n",
    "128, 64 | relu | 100 | 16 |  mae (loss): 0.0549, mse: 0.0071 |\n",
    "64, None | selu | 100 | 16 |  mae (loss): 0.0564, mse: 0.0065 |\n",
    "128, None | selu | 100 | 16 |  mae (loss): 0.0522, mse: 0.0060 |\n",
    "256, None | selu | 100 | 16 |  mae (loss): 0.0511, mse: 0.0054 |\n",
    "64, 32 | selu | 100 | 16 |  mae (loss): 0.0554, mse: 0.0061 |\n",
    "128, 64 | selu | 100 | 16 |  mae (loss): 0.0521, mse: 0.0060 |\n",
    "64, None | tanh | 100 | 16 |  mae (loss): 0.0595, mse: 0.0111 |\n",
    "128, None | tanh | 100 | 16 |  mae (loss): 0.0538, mse: 0.0063 |\n",
    "256, None | tanh | 100 | 16 |  mae (loss): 0.0515, mse: 0.0053 |\n",
    "64, 32 | tanh | 100 | 16 |  mae (loss): 0.0628, mse: 0.0086 |\n",
    "128, 64 | tanh | 100 | 16 |  mae (loss): 0.0548, mse: 0.0063 |\n",
    "64, None | relu | 500 | 16 |  mae (loss): 0.0490, mse: 0.0060 |\n",
    "128, None | relu | 500 | 16 |  mae (loss): 0.0474, mse: 0.0057 |\n",
    "256, None | relu | 500 | 16 |  mae (loss): 0.0466, mse: 0.0062 |\n",
    "64, 32 | relu | 500 | 16 |  mae (loss): 0.0491, mse: 0.0055 |\n",
    "128, 64 | relu | 500 | 16 |  mae (loss): 0.0431, mse: 0.0044 |\n",
    "64, None | selu | 500 | 16 |  mae (loss): 0.0472, mse: 0.0046 |\n",
    "128, None | selu | 500 | 16 |  mae (loss): 0.0453, mse: 0.0044 |\n",
    "256, None | selu | 500 | 16 |  mae (loss): 0.0499, mse: 0.0051 |\n",
    "64, 32 | selu | 500 | 16 |  mae (loss): 0.0467, mse: 0.0045 |\n",
    "128, 64 | selu | 500 | 16 |  mae (loss): 0.0429, mse: 0.0040 |\n",
    "64, None | tanh | 500 | 16 |  mae (loss): 0.0463, mse: 0.0044 |\n",
    "128, None | tanh | 500 | 16 |  mae (loss): 0.0467, mse: 0.0045 |\n",
    "256, None | tanh | 500 | 16 |  mae (loss): 0.0495, mse: 0.0049 |\n",
    "64, 32 | tanh | 500 | 16 |  mae (loss): 0.0454, mse: 0.0044 |\n",
    "128, 64 | tanh | 500 | 16 |  mae (loss): 0.0425, mse: 0.0037 |\n",
    "64, None | relu | 1000 | 16 |  mae (loss): 0.0466, mse: 0.0055 |\n",
    "128, None | relu | 1000 | 16 |  mae (loss): 0.0460, mse: 0.0060 |\n",
    "256, None | relu | 1000 | 16 |  mae (loss): 0.0445, mse: 0.0051 |\n",
    "64, 32 | relu | 1000 | 16 |  mae (loss): 0.0443, mse: 0.0041 |\n",
    "128, 64 | relu | 1000 | 16 |  mae (loss): 0.0428, mse: 0.0041 |\n",
    "64, None | selu | 1000 | 16 |  mae (loss): 0.0445, mse: 0.0042 |\n",
    "128, None | selu | 1000 | 16 |  mae (loss): 0.0443, mse: 0.0042 |\n",
    "256, None | selu | 1000 | 16 |  mae (loss): 0.0475, mse: 0.0052 |\n",
    "64, 32 | selu | 1000 | 16 |  mae (loss): 0.0426, mse: 0.0039 |\n",
    "128, 64 | selu | 1000 | 16 |  mae (loss): 0.0399, mse: 0.0033 |\n",
    "64, None | tanh | 1000 | 16 |  mae (loss): 0.0453, mse: 0.0045 |\n",
    "128, None | tanh | 1000 | 16 |  mae (loss): 0.0461, mse: 0.0044 |\n",
    "256, None | tanh | 1000 | 16 |  mae (loss): 0.0500, mse: 0.0051 |\n",
    "64, 32 | tanh | 1000 | 16 |  mae (loss): 0.0436, mse: 0.0040 |\n",
    "128, 64 | tanh | 1000 | 16 |  mae (loss): 0.0426, mse: 0.0037 |\n",
    "64, None | relu | 100 | 32 |  mae (loss): 0.0678, mse: 0.0122 |\n",
    "128, None | relu | 100 | 32 |  mae (loss): 0.0591, mse: 0.0079 |\n",
    "256, None | relu | 100 | 32 |  mae (loss): 0.0498, mse: 0.0051 |\n",
    "64, 32 | relu | 100 | 32 |  mae (loss): 0.0714, mse: 0.0111 |\n",
    "128, 64 | relu | 100 | 32 |  mae (loss): 0.0616, mse: 0.0085 |\n",
    "64, None | selu | 100 | 32 |  mae (loss): 0.0630, mse: 0.0079 |\n",
    "128, None | selu | 100 | 32 |  mae (loss): 0.0552, mse: 0.0064 |\n",
    "256, None | selu | 100 | 32 |  mae (loss): 0.0513, mse: 0.0056 |\n",
    "64, 32 | selu | 100 | 32 |  mae (loss): 0.0624, mse: 0.0076 |\n",
    "128, 64 | selu | 100 | 32 |  mae (loss): 0.0574, mse: 0.0065 |\n",
    "64, None | tanh | 100 | 32 |  mae (loss): 0.0713, mse: 0.0159 |\n",
    "128, None | tanh | 100 | 32 |  mae (loss): 0.0551, mse: 0.0061 |\n",
    "256, None | tanh | 100 | 32 |  mae (loss): 0.0523, mse: 0.0054 |\n",
    "64, 32 | tanh | 100 | 32 |  mae (loss): 0.0658, mse: 0.0085 |\n",
    "128, 64 | tanh | 100 | 32 |  mae (loss): 0.0585, mse: 0.0068 |\n",
    "64, None | relu | 500 | 32 |  mae (loss): 0.0510, mse: 0.0052 |\n",
    "128, None | relu | 500 | 32 |  mae (loss): 0.0469, mse: 0.0054 |\n",
    "256, None | relu | 500 | 32 |  mae (loss): 0.0430, mse: 0.0040 |\n",
    "64, 32 | relu | 500 | 32 |  mae (loss): 0.0505, mse: 0.0057 |\n",
    "128, 64 | relu | 500 | 32 |  mae (loss): 0.0457, mse: 0.0045 |\n",
    "64, None | selu | 500 | 32 |  mae (loss): 0.0477, mse: 0.0046 |\n",
    "128, None | selu | 500 | 32 |  mae (loss): 0.0452, mse: 0.0042 |\n",
    "256, None | selu | 500 | 32 |  mae (loss): 0.0452, mse: 0.0043 |\n",
    "64, 32 | selu | 500 | 32 |  mae (loss): 0.0472, mse: 0.0046 |\n",
    "128, 64 | selu | 500 | 32 |  mae (loss): 0.0449, mse: 0.0042 |\n",
    "64, None | tanh | 500 | 32 |  mae (loss): 0.0480, mse: 0.0048 |\n",
    "128, None | tanh | 500 | 32 |  mae (loss): 0.0466, mse: 0.0044 |\n",
    "256, None | tanh | 500 | 32 |  mae (loss): 0.0492, mse: 0.0049 |\n",
    "64, 32 | tanh | 500 | 32 |  mae (loss): 0.0483, mse: 0.0047 |\n",
    "128, 64 | tanh | 500 | 32 |  mae (loss): 0.0432, mse: 0.0038 |\n",
    "64, None | relu | 1000 | 32 |  mae (loss): 0.0479, mse: 0.0046 |\n",
    "128, None | relu | 1000 | 32 |  mae (loss): 0.0471, mse: 0.0053 |\n",
    "256, None | relu | 1000 | 32 |  mae (loss): 0.0428, mse: 0.0039 |\n",
    "64, 32 | relu | 1000 | 32 |  mae (loss): 0.0478, mse: 0.0050 |\n",
    "128, 64 | relu | 1000 | 32 |  mae (loss): 0.0420, mse: 0.0037 |\n",
    "64, None | selu | 1000 | 32 |  mae (loss): 0.0454, mse: 0.0043 |\n",
    "128, None | selu | 1000 | 32 |  mae (loss): 0.0434, mse: 0.0041 |\n",
    "256, None | selu | 1000 | 32 |  mae (loss): 0.0442, mse: 0.0040 |\n",
    "64, 32 | selu | 1000 | 32 |  mae (loss): 0.0426, mse: 0.0038 |\n",
    "128, 64 | selu | 1000 | 32 |  mae (loss): 0.0403, mse: 0.0035 |\n",
    "64, None | tanh | 1000 | 32 |  mae (loss): 0.0450, mse: 0.0042 |\n",
    "128, None | tanh | 1000 | 32 |  mae (loss): 0.0451, mse: 0.0042 |\n",
    "256, None | tanh | 1000 | 32 |  mae (loss): 0.0481, mse: 0.0047 |\n",
    "64, 32 | tanh | 1000 | 32 |  mae (loss): 0.0450, mse: 0.0050 |\n",
    "128, 64 | tanh | 1000 | 32 |  mae (loss): 0.0398, mse: 0.0032 |\n",
    "64, None | relu | 100 | 64 |  mae (loss): 0.0788, mse: 0.0136 |\n",
    "128, None | relu | 100 | 64 |  mae (loss): 0.0662, mse: 0.0096 |\n",
    "256, None | relu | 100 | 64 |  mae (loss): 0.0563, mse: 0.0064 |\n",
    "64, 32 | relu | 100 | 64 |  mae (loss): 0.0763, mse: 0.0123 |\n",
    "128, 64 | relu | 100 | 64 |  mae (loss): 0.0739, mse: 0.0113 |\n",
    "64, None | selu | 100 | 64 |  mae (loss): 0.0755, mse: 0.0113 |\n",
    "128, None | selu | 100 | 64 |  mae (loss): 0.0642, mse: 0.0086 |\n",
    "256, None | selu | 100 | 64 |  mae (loss): 0.0542, mse: 0.0060 |\n",
    "64, 32 | selu | 100 | 64 |  mae (loss): 0.0775, mse: 0.0119 |\n",
    "128, 64 | selu | 100 | 64 |  mae (loss): 0.0665, mse: 0.0084 |\n",
    "64, None | tanh | 100 | 64 |  mae (loss): 0.0792, mse: 0.0128 |\n",
    "128, None | tanh | 100 | 64 |  mae (loss): 0.0707, mse: 0.0103 |\n",
    "256, None | tanh | 100 | 64 |  mae (loss): 0.0552, mse: 0.0059 |\n",
    "64, 32 | tanh | 100 | 64 |  mae (loss): 0.0729, mse: 0.0106 |\n",
    "128, 64 | tanh | 100 | 64 |  mae (loss): 0.0658, mse: 0.0088 |\n",
    "64, None | relu | 500 | 64 |  mae (loss): 0.0581, mse: 0.0066 |\n",
    "128, None | relu | 500 | 64 |  mae (loss): 0.0475, mse: 0.0047 |\n",
    "256, None | relu | 500 | 64 |  mae (loss): 0.0442, mse: 0.0042 |\n",
    "64, 32 | relu | 500 | 64 |  mae (loss): 0.0565, mse: 0.0065 |\n",
    "128, 64 | relu | 500 | 64 |  mae (loss): 0.0479, mse: 0.0046 |\n",
    "64, None | selu | 500 | 64 |  mae (loss): 0.0501, mse: 0.0051 |\n",
    "128, None | selu | 500 | 64 |  mae (loss): 0.0467, mse: 0.0047 |\n",
    "256, None | selu | 500 | 64 |  mae (loss): 0.0434, mse: 0.0039 |\n",
    "64, 32 | selu | 500 | 64 |  mae (loss): 0.0516, mse: 0.0054 |\n",
    "128, 64 | selu | 500 | 64 |  mae (loss): 0.0457, mse: 0.0043 |\n",
    "64, None | tanh | 500 | 64 |  mae (loss): 0.0522, mse: 0.0065 |\n",
    "128, None | tanh | 500 | 64 |  mae (loss): 0.0473, mse: 0.0045 |\n",
    "256, None | tanh | 500 | 64 |  mae (loss): 0.0476, mse: 0.0047 |\n",
    "64, 32 | tanh | 500 | 64 |  mae (loss): 0.0522, mse: 0.0056 |\n",
    "128, 64 | tanh | 500 | 64 |  mae (loss): 0.0466, mse: 0.0044 |\n",
    "64, None | relu | 1000 | 64 |  mae (loss): 0.0522, mse: 0.0054 |\n",
    "128, None | relu | 1000 | 64 |  mae (loss): 0.0443, mse: 0.0044 |\n",
    "256, None | relu | 1000 | 64 |  mae (loss): 0.0416, mse: 0.0037 |\n",
    "64, 32 | relu | 1000 | 64 |  mae (loss): 0.0495, mse: 0.0051 |\n",
    "128, 64 | relu | 1000 | 64 |  mae (loss): 0.0420, mse: 0.0037 |\n",
    "64, None | selu | 1000 | 64 |  mae (loss): 0.0468, mse: 0.0045 |\n",
    "128, None | selu | 1000 | 64 |  mae (loss): 0.0442, mse: 0.0041 |\n",
    "256, None | selu | 1000 | 64 |  mae (loss): 0.0434, mse: 0.0039 |\n",
    "64, 32 | selu | 1000 | 64 |  mae (loss): 0.0464, mse: 0.0044 |\n",
    "128, 64 | selu | 1000 | 64 |  mae (loss): 0.0422, mse: 0.0037 |\n",
    "64, None | tanh | 1000 | 64 |  mae (loss): 0.0467, mse: 0.0044 |\n",
    "128, None | tanh | 1000 | 64 |  mae (loss): 0.0458, mse: 0.0043 |\n",
    "256, None | tanh | 1000 | 64 |  mae (loss): 0.0458, mse: 0.0042 |\n",
    "64, 32 | tanh | 1000 | 64 |  mae (loss): 0.0477, mse: 0.0051 |\n",
    "128, 64 | tanh | 1000 | 64 |  mae (loss): 0.0424, mse: 0.0037 |\n",
    "64, None | relu | 100 | 128 |  mae (loss): 0.5362, mse: 0.4794 |\n",
    "128, None | relu | 100 | 128 |  mae (loss): 0.0860, mse: 0.0189 |\n",
    "256, None | relu | 100 | 128 |  mae (loss): 0.0727, mse: 0.0107 |\n",
    "64, 32 | relu | 100 | 128 |  mae (loss): 0.6299, mse: 0.7711 |\n",
    "128, 64 | relu | 100 | 128 |  mae (loss): 0.5846, mse: 0.7124 |\n",
    "64, None | selu | 100 | 128 |  mae (loss): 0.1056, mse: 0.0221 |\n",
    "128, None | selu | 100 | 128 |  mae (loss): 0.5491, mse: 0.5668 |\n",
    "256, None | selu | 100 | 128 |  mae (loss): 0.5081, mse: 0.4727 |\n",
    "64, 32 | selu | 100 | 128 |  mae (loss): 0.1076, mse: 0.0236 |\n",
    "128, 64 | selu | 100 | 128 |  mae (loss): 0.0849, mse: 0.0151 |\n",
    "64, None | tanh | 100 | 128 |  mae (loss): 0.6166, mse: 0.7044 |\n",
    "128, None | tanh | 100 | 128 |  mae (loss): 0.0877, mse: 0.0154 |\n",
    "256, None | tanh | 100 | 128 |  mae (loss): 0.0678, mse: 0.0091 |\n",
    "64, 32 | tanh | 100 | 128 |  mae (loss): 0.0846, mse: 0.0137 |\n",
    "128, 64 | tanh | 100 | 128 |  mae (loss): 0.5282, mse: 0.4117 |\n",
    "64, None | relu | 500 | 128 |  mae (loss): 0.0635, mse: 0.0080 |\n",
    "128, None | relu | 500 | 128 |  mae (loss): 0.0565, mse: 0.0064 |\n",
    "256, None | relu | 500 | 128 |  mae (loss): 0.0471, mse: 0.0046 |\n",
    "64, 32 | relu | 500 | 128 |  mae (loss): 0.6312, mse: 0.8005 |\n",
    "128, 64 | relu | 500 | 128 |  mae (loss): 0.5723, mse: 0.7352 |\n",
    "64, None | selu | 500 | 128 |  mae (loss): 0.0589, mse: 0.0067 |\n",
    "128, None | selu | 500 | 128 |  mae (loss): 0.0518, mse: 0.0054 |\n",
    "256, None | selu | 500 | 128 |  mae (loss): 0.0455, mse: 0.0042 |\n",
    "64, 32 | selu | 500 | 128 |  mae (loss): 0.5830, mse: 0.5813 |\n",
    "128, 64 | selu | 500 | 128 |  mae (loss): 0.0513, mse: 0.0052 |\n",
    "64, None | tanh | 500 | 128 |  mae (loss): 0.5893, mse: 0.6127 |\n",
    "128, None | tanh | 500 | 128 |  mae (loss): 0.5595, mse: 0.6199 |\n",
    "256, None | tanh | 500 | 128 |  mae (loss): 0.5004, mse: 0.4036 |\n",
    "64, 32 | tanh | 500 | 128 |  mae (loss): 0.6168, mse: 0.5923 |\n",
    "128, 64 | tanh | 500 | 128 |  mae (loss): 0.0521, mse: 0.0055 |\n",
    "64, None | relu | 1000 | 128 |  mae (loss): 0.5693, mse: 0.5552 |\n",
    "128, None | relu | 1000 | 128 |  mae (loss): 0.0504, mse: 0.0052 |\n",
    "256, None | relu | 1000 | 128 |  mae (loss): 0.0432, mse: 0.0040 |\n",
    "64, 32 | relu | 1000 | 128 |  mae (loss): 0.6587, mse: 0.8470 |\n",
    "128, 64 | relu | 1000 | 128 |  mae (loss): 0.6132, mse: 0.7916 |\n",
    "64, None | selu | 1000 | 128 |  mae (loss): 0.5706, mse: 0.6322 |\n",
    "128, None | selu | 1000 | 128 |  mae (loss): 0.0460, mse: 0.0044 |\n",
    "256, None | selu | 1000 | 128 |  mae (loss): 0.0441, mse: 0.0040 |\n",
    "64, 32 | selu | 1000 | 128 |  mae (loss): 0.0503, mse: 0.0051 |\n",
    "128, 64 | selu | 1000 | 128 |  mae (loss): 0.0449, mse: 0.0041 |\n",
    "64, None | tanh | 1000 | 128 |  mae (loss): 0.5727, mse: 0.6228 |\n",
    "128, None | tanh | 1000 | 128 |  mae (loss): 0.5409, mse: 0.5551 |\n",
    "256, None | tanh | 1000 | 128 |  mae (loss): 0.0471, mse: 0.0045 |\n",
    "64, 32 | tanh | 1000 | 128 |  mae (loss): 0.6182, mse: 0.6194 |\n",
    "128, 64 | tanh | 1000 | 128 |  mae (loss): 0.5612, mse: 0.4772 |\n",
    "\n",
    "\n",
    "Best result: neurons=128, 64, activation=tanh, batch_size=32, epochs=1000, MAE=0.0398, MSE=0.0032"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAloigaG0xr4"
   },
   "source": [
    "### Nestorov Adam optimizer results\n",
    "\n",
    "- Before Feedback:  \n",
    "Nadam(lr=3e-4, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "\n",
    "| Activation Function  |\tEpochs    |  Batch Size  |\tTest set                 |\n",
    "|----------------------|------------|--------------|---------------------------|\n",
    "| relu                 | 100        | 16             | mse: 0.0035 - mae: 0.0409              | \n",
    "| relu                 | 100        | 32             | mse: 0.0033 - mae: 0.0394              | \n",
    "| relu                 | 100        | 64             | mse: 0.0035 - mae: 0.0405              | \n",
    "| relu                 | 100        | 128            | mse: 0.0041 - mae: 0.0442              |\n",
    "| **relu**             | **1000**   | **16**         | **mse: 0.0032 - mae: 0.0393**          | \n",
    "| relu                 | 1000       | 32             | mse: 0.0033 - mae: 0.0396              | \n",
    "| relu                 | 1000       | 64             | mse: 0.0036 - mae: 0.0412              | \n",
    "| relu                 | 1000       | 128            | mse: 0.0033 - mae: 0.0397              |\n",
    "| tanh                 | 100        | 16             | mse: 0.0045 - mae: 0.0464              | \n",
    "| tanh                 | 100        | 32             | mse: 0.0040 - mae: 0.0438              | \n",
    "| tanh                 | 100        | 64             | mse: 0.0042 - mae: 0.0450              | \n",
    "| tanh                 | 100        | 128            | mse: 0.0047 - mae: 0.0483              |\n",
    "| tanh                 | 1000       | 16             | mse: 0.0044 - mae: 0.0459              | \n",
    "| tanh                 | 1000       | 32             | mse: 0.0041 - mae: 0.0445              | \n",
    "| tanh                 | 1000       | 64             | mse: 0.0041 - mae: 0.0447              | \n",
    "| tanh                 | 1000       | 128            | mse: 0.0048 - mae: 0.0484              | \n",
    "| selu                 | 100        | 16             | mse: 0.0034 - mae: 0.0400              |\n",
    "| selu                 | 100        | 32             | mse: 0.0039 - mae: 0.0427              | \n",
    "| selu                 | 100        | 64             | mse: 0.0041 - mae: 0.0443              | \n",
    "| selu                 | 100        | 128            | mse: 0.0047 - mae: 0.0478              | \n",
    "| selu                 | 1000       | 16             | mse: 0.0041 - mae: 0.0443              |\n",
    "| selu                 | 1000       | 32             | mse: 0.0039 - mae: 0.0430              | \n",
    "| selu                 | 1000       | 64             | mse: 0.0041 - mae: 0.0444              | \n",
    "| selu                 | 1000       | 128            | mse: 0.0046 - mae: 0.0473              | \n",
    "\n",
    "- Before Feedback with BatchNormalization()  \n",
    "Nadam(lr=3e-4, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "| Activation Function  |\tEpochs    |  Batch Size  |\tTest set                 |\n",
    "|----------------------|------------|--------------|---------------------------|\n",
    "| relu                 | 100        | 16             | mse: 0.0074 - mae: 0.0679              | \n",
    "| relu                 | 100        | 32             | mse: 0.0050 - mae: 0.0526              | \n",
    "| relu                 | 100        | 64             | mse: 0.0060 - mae: 0.0597              | \n",
    "| **relu**                 | **100**        | **128**            | **mse: 0.0041 - mae: 0.0472**              |\n",
    "| tanh                 | 100        | 16             | mse: 0.0054 - mae: 0.0537              |\n",
    "| tanh                 | 100        | 32             | mse: 0.0071 - mae: 0.0663              | \n",
    "| tanh                 | 100        | 64             | mse: 0.0059 - mae: 0.0584              | \n",
    "| tanh                 | 100        | 128            | mse: 0.0061 - mae: 0.0596              |\n",
    "| selu                 | 100        | 16             | mse: 0.0064 - mae: 0.0614              |\n",
    "| selu                 | 100        | 32             | mse: 0.0050 - mae: 0.0534              | \n",
    "| selu                 | 100        | 64             | mse: 0.0129 - mae: 0.0910              | \n",
    "| selu                 | 100        | 128            | mse: 0.0103 - mae: 0.0823              | \n",
    "\n",
    "\n",
    "- After Feedback:  \n",
    "Nadam(lr=1e-4, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "| Neurons | Activation function | Epochs | Batch size | Results on test set|\n",
    "|---------|---------------------|--------|------------|--------------------|\n",
    "64, None | | relu | 100 | 16 |  mae (loss): 0.0555, mse: 0.0054 |\n",
    "128, None | | relu | 100 | 16 |  mae (loss): 0.0685, mse: 0.0082 |\n",
    "256, None | | relu | 100 | 16 |  mae (loss): 0.0861, mse: 0.0126 |\n",
    "64, 32 | | relu | 100 | 16 |  mae (loss): 0.0605, mse: 0.0067 |\n",
    "128, 64 | | relu | 100 | 16 |  mae (loss): 0.0538, mse: 0.0052 |\n",
    "64, None | | selu | 100 | 16 |  mae (loss): 0.0866, mse: 0.0119 |\n",
    "128, None | | selu | 100 | 16 |  mae (loss): 0.0691, mse: 0.0080 |\n",
    "256, None | | selu | 100 | 16 |  mae (loss): 0.0897, mse: 0.0131 |\n",
    "64, 32 | | selu | 100 | 16 |  mae (loss): 0.0564, mse: 0.0059 |\n",
    "128, 64 | | selu | 100 | 16 |  mae (loss): 0.0490, mse: 0.0043 |\n",
    "64, None | | tanh | 100 | 16 |  mae (loss): 0.0557, mse: 0.0061 |\n",
    "128, None | | tanh | 100 | 16 |  mae (loss): 0.0809, mse: 0.0110 |\n",
    "256, None | | tanh | 100 | 16 |  mae (loss): 0.0752, mse: 0.0104 |\n",
    "64, 32 | | tanh | 100 | 16 |  mae (loss): 0.0823, mse: 0.0120 |\n",
    "128, 64 | | tanh | 100 | 16 |  mae (loss): 0.0750, mse: 0.0097 |\n",
    "64, None | | relu | 500 | 16 |  mae (loss): 0.0653, mse: 0.0089 |\n",
    "128, None | | relu | 500 | 16 |  mae (loss): 0.0967, mse: 0.0150 |\n",
    "256, None | | relu | 500 | 16 |  mae (loss): 0.0697, mse: 0.0086 |\n",
    "64, 32 | | relu | 500 | 16 |  mae (loss): 0.0582, mse: 0.0063 |\n",
    "128, 64 | | relu | 500 | 16 |  mae (loss): 0.0489, mse: 0.0047 |\n",
    "64, None | | selu | 500 | 16 |  mae (loss): 0.0901, mse: 0.0132 |\n",
    "128, None | | selu | 500 | 16 |  mae (loss): 0.0701, mse: 0.0084 |\n",
    "256, None | | selu | 500 | 16 |  mae (loss): 0.0831, mse: 0.0115 |\n",
    "64, 32 | | selu | 500 | 16 |  mae (loss): 0.0740, mse: 0.0095 |\n",
    "128, 64 | | selu | 500 | 16 |  mae (loss): 0.0729, mse: 0.0089 |\n",
    "64, None | | tanh | 500 | 16 |  mae (loss): 0.0725, mse: 0.0091 |\n",
    "128, None | | tanh | 500 | 16 |  mae (loss): 0.0824, mse: 0.0115 |\n",
    "256, None | | tanh | 500 | 16 |  mae (loss): 0.0962, mse: 0.0157 |\n",
    "64, 32 | | tanh | 500 | 16 |  mae (loss): 0.0587, mse: 0.0063 |\n",
    "128, 64 | | tanh | 500 | 16 |  mae (loss): 0.0813, mse: 0.0112 |\n",
    "64, None | | relu | 1000 | 16 |  mae (loss): 0.0611, mse: 0.0065 |\n",
    "128, None | | relu | 1000 | 16 |  mae (loss): 0.0913, mse: 0.0138 |\n",
    "256, None | | relu | 1000 | 16 |  mae (loss): 0.0935, mse: 0.0152 |\n",
    "64, 32 | | relu | 1000 | 16 |  mae (loss): 0.0514, mse: 0.0052 |\n",
    "128, 64 | | relu | 1000 | 16 |  mae (loss): 0.0635, mse: 0.0071 |\n",
    "64, None | | selu | 1000 | 16 |  mae (loss): 0.1038, mse: 0.0176 |\n",
    "128, None | | selu | 1000 | 16 |  mae (loss): 0.0871, mse: 0.0121 |\n",
    "256, None | | selu | 1000 | 16 |  mae (loss): 0.0784, mse: 0.0097 |\n",
    "64, 32 | | selu | 1000 | 16 |  mae (loss): 0.0638, mse: 0.0069 |\n",
    "128, 64 | | selu | 1000 | 16 |  mae (loss): 0.0634, mse: 0.0066 |\n",
    "64, None | | tanh | 1000 | 16 |  mae (loss): 0.0945, mse: 0.0148 |\n",
    "128, None | | tanh | 1000 | 16 |  mae (loss): 0.0785, mse: 0.0106 |\n",
    "256, None | | tanh | 1000 | 16 |  mae (loss): 0.1204, mse: 0.0244 |\n",
    "64, 32 | | tanh | 1000 | 16 |  mae (loss): 0.0613, mse: 0.0078 |\n",
    "128, 64 | | tanh | 1000 | 16 |  mae (loss): 0.0743, mse: 0.0089 |\n",
    "64, None | | relu | 100 | 32 |  mae (loss): 0.0526, mse: 0.0063 |\n",
    "128, None | | relu | 100 | 32 |  mae (loss): 0.0528, mse: 0.0052 |\n",
    "256, None | | relu | 100 | 32 |  mae (loss): 0.0865, mse: 0.0118 |\n",
    "64, 32 | | relu | 100 | 32 |  mae (loss): 0.0456, mse: 0.0042 |\n",
    "128, 64 | | relu | 100 | 32 |  mae (loss): 0.0455, mse: 0.0037 |\n",
    "64, None | | selu | 100 | 32 |  mae (loss): 0.0701, mse: 0.0082 |\n",
    "128, None | | selu | 100 | 32 |  mae (loss): 0.0616, mse: 0.0066 |\n",
    "256, None | | selu | 100 | 32 |  mae (loss): 0.1230, mse: 0.0251 |\n",
    "64, 32 | | selu | 100 | 32 |  mae (loss): 0.0443, mse: 0.0036 |\n",
    "128, 64 | | selu | 100 | 32 |  mae (loss): 0.0551, mse: 0.0053 |\n",
    "64, None | | tanh | 100 | 32 |  mae (loss): 0.0574, mse: 0.0059 |\n",
    "128, None | | tanh | 100 | 32 |  mae (loss): 0.0583, mse: 0.0059 |\n",
    "256, None | | tanh | 100 | 32 |  mae (loss): 0.0803, mse: 0.0111 |\n",
    "64, 32 | | tanh | 100 | 32 |  mae (loss): 0.0644, mse: 0.0073 |\n",
    "128, 64 | | tanh | 100 | 32 |  mae (loss): 0.0522, mse: 0.0048 |\n",
    "64, None | | relu | 500 | 32 |  mae (loss): 0.0509, mse: 0.0054 |\n",
    "128, None | | relu | 500 | 32 |  mae (loss): 0.0540, mse: 0.0054 |\n",
    "256, None | | relu | 500 | 32 |  mae (loss): 0.0700, mse: 0.0079 |\n",
    "64, 32 | | relu | 500 | 32 |  mae (loss): 0.0592, mse: 0.0060 |\n",
    "128, 64 | | relu | 500 | 32 |  mae (loss): 0.0457, mse: 0.0040 |\n",
    "64, None | | selu | 500 | 32 |  mae (loss): 0.0797, mse: 0.0102 |\n",
    "128, None | | selu | 500 | 32 |  mae (loss): 0.0596, mse: 0.0060 |\n",
    "256, None | | selu | 500 | 32 |  mae (loss): 0.0899, mse: 0.0128 |\n",
    "64, 32 | | selu | 500 | 32 |  mae (loss): 0.0492, mse: 0.0044 |\n",
    "128, 64 | | selu | 500 | 32 |  mae (loss): 0.0551, mse: 0.0052 |\n",
    "64, None | | tanh | 500 | 32 |  mae (loss): 0.0566, mse: 0.0063 |\n",
    "128, None | | tanh | 500 | 32 |  mae (loss): 0.0650, mse: 0.0071 |\n",
    "256, None | | tanh | 500 | 32 |  mae (loss): 0.1004, mse: 0.0169 |\n",
    "64, 32 | | tanh | 500 | 32 |  mae (loss): 0.0500, mse: 0.0045 |\n",
    "128, 64 | | tanh | 500 | 32 |  mae (loss): 0.0735, mse: 0.0095 |\n",
    "64, None | | relu | 1000 | 32 |  mae (loss): 0.0575, mse: 0.0066 |\n",
    "128, None | | relu | 1000 | 32 |  mae (loss): 0.0687, mse: 0.0080 |\n",
    "256, None | | relu | 1000 | 32 |  mae (loss): 0.0662, mse: 0.0073 |\n",
    "64, 32 | | relu | 1000 | 32 |  mae (loss): 0.0514, mse: 0.0049 |\n",
    "128, 64 | | relu | 1000 | 32 |  mae (loss): 0.0439, mse: 0.0036 |\n",
    "64, None | | selu | 1000 | 32 |  mae (loss): 0.0691, mse: 0.0080 |\n",
    "128, None | | selu | 1000 | 32 |  mae (loss): 0.0701, mse: 0.0076 |\n",
    "256, None | | selu | 1000 | 32 |  mae (loss): 0.1051, mse: 0.0167 |\n",
    "64, 32 | | selu | 1000 | 32 |  mae (loss): 0.0590, mse: 0.0061 |\n",
    "128, 64 | | selu | 1000 | 32 |  mae (loss): 0.0613, mse: 0.0063 |\n",
    "64, None | | tanh | 1000 | 32 |  mae (loss): 0.0661, mse: 0.0072 |\n",
    "128, None | | tanh | 1000 | 32 |  mae (loss): 0.0656, mse: 0.0075 |\n",
    "256, None | | tanh | 1000 | 32 |  mae (loss): 0.0959, mse: 0.0148 |\n",
    "64, 32 | | tanh | 1000 | 32 |  mae (loss): 0.0548, mse: 0.0055 |\n",
    "128, 64 | | tanh | 1000 | 32 |  mae (loss): 0.0603, mse: 0.0060 |\n",
    "64, None | | relu | 100 | 64 |  mae (loss): 0.0582, mse: 0.0061 |\n",
    "128, None | | relu | 100 | 64 |  mae (loss): 0.0576, mse: 0.0057 |\n",
    "256, None | | relu | 100 | 64 |  mae (loss): 0.0542, mse: 0.0051 |\n",
    "64, 32 | | relu | 100 | 64 |  mae (loss): 0.0424, mse: 0.0036 |\n",
    "128, 64 | | relu | 100 | 64 |  mae (loss): 0.0509, mse: 0.0048 |\n",
    "64, None | | selu | 100 | 64 |  mae (loss): 0.0614, mse: 0.0064 |\n",
    "128, None | | selu | 100 | 64 |  mae (loss): 0.0685, mse: 0.0076 |\n",
    "256, None | | selu | 100 | 64 |  mae (loss): 0.0801, mse: 0.0100 |\n",
    "64, 32 | | selu | 100 | 64 |  mae (loss): 0.0459, mse: 0.0040 |\n",
    "128, 64 | | selu | 100 | 64 |  mae (loss): 0.0614, mse: 0.0063 |\n",
    "64, None | | tanh | 100 | 64 |  mae (loss): 0.0741, mse: 0.0091 |\n",
    "128, None | | tanh | 100 | 64 |  mae (loss): 0.0623, mse: 0.0063 |\n",
    "256, None | | tanh | 100 | 64 |  mae (loss): 0.1688, mse: 0.0380 |\n",
    "64, 32 | | tanh | 100 | 64 |  mae (loss): 0.0625, mse: 0.0065 |\n",
    "128, 64 | | tanh | 100 | 64 |  mae (loss): 0.0846, mse: 0.0112 |\n",
    "64, None | | relu | 500 | 64 |  mae (loss): 0.0598, mse: 0.0062 |\n",
    "128, None | | relu | 500 | 64 |  mae (loss): 0.0549, mse: 0.0054 |\n",
    "256, None | | relu | 500 | 64 |  mae (loss): 0.0595, mse: 0.0058 |\n",
    "64, 32 | | relu | 500 | 64 |  mae (loss): 0.0436, mse: 0.0037 |\n",
    "128, 64 | | relu | 500 | 64 |  mae (loss): 0.0518, mse: 0.0048 |\n",
    "64, None | | selu | 500 | 64 |  mae (loss): 0.0575, mse: 0.0058 |\n",
    "128, None | | selu | 500 | 64 |  mae (loss): 0.0769, mse: 0.0094 |\n",
    "256, None | | selu | 500 | 64 |  mae (loss): 0.0658, mse: 0.0075 |\n",
    "64, 32 | | selu | 500 | 64 |  mae (loss): 0.0497, mse: 0.0046 |\n",
    "128, 64 | | selu | 500 | 64 |  mae (loss): 0.0588, mse: 0.0060 |\n",
    "64, None | | tanh | 500 | 64 |  mae (loss): 0.0513, mse: 0.0049 |\n",
    "128, None | | tanh | 500 | 64 |  mae (loss): 0.1056, mse: 0.0166 |\n",
    "256, None | | tanh | 500 | 64 |  mae (loss): 0.0700, mse: 0.0079 |\n",
    "64, 32 | | tanh | 500 | 64 |  mae (loss): 0.0548, mse: 0.0053 |\n",
    "128, 64 | | tanh | 500 | 64 |  mae (loss): 0.0724, mse: 0.0087 |\n",
    "64, None | | relu | 1000 | 64 |  mae (loss): 0.0518, mse: 0.0050 |\n",
    "128, None | | relu | 1000 | 64 |  mae (loss): 0.0471, mse: 0.0043 |\n",
    "256, None | | relu | 1000 | 64 |  mae (loss): 0.0679, mse: 0.0077 |\n",
    "64, 32 | | relu | 1000 | 64 |  mae (loss): 0.0530, mse: 0.0063 |\n",
    "128, 64 | | relu | 1000 | 64 |  mae (loss): 0.0476, mse: 0.0043 |\n",
    "64, None | | selu | 1000 | 64 |  mae (loss): 0.0565, mse: 0.0055 |\n",
    "128, None | | selu | 1000 | 64 |  mae (loss): 0.0725, mse: 0.0089 |\n",
    "256, None | | selu | 1000 | 64 |  mae (loss): 0.0794, mse: 0.0108 |\n",
    "64, 32 | | selu | 1000 | 64 |  mae (loss): 0.0483, mse: 0.0043 |\n",
    "128, 64 | | selu | 1000 | 64 |  mae (loss): 0.0614, mse: 0.0061 |\n",
    "64, None | | tanh | 1000 | 64 |  mae (loss): 0.0524, mse: 0.0050 |\n",
    "128, None | | tanh | 1000 | 64 |  mae (loss): 0.0718, mse: 0.0082 |\n",
    "256, None | | tanh | 1000 | 64 |  mae (loss): 0.0701, mse: 0.0089 |\n",
    "64, 32 | | tanh | 1000 | 64 |  mae (loss): 0.0493, mse: 0.0044 |\n",
    "128, 64 | | tanh | 1000 | 64 |  mae (loss): 0.0667, mse: 0.0073 |\n",
    "64, None | | relu | 100 | 128 |  mae (loss): 0.0513, mse: 0.0049 |\n",
    "128, None | | relu | 100 | 128 |  mae (loss): 0.0554, mse: 0.0055 |\n",
    "256, None | | relu | 100 | 128 |  mae (loss): 0.0823, mse: 0.0102 |\n",
    "64, 32 | | relu | 100 | 128 |  mae (loss): 0.0422, mse: 0.0035 |\n",
    "128, 64 | | relu | 100 | 128 |  mae (loss): 0.0440, mse: 0.0036 |\n",
    "64, None | | selu | 100 | 128 |  mae (loss): 0.0556, mse: 0.0055 |\n",
    "128, None | | selu | 100 | 128 |  mae (loss): 0.0767, mse: 0.0097 |\n",
    "256, None | | selu | 100 | 128 |  mae (loss): 0.1208, mse: 0.0215 |\n",
    "64, 32 | | selu | 100 | 128 |  mae (loss): 0.0509, mse: 0.0047 |\n",
    "128, 64 | | selu | 100 | 128 |  mae (loss): 0.0446, mse: 0.0036 |\n",
    "64, None | | tanh | 100 | 128 |  mae (loss): 0.0534, mse: 0.0051 |\n",
    "128, None | | tanh | 100 | 128 |  mae (loss): 0.0577, mse: 0.0056 |\n",
    "256, None | | tanh | 100 | 128 |  mae (loss): 0.2153, mse: 0.0647 |\n",
    "64, 32 | | tanh | 100 | 128 |  mae (loss): 0.0511, mse: 0.0045 |\n",
    "128, 64 | | tanh | 100 | 128 |  mae (loss): 0.0667, mse: 0.0077 |\n",
    "64, None | | relu | 500 | 128 |  mae (loss): 0.0522, mse: 0.0052 |\n",
    "128, None | | relu | 500 | 128 |  mae (loss): 0.0487, mse: 0.0045 |\n",
    "256, None | | relu | 500 | 128 |  mae (loss): 0.0994, mse: 0.0151 |\n",
    "64, 32 | | relu | 500 | 128 |  mae (loss): 0.0431, mse: 0.0037 |\n",
    "128, 64 | | relu | 500 | 128 |  mae (loss): 0.0476, mse: 0.0042 |\n",
    "64, None | | selu | 500 | 128 |  mae (loss): 0.0628, mse: 0.0068 |\n",
    "128, None | | selu | 500 | 128 |  mae (loss): 0.0992, mse: 0.0148 |\n",
    "256, None | | selu | 500 | 128 |  mae (loss): 0.1198, mse: 0.0215 |\n",
    "64, 32 | | selu | 500 | 128 |  mae (loss): 0.0609, mse: 0.0063 |\n",
    "128, 64 | | selu | 500 | 128 |  mae (loss): 0.0534, mse: 0.0050 |\n",
    "64, None | | tanh | 500 | 128 |  mae (loss): 0.0478, mse: 0.0043 |\n",
    "128, None | | tanh | 500 | 128 |  mae (loss): 0.0680, mse: 0.0079 |\n",
    "256, None | | tanh | 500 | 128 |  mae (loss): 0.1204, mse: 0.0228 |\n",
    "64, 32 | | tanh | 500 | 128 |  mae (loss): 0.0629, mse: 0.0066 |\n",
    "128, 64 | | tanh | 500 | 128 |  mae (loss): 0.0648, mse: 0.0072 |\n",
    "64, None | | relu | 1000 | 128 |  mae (loss): 0.0542, mse: 0.0054 |\n",
    "128, None | | relu | 1000 | 128 |  mae (loss): 0.0512, mse: 0.0050 |\n",
    "256, None | | relu | 1000 | 128 |  mae (loss): 0.0640, mse: 0.0068 |\n",
    "64, 32 | | relu | 1000 | 128 |  mae (loss): 0.0441, mse: 0.0039 |\n",
    "128, 64 | | relu | 1000 | 128 |  mae (loss): 0.0491, mse: 0.0046 |\n",
    "64, None | | selu | 1000 | 128 |  mae (loss): 0.0534, mse: 0.0051 |\n",
    "128, None | | selu | 1000 | 128 |  mae (loss): 0.0876, mse: 0.0120 |\n",
    "256, None | | selu | 1000 | 128 |  mae (loss): 0.0907, mse: 0.0126 |\n",
    "64, 32 | | selu | 1000 | 128 |  mae (loss): 0.0493, mse: 0.0044 |\n",
    "128, 64 | | selu | 1000 | 128 |  mae (loss): 0.0523, mse: 0.0049 |\n",
    "64, None | | tanh | 1000 | 128 |  mae (loss): 0.0535, mse: 0.0053 |\n",
    "128, None | | tanh | 1000 | 128 |  mae (loss): 0.0773, mse: 0.0097 |\n",
    "256, None | | tanh | 1000 | 128 |  mae (loss): 0.2118, mse: 0.0652 |\n",
    "64, 32 | | tanh | 1000 | 128 |  mae (loss): 0.0541, mse: 0.0050 |\n",
    "128, 64 | | tanh | 1000 | 128 |  mae (loss): 0.0552, mse: 0.0053 |\n",
    "\n",
    "\n",
    "Best result: neurons=64, 32, activation=relu, batch_size=128, epochs=100, MAE=0.0422, MSE=0.0035"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
