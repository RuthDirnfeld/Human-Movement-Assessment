{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj6mAwdYGfWk"
   },
   "source": [
    "## Sprint 9 - updated version\n",
    "\n",
    "**-------Fixes for sprint 9-------**\n",
    "\n",
    "After getting a feedback, we applied suggested fixes for sprint 9 deep learning part:\n",
    "\n",
    "1.   Selected all the data (instead of a portion of it)\n",
    "2.   Scaled the data\n",
    "3.   Ran experiments with more than one activation functions (included SELU and tanh)\n",
    "4.   Tried out different batch sizes\n",
    "5.   Avoided larger networks, as they do not seem to improve the performance of the model\n",
    "6.   Included batch normalization\n",
    "\n",
    "After these changes were applied, we executed experiments again. The following parameters and options were tested:\n",
    "\n",
    "* batch_sizes  - 16, 32, 64, 128\n",
    "* epochs  -100, 500, 1000\n",
    "* activations - 'relu', 'selu', 'tanh'\n",
    "* architecture (neurons in each layer) - 64; 128; 256; (64, 32); (128, 64)\n",
    "\n",
    "**Deep Learning Steps**\n",
    "\n",
    "*1.Load Kinect movement data: Kinect frame sequences*\n",
    "\n",
    "All the provided Kinect frame sequences were loaded into a single dataset. Then, all the data was randomized. All the X and Y coordinates were selected as features while all the Z coordinates - as labels. Then features and labels were scaled.\n",
    "\n",
    "\n",
    "*2.Define a Deep Learning network (model)*\n",
    "\n",
    "Our deep learning network consists of one input layer with an input shape of 26 (same as the number of features) and an output layer with 13 neurons (this is the number of the outputs - Z coordinates). In addition, we experimented by adding one or several hidden layers and changing their number of neurons to see if the model performs better.\n",
    "\n",
    "*3.Compile the DL model*\n",
    "\n",
    "We used mean absolute error loss function and experimented with Adam, Stochastic gradient descent and Nadam optimizers. The metrics we used to evaluate model were mean absolute error and mean squared error.\n",
    "\n",
    "*4.Split training and testing sets*\n",
    "\n",
    "The selected data sample was split into 3 parts: 70% training set, 20% validation set and 10% testing set. \n",
    "\n",
    "*5.Train the DL model*\n",
    "\n",
    "For training the model, we used early stopping based on the validation set. This helped the model not overfit the training set. On top of that, we used 100, 500 and 1000 epochs for all optimizers when testing. All tests were performed with 16, 32, 64, 128 respectively and activation functions ReLU, SELU, Tanh.   \n",
    "\n",
    "*6.Evaluate DL model predictions*\n",
    "\n",
    "evaluate() method was used to evaluate the model. We ran this method on both, test and training sets to see if the model performed much worse on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXcdAPd8-AkE"
   },
   "source": [
    "### Adam optimizer\n",
    " \n",
    "The best result with Adam optimizer was a model with 2 hidden layers, first layer having 128 neurons and second - 64, the activation function ReLU and a learning rate of 1e-4. The batch size in the best result is 128 and 500 epochs. This resulted in the mean absolute error being 0.0382 and mean squared error being 0.0028.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuO7TIDdqjTd"
   },
   "source": [
    "### Stochastic gradient descent optimizer\n",
    "\n",
    "The best result with SGD optimizer was a model with 2 hidden layers, first layer having 128 neurons and second - 64, the activation function Tanh and a learning rate of 1e-4. The batch size in the best result is 64 and 1000 epochs. This resulted in the mean absolute error being 0.0446 and mean squared error being 0.0040.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdPYxLmuvulc"
   },
   "source": [
    "### Nestorov Adam optimizer\n",
    "\n",
    "The table at the end of the document show the results of all tests. The best result with Nadam optimizer was a model with 2 hidden layers, first layer having 128 neurons and second - 64, the activation function ReLu and a learning rate of 1e-4. The batch size in the best result is 32 and 1000 epochs. This resulted in the mean absolute error being 0.0384 and mean squared error being 0.0028.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rRyD-RT7wnl"
   },
   "source": [
    "From the automated tests it seems the the best architecture and the best parameters for the model is:\n",
    "- Adam optimizer (*Adam(learning_rate = 1e-4)*)\n",
    "- fitted with batch size of 128, 500 epochs\n",
    "- 2 hidden layers, 128 and 64 neurons with the ReLU activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-kElBzMx4I9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "_QHqLFgS2fnZ",
    "outputId": "cb4335dd-9ce1-415a-a3ca-99d7b2629eea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24005, 40)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "for files in glob.glob('*_kinect.csv'):\n",
    "  d = pd.read_csv(files)\n",
    "  data = pd.concat([data,d],axis=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "MbTGNpI64_mg",
    "outputId": "ed420ce9-eaaa-47e9-960c-34b57f070407"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FrameNo</th>\n",
       "      <th>head_x</th>\n",
       "      <th>head_y</th>\n",
       "      <th>head_z</th>\n",
       "      <th>left_shoulder_x</th>\n",
       "      <th>left_shoulder_y</th>\n",
       "      <th>left_shoulder_z</th>\n",
       "      <th>left_elbow_x</th>\n",
       "      <th>left_elbow_y</th>\n",
       "      <th>left_elbow_z</th>\n",
       "      <th>right_shoulder_x</th>\n",
       "      <th>right_shoulder_y</th>\n",
       "      <th>right_shoulder_z</th>\n",
       "      <th>right_elbow_x</th>\n",
       "      <th>right_elbow_y</th>\n",
       "      <th>right_elbow_z</th>\n",
       "      <th>left_hand_x</th>\n",
       "      <th>left_hand_y</th>\n",
       "      <th>left_hand_z</th>\n",
       "      <th>right_hand_x</th>\n",
       "      <th>right_hand_y</th>\n",
       "      <th>right_hand_z</th>\n",
       "      <th>left_hip_x</th>\n",
       "      <th>left_hip_y</th>\n",
       "      <th>left_hip_z</th>\n",
       "      <th>right_hip_x</th>\n",
       "      <th>right_hip_y</th>\n",
       "      <th>right_hip_z</th>\n",
       "      <th>left_knee_x</th>\n",
       "      <th>left_knee_y</th>\n",
       "      <th>left_knee_z</th>\n",
       "      <th>right_knee_x</th>\n",
       "      <th>right_knee_y</th>\n",
       "      <th>right_knee_z</th>\n",
       "      <th>left_foot_x</th>\n",
       "      <th>left_foot_y</th>\n",
       "      <th>left_foot_z</th>\n",
       "      <th>right_foot_x</th>\n",
       "      <th>right_foot_y</th>\n",
       "      <th>right_foot_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>-0.000362</td>\n",
       "      <td>0.77182</td>\n",
       "      <td>0.038403</td>\n",
       "      <td>-0.14051</td>\n",
       "      <td>0.55397</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>-0.22268</td>\n",
       "      <td>0.76110</td>\n",
       "      <td>-0.041707</td>\n",
       "      <td>0.14028</td>\n",
       "      <td>0.53996</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.21296</td>\n",
       "      <td>0.75192</td>\n",
       "      <td>-0.045553</td>\n",
       "      <td>-0.26133</td>\n",
       "      <td>0.97581</td>\n",
       "      <td>-0.11709</td>\n",
       "      <td>0.25002</td>\n",
       "      <td>0.96937</td>\n",
       "      <td>-0.11229</td>\n",
       "      <td>-0.070782</td>\n",
       "      <td>0.058053</td>\n",
       "      <td>-0.036604</td>\n",
       "      <td>0.073053</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>-0.030421</td>\n",
       "      <td>-0.12288</td>\n",
       "      <td>-0.35631</td>\n",
       "      <td>-0.048340</td>\n",
       "      <td>0.11515</td>\n",
       "      <td>-0.38198</td>\n",
       "      <td>-0.027392</td>\n",
       "      <td>-0.12338</td>\n",
       "      <td>-0.65977</td>\n",
       "      <td>-0.050811</td>\n",
       "      <td>0.12554</td>\n",
       "      <td>-0.68488</td>\n",
       "      <td>-0.057272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>-0.000749</td>\n",
       "      <td>0.77233</td>\n",
       "      <td>0.037489</td>\n",
       "      <td>-0.14042</td>\n",
       "      <td>0.55524</td>\n",
       "      <td>0.018899</td>\n",
       "      <td>-0.22249</td>\n",
       "      <td>0.76242</td>\n",
       "      <td>-0.041701</td>\n",
       "      <td>0.14023</td>\n",
       "      <td>0.53996</td>\n",
       "      <td>0.012913</td>\n",
       "      <td>0.21235</td>\n",
       "      <td>0.75195</td>\n",
       "      <td>-0.045572</td>\n",
       "      <td>-0.26108</td>\n",
       "      <td>0.97718</td>\n",
       "      <td>-0.11708</td>\n",
       "      <td>0.24824</td>\n",
       "      <td>0.96951</td>\n",
       "      <td>-0.11235</td>\n",
       "      <td>-0.070950</td>\n",
       "      <td>0.059132</td>\n",
       "      <td>-0.036741</td>\n",
       "      <td>0.072963</td>\n",
       "      <td>0.053121</td>\n",
       "      <td>-0.030419</td>\n",
       "      <td>-0.12286</td>\n",
       "      <td>-0.35630</td>\n",
       "      <td>-0.048592</td>\n",
       "      <td>0.11517</td>\n",
       "      <td>-0.38089</td>\n",
       "      <td>-0.027927</td>\n",
       "      <td>-0.12338</td>\n",
       "      <td>-0.65973</td>\n",
       "      <td>-0.050477</td>\n",
       "      <td>0.12547</td>\n",
       "      <td>-0.68383</td>\n",
       "      <td>-0.057571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>-0.001093</td>\n",
       "      <td>0.77294</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>-0.14035</td>\n",
       "      <td>0.55651</td>\n",
       "      <td>0.018697</td>\n",
       "      <td>-0.22230</td>\n",
       "      <td>0.76374</td>\n",
       "      <td>-0.041695</td>\n",
       "      <td>0.14025</td>\n",
       "      <td>0.53996</td>\n",
       "      <td>0.012135</td>\n",
       "      <td>0.21187</td>\n",
       "      <td>0.75195</td>\n",
       "      <td>-0.045587</td>\n",
       "      <td>-0.26079</td>\n",
       "      <td>0.97849</td>\n",
       "      <td>-0.11708</td>\n",
       "      <td>0.24661</td>\n",
       "      <td>0.96951</td>\n",
       "      <td>-0.11240</td>\n",
       "      <td>-0.071111</td>\n",
       "      <td>0.060158</td>\n",
       "      <td>-0.036746</td>\n",
       "      <td>0.072893</td>\n",
       "      <td>0.054178</td>\n",
       "      <td>-0.030421</td>\n",
       "      <td>-0.12281</td>\n",
       "      <td>-0.35627</td>\n",
       "      <td>-0.048846</td>\n",
       "      <td>0.11518</td>\n",
       "      <td>-0.37993</td>\n",
       "      <td>-0.028485</td>\n",
       "      <td>-0.12338</td>\n",
       "      <td>-0.65970</td>\n",
       "      <td>-0.050198</td>\n",
       "      <td>0.12542</td>\n",
       "      <td>-0.68291</td>\n",
       "      <td>-0.057858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57</td>\n",
       "      <td>-0.001415</td>\n",
       "      <td>0.77310</td>\n",
       "      <td>0.036013</td>\n",
       "      <td>-0.14035</td>\n",
       "      <td>0.55701</td>\n",
       "      <td>0.018478</td>\n",
       "      <td>-0.22208</td>\n",
       "      <td>0.76445</td>\n",
       "      <td>-0.041688</td>\n",
       "      <td>0.14026</td>\n",
       "      <td>0.53982</td>\n",
       "      <td>0.011722</td>\n",
       "      <td>0.21146</td>\n",
       "      <td>0.75203</td>\n",
       "      <td>-0.045600</td>\n",
       "      <td>-0.26043</td>\n",
       "      <td>0.97905</td>\n",
       "      <td>-0.11706</td>\n",
       "      <td>0.24499</td>\n",
       "      <td>0.96951</td>\n",
       "      <td>-0.11245</td>\n",
       "      <td>-0.071132</td>\n",
       "      <td>0.060480</td>\n",
       "      <td>-0.036747</td>\n",
       "      <td>0.072892</td>\n",
       "      <td>0.054551</td>\n",
       "      <td>-0.030410</td>\n",
       "      <td>-0.12278</td>\n",
       "      <td>-0.35605</td>\n",
       "      <td>-0.049004</td>\n",
       "      <td>0.11520</td>\n",
       "      <td>-0.37956</td>\n",
       "      <td>-0.029059</td>\n",
       "      <td>-0.12339</td>\n",
       "      <td>-0.65947</td>\n",
       "      <td>-0.050053</td>\n",
       "      <td>0.12543</td>\n",
       "      <td>-0.68254</td>\n",
       "      <td>-0.058122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58</td>\n",
       "      <td>-0.001701</td>\n",
       "      <td>0.77322</td>\n",
       "      <td>0.035327</td>\n",
       "      <td>-0.14034</td>\n",
       "      <td>0.55748</td>\n",
       "      <td>0.018235</td>\n",
       "      <td>-0.22194</td>\n",
       "      <td>0.76476</td>\n",
       "      <td>-0.043394</td>\n",
       "      <td>0.14027</td>\n",
       "      <td>0.53953</td>\n",
       "      <td>0.011416</td>\n",
       "      <td>0.21115</td>\n",
       "      <td>0.75200</td>\n",
       "      <td>-0.046141</td>\n",
       "      <td>-0.25977</td>\n",
       "      <td>0.97929</td>\n",
       "      <td>-0.11945</td>\n",
       "      <td>0.24358</td>\n",
       "      <td>0.96951</td>\n",
       "      <td>-0.11303</td>\n",
       "      <td>-0.071132</td>\n",
       "      <td>0.060772</td>\n",
       "      <td>-0.036747</td>\n",
       "      <td>0.072892</td>\n",
       "      <td>0.054903</td>\n",
       "      <td>-0.030408</td>\n",
       "      <td>-0.12277</td>\n",
       "      <td>-0.35581</td>\n",
       "      <td>-0.049096</td>\n",
       "      <td>0.11526</td>\n",
       "      <td>-0.37926</td>\n",
       "      <td>-0.029677</td>\n",
       "      <td>-0.12339</td>\n",
       "      <td>-0.65924</td>\n",
       "      <td>-0.049983</td>\n",
       "      <td>0.12544</td>\n",
       "      <td>-0.68226</td>\n",
       "      <td>-0.058466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FrameNo    head_x   head_y  ...  right_foot_x  right_foot_y  right_foot_z\n",
       "0       54 -0.000362  0.77182  ...       0.12554      -0.68488     -0.057272\n",
       "1       55 -0.000749  0.77233  ...       0.12547      -0.68383     -0.057571\n",
       "2       56 -0.001093  0.77294  ...       0.12542      -0.68291     -0.057858\n",
       "3       57 -0.001415  0.77310  ...       0.12543      -0.68254     -0.058122\n",
       "4       58 -0.001701  0.77322  ...       0.12544      -0.68226     -0.058466\n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCWrScY09BHg"
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_data = data.sample(n=(data.shape[0]), random_state=42)\n",
    "# Drop 'FrameNo' column, because it's not needed\n",
    "shuffled_data.drop('FrameNo', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkHgiLQ8C6iJ"
   },
   "outputs": [],
   "source": [
    "# Split into features and labels\n",
    "X = shuffled_data.filter(regex='_x|_y')\n",
    "y = shuffled_data.filter(regex='_z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kno0pdozoIxL"
   },
   "outputs": [],
   "source": [
    "# Scale features and labels\n",
    "feature_scaler = MinMaxScaler()\n",
    "label_scaler = MinMaxScaler()\n",
    "features = feature_scaler.fit_transform(X)\n",
    "labels = label_scaler.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuDe9gJA-kfL"
   },
   "outputs": [],
   "source": [
    "# Split into train, test, validation\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "OCfqZ1tuD8I1",
    "outputId": "0dc3aecb-ff80-4365-eeb6-1165debc0da9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24005, 26)\n",
      "(16803, 26)\n",
      "(5761, 26)\n",
      "(1441, 26)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzBAkNKAECIi"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(26,)))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDnKStkdIldF"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-4), metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "TNy1xpAIIqfX",
    "outputId": "8b38d9d0-38b2-4f36-c0e1-db85f98f28d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1c25a47f60>"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6)\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=500, verbose=0, batch_size=128, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "owgiencVwxgV",
    "outputId": "1a0cfad7-48dc-4088-a74b-fa10c0d51537"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04554207995533943, 0.004041305743157864]"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmotUHRWjs-m"
   },
   "source": [
    "## Software Development part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALG0JyIej80E"
   },
   "source": [
    "1. Designed deep learning pipeline to simplify model training, testing and deployment for conducting the inferance. The pipeline consists of the following modules:\n",
    "* Data creator - Reads the data from the given folder. Processes it into separate datasets in order to train and test the model.\n",
    "* Model - Serves as a wrapper for the model architecture to simplify the model compilation, saving, serving.\n",
    "* Trainer - Defines the pipeline for the model training. Integrates Model, Data creator with the configurate hyperparameters to start the training. Includes early stopping callback to stop the traininng session if no loss decrease was made in the given N epochs. Aslo contains the Tensorboard callback for interactive visualization of optimization process and logs saving.\n",
    "* Estimator - the module to serve the model and conduct the inferance on a given set of data\n",
    "* Config - Includes multi-level fancy dictionary for project configuration.\n",
    "2. Implemented the wrapper class for PoseNet. This step was made to maintain code clean and flexible as it will serve as one of key components during end-product service integration. The optimization of model and session components was also performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0yYOl0ByUzf"
   },
   "source": [
    "### Adam optimizer results\n",
    "\n",
    "- Before Feedback:  \n",
    "\n",
    "| Optimizer |\tHidden layer neurons (in sequence) |\tTrain set |\tTest set |\n",
    "|---------------|---------------|---------------------------|---------------------------|\n",
    "| Adam, lr=3e-4 | 128            | mse: 0.0028 - mae: 0.0360     | mse: 0.0027 - mae: 0.0358     |\n",
    "| Adam, lr=3e-5 | 128            | mse: 0.0029 - mae: 0.0366     | mse: 0.0028 - mae: 0.0363     |\n",
    "| Adam, lr=3e-6 | 128            | mse: 0.0039 - mae: 0.0433     | mse: 0.0037 - mae: 0.0427     |\n",
    "| Adam, lr=3e-4 | 512, 256, 128  | mse: 0.0009 - mae: 0.0207     | mse: 0.0011 - mae: 0.0232     |\n",
    "| Adam, lr=3e-5 | 512, 256, 128  | mse: 0.0015 - mae: 0.0261     | mse: 0.0018 - mae: 0.0281     |\n",
    "| Adam, lr=3e-6 | 512, 256, 128  | mse: 0.0020 - mae: 0.0301     | mse: 0.0023 - mae: 0.0320     |\n",
    "|**Adam, lr=3e-4** | 1024, 512, 128 | mse: 4.5195e-04 - mae: 0.0146 | **mse: 6.1760e-04 - mae: 0.0167** |\n",
    "| Adam, lr=3e-5 | 1024, 512, 128 | mse: 0.0014 - mae: 0.0248     | mse: 0.0017 - mae: 0.0272     |\n",
    "| Adam, lr=3e-6 | 1024, 512, 1288 | mse: 0.0023 - mae: 0.0317     | mse: 0.0022 - mae: 0.0317     |\n",
    "\n",
    "\n",
    "- After Feedback:  \n",
    "Adam(learning_rate = 1e-4) \n",
    "\n",
    "| Neurons | Activation function | Epochs | Batch size | Results on test set|\n",
    "|---------|---------------------|--------|------------|--------------------|\n",
    "|64, None|relu | 100 | 16 |  mae (loss): 0.0537, mse: 0.0054 |\n",
    "|128, None|relu | 100 | 16 |  mae (loss): 0.0498, mse: 0.0046 |\n",
    "|256, None|relu | 100 | 16 |  mae (loss): 0.0515, mse: 0.0048 |\n",
    "|64, 32|relu | 100 | 16 |  mae (loss): 0.0471, mse: 0.0042 |\n",
    "|128, 64|relu | 100 | 16 |  mae (loss): 0.0390, mse: 0.0028 |\n",
    "|64, None|selu | 100 | 16 |  mae (loss): 0.0541, mse: 0.0057 |\n",
    "|128, None|selu | 100 | 16 |  mae (loss): 0.0607, mse: 0.0064 |\n",
    "|256, None|selu | 100 | 16 |  mae (loss): 0.0898, mse: 0.0131 |\n",
    "|64, 32|selu | 100 | 16 |  mae (loss): 0.0524, mse: 0.0052 |\n",
    "|128, 64|selu | 100 | 16 |  mae (loss): 0.0465, mse: 0.0041 |\n",
    "|64, None|tanh | 100 | 16 |  mae (loss): 0.0577, mse: 0.0063 |\n",
    "|128, None|tanh | 100 | 16 |  mae (loss): 0.0682, mse: 0.0081 |\n",
    "|256, None|tanh | 100 | 16 |  mae (loss): 0.0698, mse: 0.0085 |\n",
    "|64, 32|tanh | 100 | 16 |  mae (loss): 0.0511, mse: 0.0047 |\n",
    "|128, 64|tanh | 100 | 16 |  mae (loss): 0.0517, mse: 0.0048 |\n",
    "|64, None|relu | 500 | 16 |  mae (loss): 0.0536, mse: 0.0055 |\n",
    "|128, None|relu | 500 | 16 |  mae (loss): 0.0574, mse: 0.0061 |\n",
    "|256, None|relu | 500 | 16 |  mae (loss): 0.0485, mse: 0.0043 |\n",
    "|64, 32|relu | 500 | 16 |  mae (loss): 0.0448, mse: 0.0041 |\n",
    "|128, 64|relu | 500 | 16 |  mae (loss): 0.0422, mse: 0.0033 |\n",
    "|64, None|selu | 500 | 16 |  mae (loss): 0.0610, mse: 0.0069 |\n",
    "|128, None|selu | 500 | 16 |  mae (loss): 0.0607, mse: 0.0067 |\n",
    "|256, None|selu | 500 | 16 |  mae (loss): 0.0743, mse: 0.0092 |\n",
    "|64, 32|selu | 500 | 16 |  mae (loss): 0.0504, mse: 0.0048 |\n",
    "|128, 64|selu | 500 | 16 |  mae (loss): 0.0488, mse: 0.0045 |\n",
    "|64, None|tanh | 500 | 16 |  mae (loss): 0.0578, mse: 0.0062 |\n",
    "|128, None|tanh | 500 | 16 |  mae (loss): 0.0741, mse: 0.0095 |\n",
    "|256, None|tanh | 500 | 16 |  mae (loss): 0.0700, mse: 0.0088 |\n",
    "|64, 32|tanh | 500 | 16 |  mae (loss): 0.0527, mse: 0.0051 |\n",
    "|128, 64|tanh | 500 | 16 |  mae (loss): 0.0589, mse: 0.0061 |\n",
    "|64, None|relu | 1000 | 16 |  mae (loss): 0.0525, mse: 0.0053 |\n",
    "|128, None|relu | 1000 | 16 |  mae (loss): 0.0549, mse: 0.0055 |\n",
    "|256, None|relu | 1000 | 16 |  mae (loss): 0.0510, mse: 0.0045 |\n",
    "|64, 32|relu | 1000 | 16 |  mae (loss): 0.0508, mse: 0.0050 |\n",
    "|128, 64|relu | 1000 | 16 |  mae (loss): 0.0432, mse: 0.0035 |\n",
    "|64, None|selu | 1000 | 16 |  mae (loss): 0.0594, mse: 0.0065 |\n",
    "|128, None|selu | 1000 | 16 |  mae (loss): 0.0624, mse: 0.0070 |\n",
    "|256, None|selu | 1000 | 16 |  mae (loss): 0.0717, mse: 0.0085 |\n",
    "|64, 32|selu | 1000 | 16 |  mae (loss): 0.0529, mse: 0.0051 |\n",
    "|128, 64|selu | 1000 | 16 |  mae (loss): 0.0462, mse: 0.0040 |\n",
    "|64, None|tanh | 1000 | 16 |  mae (loss): 0.0612, mse: 0.0070 |\n",
    "|128, None|tanh | 1000 | 16 |  mae (loss): 0.0574, mse: 0.0059 |\n",
    "|256, None|tanh | 1000 | 16 |  mae (loss): 0.0708, mse: 0.0091 |\n",
    "|64, 32|tanh | 1000 | 16 |  mae (loss): 0.0531, mse: 0.0053 |\n",
    "|128, 64|tanh | 1000 | 16 |  mae (loss): 0.0547, mse: 0.0052 |\n",
    "|64, None|relu | 100 | 32 |  mae (loss): 0.0532, mse: 0.0053 |\n",
    "|128, None|relu | 100 | 32 |  mae (loss): 0.0506, mse: 0.0048 |\n",
    "|256, None|relu | 100 | 32 |  mae (loss): 0.0480, mse: 0.0041 |\n",
    "|64, 32|relu | 100 | 32 |  mae (loss): 0.0463, mse: 0.0042 |\n",
    "|128, 64|relu | 100 | 32 |  mae (loss): 0.0433, mse: 0.0034 |\n",
    "|64, None|selu | 100 | 32 |  mae (loss): 0.0550, mse: 0.0059 |\n",
    "|128, None|selu | 100 | 32 |  mae (loss): 0.0584, mse: 0.0062 |\n",
    "|256, None|selu | 100 | 32 |  mae (loss): 0.0628, mse: 0.0069 |\n",
    "|64, 32|selu | 100 | 32 |  mae (loss): 0.0555, mse: 0.0056 |\n",
    "|128, 64|selu | 100 | 32 |  mae (loss): 0.0504, mse: 0.0045 |\n",
    "|64, None|tanh | 100 | 32 |  mae (loss): 0.0555, mse: 0.0060 |\n",
    "|128, None|tanh | 100 | 32 |  mae (loss): 0.0623, mse: 0.0071 |\n",
    "|256, None|tanh | 100 | 32 |  mae (loss): 0.0885, mse: 0.0129 |\n",
    "|64, 32|tanh | 100 | 32 |  mae (loss): 0.0483, mse: 0.0043 |\n",
    "|128, 64|tanh | 100 | 32 |  mae (loss): 0.0524, mse: 0.0049 |\n",
    "|64, None|relu | 500 | 32 |  mae (loss): 0.0510, mse: 0.0050 |\n",
    "|128, None|relu | 500 | 32 |  mae (loss): 0.0481, mse: 0.0044 |\n",
    "|256, None|relu | 500 | 32 |  mae (loss): 0.0520, mse: 0.0047 |\n",
    "|64, 32|relu | 500 | 32 |  mae (loss): 0.0494, mse: 0.0046 |\n",
    "|128, 64|relu | 500 | 32 |  mae (loss): 0.0444, mse: 0.0036 |\n",
    "|64, None|selu | 500 | 32 |  mae (loss): 0.0591, mse: 0.0066 |\n",
    "|128, None|selu | 500 | 32 |  mae (loss): 0.0576, mse: 0.0063 |\n",
    "|256, None|selu | 500 | 32 |  mae (loss): 0.0702, mse: 0.0083 |\n",
    "|64, 32|selu | 500 | 32 |  mae (loss): 0.0511, mse: 0.0051 |\n",
    "|128, 64|selu | 500 | 32 |  mae (loss): 0.0479, mse: 0.0043 |\n",
    "|64, None|tanh | 500 | 32 |  mae (loss): 0.0562, mse: 0.0061 |\n",
    "|128, None|tanh | 500 | 32 |  mae (loss): 0.0574, mse: 0.0060 |\n",
    "|256, None|tanh | 500 | 32 |  mae (loss): 0.0680, mse: 0.0082 |\n",
    "|64, 32|tanh | 500 | 32 |  mae (loss): 0.0456, mse: 0.0040 |\n",
    "|128, 64|tanh | 500 | 32 |  mae (loss): 0.0530, mse: 0.0052 |\n",
    "|64, None|relu | 1000 | 32 |  mae (loss): 0.0503, mse: 0.0048 |\n",
    "|128, None|relu | 1000 | 32 |  mae (loss): 0.0503, mse: 0.0048 |\n",
    "|256, None|relu | 1000 | 32 |  mae (loss): 0.0490, mse: 0.0044 |\n",
    "|64, 32|relu | 1000 | 32 |  mae (loss): 0.0467, mse: 0.0042 |\n",
    "|128, 64|relu | 1000 | 32 |  mae (loss): 0.0403, mse: 0.0031 |\n",
    "|64, None|selu | 1000 | 32 |  mae (loss): 0.0539, mse: 0.0056 |\n",
    "|128, None|selu | 1000 | 32 |  mae (loss): 0.0563, mse: 0.0060 |\n",
    "|256, None|selu | 1000 | 32 |  mae (loss): 0.0584, mse: 0.0060 |\n",
    "|64, 32|selu | 1000 | 32 |  mae (loss): 0.0498, mse: 0.0047 |\n",
    "|128, 64|selu | 1000 | 32 |  mae (loss): 0.0528, mse: 0.0049 |\n",
    "|64, None|tanh | 1000 | 32 |  mae (loss): 0.0562, mse: 0.0060 |\n",
    "|128, None|tanh | 1000 | 32 |  mae (loss): 0.0573, mse: 0.0062 |\n",
    "|256, None|tanh | 1000 | 32 |  mae (loss): 0.0697, mse: 0.0084 |\n",
    "|64, 32|tanh | 1000 | 32 |  mae (loss): 0.0506, mse: 0.0047 |\n",
    "|128, 64|tanh | 1000 | 32 |  mae (loss): 0.0530, mse: 0.0051 |\n",
    "|64, None|relu | 100 | 64 |  mae (loss): 0.0542, mse: 0.0058 |\n",
    "|128, None|relu | 100 | 64 |  mae (loss): 0.0542, mse: 0.0055 |\n",
    "|256, None|relu | 100 | 64 |  mae (loss): 0.0478, mse: 0.0041 |\n",
    "|64, 32|relu | 100 | 64 |  mae (loss): 0.0448, mse: 0.0040 |\n",
    "|128, 64|relu | 100 | 64 |  mae (loss): 0.0418, mse: 0.0033 |\n",
    "|64, None|selu | 100 | 64 |  mae (loss): 0.0653, mse: 0.0078 |\n",
    "|128, None|selu | 100 | 64 |  mae (loss): 0.0587, mse: 0.0065 |\n",
    "|256, None|selu | 100 | 64 |  mae (loss): 0.0677, mse: 0.0076 |\n",
    "|64, 32|selu | 100 | 64 |  mae (loss): 0.0519, mse: 0.0051 |\n",
    "|128, 64|selu | 100 | 64 |  mae (loss): 0.0449, mse: 0.0039 |\n",
    "|64, None|tanh | 100 | 64 |  mae (loss): 0.0582, mse: 0.0065 |\n",
    "|128, None|tanh | 100 | 64 |  mae (loss): 0.0593, mse: 0.0064 |\n",
    "|256, None|tanh | 100 | 64 |  mae (loss): 0.0865, mse: 0.0121 |\n",
    "|64, 32|tanh | 100 | 64 |  mae (loss): 0.0541, mse: 0.0052 |\n",
    "|128, 64|tanh | 100 | 64 |  mae (loss): 0.0552, mse: 0.0055 |\n",
    "|64, None|relu | 500 | 64 |  mae (loss): 0.0639, mse: 0.0078 |\n",
    "|128, None|relu | 500 | 64 |  mae (loss): 0.0585, mse: 0.0061 |\n",
    "|256, None|relu | 500 | 64 |  mae (loss): 0.0511, mse: 0.0046 |\n",
    "|64, 32|relu | 500 | 64 |  mae (loss): 0.0477, mse: 0.0043 |\n",
    "|128, 64|relu | 500 | 64 |  mae (loss): 0.0414, mse: 0.0032 |\n",
    "|64, None|selu | 500 | 64 |  mae (loss): 0.0555, mse: 0.0060 |\n",
    "|128, None|selu | 500 | 64 |  mae (loss): 0.0594, mse: 0.0064 |\n",
    "|256, None|selu | 500 | 64 |  mae (loss): 0.0625, mse: 0.0066 |\n",
    "|64, 32|selu | 500 | 64 |  mae (loss): 0.0503, mse: 0.0049 |\n",
    "|128, 64|selu | 500 | 64 |  mae (loss): 0.0502, mse: 0.0047 |\n",
    "|64, None|tanh | 500 | 64 |  mae (loss): 0.0603, mse: 0.0067 |\n",
    "|128, None|tanh | 500 | 64 |  mae (loss): 0.0637, mse: 0.0072 |\n",
    "|256, None|tanh | 500 | 64 |  mae (loss): 0.0660, mse: 0.0081 |\n",
    "|64, 32|tanh | 500 | 64 |  mae (loss): 0.0530, mse: 0.0053 |\n",
    "|128, 64|tanh | 500 | 64 |  mae (loss): 0.0506, mse: 0.0046 |\n",
    "|64, None|relu | 1000 | 64 |  mae (loss): 0.0560, mse: 0.0059 |\n",
    "|128, None|relu | 1000 | 64 |  mae (loss): 0.0550, mse: 0.0056 |\n",
    "|256, None|relu | 1000 | 64 |  mae (loss): 0.0518, mse: 0.0047 |\n",
    "|64, 32|relu | 1000 | 64 |  mae (loss): 0.0456, mse: 0.0040 |\n",
    "|128, 64|relu | 1000 | 64 |  mae (loss): 0.0411, mse: 0.0030 |\n",
    "|64, None|selu | 1000 | 64 |  mae (loss): 0.0536, mse: 0.0057 |\n",
    "|128, None|selu | 1000 | 64 |  mae (loss): 0.0600, mse: 0.0062 |\n",
    "|256, None|selu | 1000 | 64 |  mae (loss): 0.0682, mse: 0.0079 |\n",
    "|64, 32|selu | 1000 | 64 |  mae (loss): 0.0491, mse: 0.0047 |\n",
    "|128, 64|selu | 1000 | 64 |  mae (loss): 0.0450, mse: 0.0037 |\n",
    "|64, None|tanh | 1000 | 64 |  mae (loss): 0.0564, mse: 0.0062 |\n",
    "|128, None|tanh | 1000 | 64 |  mae (loss): 0.0654, mse: 0.0072 |\n",
    "|256, None|tanh | 1000 | 64 |  mae (loss): 0.0620, mse: 0.0068 |\n",
    "|64, 32|tanh | 1000 | 64 |  mae (loss): 0.0462, mse: 0.0041 |\n",
    "|128, 64|tanh | 1000 | 64 |  mae (loss): 0.0477, mse: 0.0041 |\n",
    "|64, None|relu | 100 | 128 |  mae (loss): 0.0592, mse: 0.0066 |\n",
    "|128, None|relu | 100 | 128 |  mae (loss): 0.0566, mse: 0.0060 |\n",
    "|256, None|relu | 100 | 128 |  mae (loss): 0.0507, mse: 0.0046 |\n",
    "|64, 32|relu | 100 | 128 |  mae (loss): 0.0468, mse: 0.0042 |\n",
    "|128, 64|relu | 100 | 128 |  mae (loss): 0.0431, mse: 0.0036 |\n",
    "|64, None|selu | 100 | 128 |  mae (loss): 0.0604, mse: 0.0070 |\n",
    "|128, None|selu | 100 | 128 |  mae (loss): 0.0661, mse: 0.0078 |\n",
    "|256, None|selu | 100 | 128 |  mae (loss): 0.0585, mse: 0.0061 |\n",
    "|64, 32|selu | 100 | 128 |  mae (loss): 0.0497, mse: 0.0047 |\n",
    "|128, 64|selu | 100 | 128 |  mae (loss): 0.0498, mse: 0.0045 |\n",
    "|64, None|tanh | 100 | 128 |  mae (loss): 0.0559, mse: 0.0060 |\n",
    "|128, None|tanh | 100 | 128 |  mae (loss): 0.0631, mse: 0.0072 |\n",
    "|256, None|tanh | 100 | 128 |  mae (loss): 0.0708, mse: 0.0086 |\n",
    "|64, 32|tanh | 100 | 128 |  mae (loss): 0.0479, mse: 0.0044 |\n",
    "|128, 64|tanh | 100 | 128 |  mae (loss): 0.0573, mse: 0.0057 |\n",
    "|64, None|relu | 500 | 128 |  mae (loss): 0.0553, mse: 0.0060 |\n",
    "|128, None|relu | 500 | 128 |  mae (loss): 0.0511, mse: 0.0048 |\n",
    "|256, None|relu | 500 | 128 |  mae (loss): 0.0574, mse: 0.0059 |\n",
    "|64, 32|relu | 500 | 128 |  mae (loss): 0.0462, mse: 0.0043 |\n",
    "|128, 64|relu | 500 | 128 |  mae (loss): 0.0382, mse: 0.0028 |\n",
    "|64, None|selu | 500 | 128 |  mae (loss): 0.0630, mse: 0.0074 |\n",
    "|128, None|selu | 500 | 128 |  mae (loss): 0.0648, mse: 0.0077 |\n",
    "|256, None|selu | 500 | 128 |  mae (loss): 0.0673, mse: 0.0078 |\n",
    "|64, 32|selu | 500 | 128 |  mae (loss): 0.0493, mse: 0.0047 |\n",
    "|128, 64|selu | 500 | 128 |  mae (loss): 0.0462, mse: 0.0040 |\n",
    "|64, None|tanh | 500 | 128 |  mae (loss): 0.0584, mse: 0.0066 |\n",
    "|128, None|tanh | 500 | 128 |  mae (loss): 0.0645, mse: 0.0075 |\n",
    "|256, None|tanh | 500 | 128 |  mae (loss): 0.0686, mse: 0.0083 |\n",
    "|64, 32|tanh | 500 | 128 |  mae (loss): 0.0516, mse: 0.0051 |\n",
    "|128, 64|tanh | 500 | 128 |  mae (loss): 0.0497, mse: 0.0046 |\n",
    "|64, None|relu | 1000 | 128 |  mae (loss): 0.0572, mse: 0.0064 |\n",
    "|128, None|relu | 1000 | 128 |  mae (loss): 0.0555, mse: 0.0055 |\n",
    "|256, None|relu | 1000 | 128 |  mae (loss): 0.0492, mse: 0.0043 |\n",
    "|64, 32|relu | 1000 | 128 |  mae (loss): 0.0444, mse: 0.0039 |\n",
    "|128, 64|relu | 1000 | 128 |  mae (loss): 0.0441, mse: 0.0038 |\n",
    "|64, None|selu | 1000 | 128 |  mae (loss): 0.0564, mse: 0.0060 |\n",
    "|128, None|selu | 1000 | 128 |  mae (loss): 0.0603, mse: 0.0068 |\n",
    "|256, None|selu | 1000 | 128 |  mae (loss): 0.0688, mse: 0.0083 |\n",
    "|64, 32|selu | 1000 | 128 |  mae (loss): 0.0458, mse: 0.0041 |\n",
    "|128, 64|selu | 1000 | 128 |  mae (loss): 0.0498, mse: 0.0046 |\n",
    "|64, None|tanh | 1000 | 128 |  mae (loss): 0.0580, mse: 0.0066 |\n",
    "|128, None|tanh | 1000 | 128 |  mae (loss): 0.0738, mse: 0.0095 |\n",
    "|256, None|tanh | 1000 | 128 |  mae (loss): 0.0633, mse: 0.0072 |\n",
    "|64, 32|tanh | 1000 | 128 |  mae (loss): 0.0533, mse: 0.0053 |\n",
    "|128, 64|tanh | 1000 | 128 |  mae (loss): 0.0529, mse: 0.0052 |\n",
    "\n",
    "\n",
    "Best result: neurons=128, 64, activation=relu, batch_size=128, epochs=500, MAE=0.0382, MSE=0.0028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FT5Zbd7YvyJ0"
   },
   "source": [
    "### SGD optimizer results\n",
    "\n",
    "- Before Feedback:  \n",
    "\n",
    "| Optimizer |\tHidden layer neurons (in sequence) |\tTrain set |\tTest set |\n",
    "|---------------|---------------|---------------------------|---------------------------|\n",
    "| **SGD, lr=3e-4** | 128            | mse: 0.0046 - mae: 0.0478     | **mse: 0.0042 - mae: 0.0466**     |\n",
    "| **SGD, lr=3e-5** | 128            | mse: 0.0046 - mae: 0.0478     | **mse: 0.0042 - mae: 0.0466**     |\n",
    "| **SGD, lr=3e-6** | 128            | mse: 0.0046 - mae: 0.0478     | **mse: 0.0042 - mae: 0.0466**     |\n",
    "| SGD, lr=3e-4 | 512, 256, 128  | mse: 0.0074 - mae: 0.0632     | mse: 0.0070 - mae: 0.0624     |\n",
    "| SGD, lr=3e-5 | 512, 256, 128  | mse: 0.0157 - mae: 0.0907     | mse: 0.0151 - mae: 0.0899     |\n",
    "| SGD, lr=3e-6 | 512, 256, 128  | mse: 0.0297 - mae: 0.1292     | mse: 0.0290 - mae: 0.1281     |\n",
    "|SGD, lr=3e-4 | 1024, 512, 256 | mse: 0.0070 - mae: 0.0617 | mse: 0.0067 - mae: 0.0610 |\n",
    "| SGD, lr=3e-5 | 1024, 512, 256 | mse: 0.0143 - mae: 0.0861     | mse: 0.0138 - mae: 0.0857     |\n",
    "| SGD, lr=3e-6 | 1024, 512, 256 | mse: 0.0258 - mae: 0.1168     | mse: 0.0252 - mae: 0.1159     |\n",
    "\n",
    "\n",
    "\n",
    "- After Feedback:  \n",
    "SGD(learning_rate = 1e-4, decay=1e-7, momentum=0.9, nesterov=True)\n",
    "\n",
    "| Neurons | Activation function | Epochs | Batch size | Results on test set|\n",
    "|---------|---------------------|--------|------------|--------------------|\n",
    "64, None | relu | 100 | 16 |  mae (loss): 0.0586, mse: 0.0066 |\n",
    "128, None | relu | 100 | 16 |  mae (loss): 0.0522, mse: 0.0056 |\n",
    "256, None | relu | 100 | 16 |  mae (loss): 0.0512, mse: 0.0051 |\n",
    "64, 32 | relu | 100 | 16 |  mae (loss): 0.0580, mse: 0.0068 |\n",
    "128, 64 | relu | 100 | 16 |  mae (loss): 0.0520, mse: 0.0056 |\n",
    "64, None | selu | 100 | 16 |  mae (loss): 0.0572, mse: 0.0063 |\n",
    "128, None | selu | 100 | 16 |  mae (loss): 0.0553, mse: 0.0061 |\n",
    "256, None | selu | 100 | 16 |  mae (loss): 0.0522, mse: 0.0052 |\n",
    "64, 32 | selu | 100 | 16 |  mae (loss): 0.0568, mse: 0.0064 |\n",
    "128, 64 | selu | 100 | 16 |  mae (loss): 0.0522, mse: 0.0056 |\n",
    "64, None | tanh | 100 | 16 |  mae (loss): 0.0587, mse: 0.0068 |\n",
    "128, None | tanh | 100 | 16 |  mae (loss): 0.0569, mse: 0.0064 |\n",
    "256, None | tanh | 100 | 16 |  mae (loss): 0.0616, mse: 0.0074 |\n",
    "64, 32 | tanh | 100 | 16 |  mae (loss): 0.0615, mse: 0.0073 |\n",
    "128, 64 | tanh | 100 | 16 |  mae (loss): 0.0570, mse: 0.0064 |\n",
    "64, None | relu | 500 | 16 |  mae (loss): 0.0561, mse: 0.0061 |\n",
    "128, None | relu | 500 | 16 |  mae (loss): 0.0517, mse: 0.0052 |\n",
    "256, None | relu | 500 | 16 |  mae (loss): 0.0487, mse: 0.0047 |\n",
    "64, 32 | relu | 500 | 16 |  mae (loss): 0.0517, mse: 0.0054 |\n",
    "128, 64 | relu | 500 | 16 |  mae (loss): 0.0488, mse: 0.0048 |\n",
    "64, None | selu | 500 | 16 |  mae (loss): 0.0525, mse: 0.0056 |\n",
    "128, None | selu | 500 | 16 |  mae (loss): 0.0552, mse: 0.0061 |\n",
    "256, None | selu | 500 | 16 |  mae (loss): 0.0567, mse: 0.0061 |\n",
    "64, 32 | selu | 500 | 16 |  mae (loss): 0.0523, mse: 0.0054 |\n",
    "128, 64 | selu | 500 | 16 |  mae (loss): 0.0498, mse: 0.0050 |\n",
    "64, None | tanh | 500 | 16 |  mae (loss): 0.0554, mse: 0.0061 |\n",
    "128, None | tanh | 500 | 16 |  mae (loss): 0.0571, mse: 0.0064 |\n",
    "256, None | tanh | 500 | 16 |  mae (loss): 0.0607, mse: 0.0071 |\n",
    "64, 32 | tanh | 500 | 16 |  mae (loss): 0.0549, mse: 0.0058 |\n",
    "128, 64 | tanh | 500 | 16 |  mae (loss): 0.0508, mse: 0.0051 |\n",
    "64, None | relu | 1000 | 16 |  mae (loss): 0.0498, mse: 0.0050 |\n",
    "128, None | relu | 1000 | 16 |  mae (loss): 0.0534, mse: 0.0056 |\n",
    "256, None | relu | 1000 | 16 |  mae (loss): 0.0462, mse: 0.0041 |\n",
    "64, 32 | relu | 1000 | 16 |  mae (loss): 0.0522, mse: 0.0055 |\n",
    "128, 64 | relu | 1000 | 16 |  mae (loss): 0.0486, mse: 0.0048 |\n",
    "64, None | selu | 1000 | 16 |  mae (loss): 0.0558, mse: 0.0063 |\n",
    "128, None | selu | 1000 | 16 |  mae (loss): 0.0539, mse: 0.0060 |\n",
    "256, None | selu | 1000 | 16 |  mae (loss): 0.0564, mse: 0.0063 |\n",
    "64, 32 | selu | 1000 | 16 |  mae (loss): 0.0522, mse: 0.0055 |\n",
    "128, 64 | selu | 1000 | 16 |  mae (loss): 0.0509, mse: 0.0052 |\n",
    "64, None | tanh | 1000 | 16 |  mae (loss): 0.0551, mse: 0.0062 |\n",
    "128, None | tanh | 1000 | 16 |  mae (loss): 0.0560, mse: 0.0063 |\n",
    "256, None | tanh | 1000 | 16 |  mae (loss): 0.0578, mse: 0.0066 |\n",
    "64, 32 | tanh | 1000 | 16 |  mae (loss): 0.0543, mse: 0.0056 |\n",
    "128, 64 | tanh | 1000 | 16 |  mae (loss): 0.0509, mse: 0.0051 |\n",
    "64, None | relu | 100 | 32 |  mae (loss): 0.0637, mse: 0.0079 |\n",
    "128, None | relu | 100 | 32 |  mae (loss): 0.0586, mse: 0.0067 |\n",
    "256, None | relu | 100 | 32 |  mae (loss): 0.0542, mse: 0.0060 |\n",
    "64, 32 | relu | 100 | 32 |  mae (loss): 0.0623, mse: 0.0076 |\n",
    "128, 64 | relu | 100 | 32 |  mae (loss): 0.0546, mse: 0.0063 |\n",
    "64, None | selu | 100 | 32 |  mae (loss): 0.0627, mse: 0.0078 |\n",
    "128, None | selu | 100 | 32 |  mae (loss): 0.0573, mse: 0.0067 |\n",
    "256, None | selu | 100 | 32 |  mae (loss): 0.0561, mse: 0.0063 |\n",
    "64, 32 | selu | 100 | 32 |  mae (loss): 0.0622, mse: 0.0075 |\n",
    "128, 64 | selu | 100 | 32 |  mae (loss): 0.0548, mse: 0.0060 |\n",
    "64, None | tanh | 100 | 32 |  mae (loss): 0.0603, mse: 0.0073 |\n",
    "128, None | tanh | 100 | 32 |  mae (loss): 0.0587, mse: 0.0067 |\n",
    "256, None | tanh | 100 | 32 |  mae (loss): 0.0607, mse: 0.0072 |\n",
    "64, 32 | tanh | 100 | 32 |  mae (loss): 0.0688, mse: 0.0093 |\n",
    "128, 64 | tanh | 100 | 32 |  mae (loss): 0.0590, mse: 0.0068 |\n",
    "64, None | relu | 500 | 32 |  mae (loss): 0.0561, mse: 0.0064 |\n",
    "128, None | relu | 500 | 32 |  mae (loss): 0.0533, mse: 0.0057 |\n",
    "256, None | relu | 500 | 32 |  mae (loss): 0.0462, mse: 0.0043 |\n",
    "64, 32 | relu | 500 | 32 |  mae (loss): 0.0522, mse: 0.0056 |\n",
    "128, 64 | relu | 500 | 32 |  mae (loss): 0.0492, mse: 0.0049 |\n",
    "64, None | selu | 500 | 32 |  mae (loss): 0.0557, mse: 0.0063 |\n",
    "128, None | selu | 500 | 32 |  mae (loss): 0.0535, mse: 0.0058 |\n",
    "256, None | selu | 500 | 32 |  mae (loss): 0.0543, mse: 0.0057 |\n",
    "64, 32 | selu | 500 | 32 |  mae (loss): 0.0506, mse: 0.0053 |\n",
    "128, 64 | selu | 500 | 32 |  mae (loss): 0.0502, mse: 0.0051 |\n",
    "64, None | tanh | 500 | 32 |  mae (loss): 0.0541, mse: 0.0059 |\n",
    "128, None | tanh | 500 | 32 |  mae (loss): 0.0565, mse: 0.0065 |\n",
    "256, None | tanh | 500 | 32 |  mae (loss): 0.0585, mse: 0.0067 |\n",
    "64, 32 | tanh | 500 | 32 |  mae (loss): 0.0517, mse: 0.0053 |\n",
    "128, 64 | tanh | 500 | 32 |  mae (loss): 0.0474, mse: 0.0044 |\n",
    "64, None | relu | 1000 | 32 |  mae (loss): 0.0544, mse: 0.0058 |\n",
    "128, None | relu | 1000 | 32 |  mae (loss): 0.0519, mse: 0.0053 |\n",
    "256, None | relu | 1000 | 32 |  mae (loss): 0.0467, mse: 0.0043 |\n",
    "64, 32 | relu | 1000 | 32 |  mae (loss): 0.0480, mse: 0.0048 |\n",
    "128, 64 | relu | 1000 | 32 |  mae (loss): 0.0474, mse: 0.0046 |\n",
    "64, None | selu | 1000 | 32 |  mae (loss): 0.0564, mse: 0.0064 |\n",
    "128, None | selu | 1000 | 32 |  mae (loss): 0.0533, mse: 0.0058 |\n",
    "256, None | selu | 1000 | 32 |  mae (loss): 0.0511, mse: 0.0054 |\n",
    "64, 32 | selu | 1000 | 32 |  mae (loss): 0.0496, mse: 0.0050 |\n",
    "128, 64 | selu | 1000 | 32 |  mae (loss): 0.0487, mse: 0.0049 |\n",
    "64, None | tanh | 1000 | 32 |  mae (loss): 0.0542, mse: 0.0060 |\n",
    "128, None | tanh | 1000 | 32 |  mae (loss): 0.0567, mse: 0.0064 |\n",
    "256, None | tanh | 1000 | 32 |  mae (loss): 0.0596, mse: 0.0070 |\n",
    "64, 32 | tanh | 1000 | 32 |  mae (loss): 0.0528, mse: 0.0056 |\n",
    "128, 64 | tanh | 1000 | 32 |  mae (loss): 0.0502, mse: 0.0050 |\n",
    "64, None | relu | 100 | 64 |  mae (loss): 0.0702, mse: 0.0092 |\n",
    "128, None | relu | 100 | 64 |  mae (loss): 0.0626, mse: 0.0076 |\n",
    "256, None | relu | 100 | 64 |  mae (loss): 0.0573, mse: 0.0063 |\n",
    "64, 32 | relu | 100 | 64 |  mae (loss): 0.0707, mse: 0.0096 |\n",
    "128, 64 | relu | 100 | 64 |  mae (loss): 0.0652, mse: 0.0082 |\n",
    "64, None | selu | 100 | 64 |  mae (loss): 0.0677, mse: 0.0092 |\n",
    "128, None | selu | 100 | 64 |  mae (loss): 0.0605, mse: 0.0071 |\n",
    "256, None | selu | 100 | 64 |  mae (loss): 0.0588, mse: 0.0069 |\n",
    "64, 32 | selu | 100 | 64 |  mae (loss): 0.0704, mse: 0.0095 |\n",
    "128, 64 | selu | 100 | 64 |  mae (loss): 0.0635, mse: 0.0079 |\n",
    "64, None | tanh | 100 | 64 |  mae (loss): 0.0678, mse: 0.0090 |\n",
    "128, None | tanh | 100 | 64 |  mae (loss): 0.0612, mse: 0.0073 |\n",
    "256, None | tanh | 100 | 64 |  mae (loss): 0.0613, mse: 0.0073 |\n",
    "64, 32 | tanh | 100 | 64 |  mae (loss): 0.0733, mse: 0.0106 |\n",
    "128, 64 | tanh | 100 | 64 |  mae (loss): 0.0659, mse: 0.0086 |\n",
    "64, None | relu | 500 | 64 |  mae (loss): 0.0586, mse: 0.0069 |\n",
    "128, None | relu | 500 | 64 |  mae (loss): 0.0512, mse: 0.0053 |\n",
    "256, None | relu | 500 | 64 |  mae (loss): 0.0495, mse: 0.0049 |\n",
    "64, 32 | relu | 500 | 64 |  mae (loss): 0.0561, mse: 0.0065 |\n",
    "128, 64 | relu | 500 | 64 |  mae (loss): 0.0493, mse: 0.0050 |\n",
    "64, None | selu | 500 | 64 |  mae (loss): 0.0567, mse: 0.0065 |\n",
    "128, None | selu | 500 | 64 |  mae (loss): 0.0584, mse: 0.0068 |\n",
    "256, None | selu | 500 | 64 |  mae (loss): 0.0593, mse: 0.0066 |\n",
    "64, 32 | selu | 500 | 64 |  mae (loss): 0.0523, mse: 0.0057 |\n",
    "128, 64 | selu | 500 | 64 |  mae (loss): 0.0469, mse: 0.0045 |\n",
    "64, None | tanh | 500 | 64 |  mae (loss): 0.0546, mse: 0.0061 |\n",
    "128, None | tanh | 500 | 64 |  mae (loss): 0.0569, mse: 0.0065 |\n",
    "256, None | tanh | 500 | 64 |  mae (loss): 0.0639, mse: 0.0080 |\n",
    "64, 32 | tanh | 500 | 64 |  mae (loss): 0.0593, mse: 0.0070 |\n",
    "128, 64 | tanh | 500 | 64 |  mae (loss): 0.0500, mse: 0.0052 |\n",
    "64, None | relu | 1000 | 64 |  mae (loss): 0.0549, mse: 0.0062 |\n",
    "128, None | relu | 1000 | 64 |  mae (loss): 0.0523, mse: 0.0055 |\n",
    "256, None | relu | 1000 | 64 |  mae (loss): 0.0490, mse: 0.0048 |\n",
    "64, 32 | relu | 1000 | 64 |  mae (loss): 0.4887, mse: 0.4384 |\n",
    "128, 64 | relu | 1000 | 64 |  mae (loss): 0.0475, mse: 0.0046 |\n",
    "64, None | selu | 1000 | 64 |  mae (loss): 0.0542, mse: 0.0059 |\n",
    "128, None | selu | 1000 | 64 |  mae (loss): 0.0584, mse: 0.0068 |\n",
    "256, None | selu | 1000 | 64 |  mae (loss): 0.0550, mse: 0.0060 |\n",
    "64, 32 | selu | 1000 | 64 |  mae (loss): 0.0497, mse: 0.0051 |\n",
    "128, 64 | selu | 1000 | 64 |  mae (loss): 0.0479, mse: 0.0046 |\n",
    "64, None | tanh | 1000 | 64 |  mae (loss): 0.0543, mse: 0.0061 |\n",
    "128, None | tanh | 1000 | 64 |  mae (loss): 0.0562, mse: 0.0064 |\n",
    "256, None | tanh | 1000 | 64 |  mae (loss): 0.0603, mse: 0.0071 |\n",
    "64, 32 | tanh | 1000 | 64 |  mae (loss): 0.0502, mse: 0.0051 |\n",
    "128, 64 | tanh | 1000 | 64 |  mae (loss): 0.0446, mse: 0.0040 |\n",
    "64, None | relu | 100 | 128 |  mae (loss): 0.0811, mse: 0.0126 |\n",
    "128, None | relu | 100 | 128 |  mae (loss): 0.0716, mse: 0.0100 |\n",
    "256, None | relu | 100 | 128 |  mae (loss): 0.0650, mse: 0.0090 |\n",
    "64, 32 | relu | 100 | 128 |  mae (loss): 0.6550, mse: 0.7704 |\n",
    "128, 64 | relu | 100 | 128 |  mae (loss): 0.0945, mse: 0.0167 |\n",
    "64, None | selu | 100 | 128 |  mae (loss): 0.0849, mse: 0.0145 |\n",
    "128, None | selu | 100 | 128 |  mae (loss): 0.0693, mse: 0.0093 |\n",
    "256, None | selu | 100 | 128 |  mae (loss): 0.0616, mse: 0.0074 |\n",
    "64, 32 | selu | 100 | 128 |  mae (loss): 0.0870, mse: 0.0156 |\n",
    "128, 64 | selu | 100 | 128 |  mae (loss): 0.0777, mse: 0.0118 |\n",
    "64, None | tanh | 100 | 128 |  mae (loss): 0.0811, mse: 0.0126 |\n",
    "128, None | tanh | 100 | 128 |  mae (loss): 0.5399, mse: 0.4633 |\n",
    "256, None | tanh | 100 | 128 |  mae (loss): 0.0645, mse: 0.0081 |\n",
    "64, 32 | tanh | 100 | 128 |  mae (loss): 0.0874, mse: 0.0155 |\n",
    "128, 64 | tanh | 100 | 128 |  mae (loss): 0.0735, mse: 0.0107 |\n",
    "64, None | relu | 500 | 128 |  mae (loss): 0.0602, mse: 0.0072 |\n",
    "128, None | relu | 500 | 128 |  mae (loss): 0.0527, mse: 0.0057 |\n",
    "256, None | relu | 500 | 128 |  mae (loss): 0.0485, mse: 0.0049 |\n",
    "64, 32 | relu | 500 | 128 |  mae (loss): 0.6235, mse: 0.6792 |\n",
    "128, 64 | relu | 500 | 128 |  mae (loss): 0.0513, mse: 0.0053 |\n",
    "64, None | selu | 500 | 128 |  mae (loss): 0.5431, mse: 0.4909 |\n",
    "128, None | selu | 500 | 128 |  mae (loss): 0.0581, mse: 0.0068 |\n",
    "256, None | selu | 500 | 128 |  mae (loss): 0.0566, mse: 0.0063 |\n",
    "64, 32 | selu | 500 | 128 |  mae (loss): 0.0582, mse: 0.0069 |\n",
    "128, 64 | selu | 500 | 128 |  mae (loss): 0.0520, mse: 0.0054 |\n",
    "64, None | tanh | 500 | 128 |  mae (loss): 0.5504, mse: 0.4784 |\n",
    "128, None | tanh | 500 | 128 |  mae (loss): 0.0581, mse: 0.0068 |\n",
    "256, None | tanh | 500 | 128 |  mae (loss): 0.0611, mse: 0.0074 |\n",
    "64, 32 | tanh | 500 | 128 |  mae (loss): 0.0640, mse: 0.0082 |\n",
    "128, 64 | tanh | 500 | 128 |  mae (loss): 0.5182, mse: 0.4259 |\n",
    "64, None | relu | 1000 | 128 |  mae (loss): 0.5490, mse: 0.4921 |\n",
    "128, None | relu | 1000 | 128 |  mae (loss): 0.0576, mse: 0.0065 |\n",
    "256, None | relu | 1000 | 128 |  mae (loss): 0.0494, mse: 0.0052 |\n",
    "64, 32 | relu | 1000 | 128 |  mae (loss): 0.6454, mse: 0.7040 |\n",
    "128, 64 | relu | 1000 | 128 |  mae (loss): 0.5908, mse: 0.6249 |\n",
    "64, None | selu | 1000 | 128 |  mae (loss): 0.0552, mse: 0.0062 |\n",
    "128, None | selu | 1000 | 128 |  mae (loss): 0.5058, mse: 0.4101 |\n",
    "256, None | selu | 1000 | 128 |  mae (loss): 0.0571, mse: 0.0065 |\n",
    "64, 32 | selu | 1000 | 128 |  mae (loss): 0.0538, mse: 0.0058 |\n",
    "128, 64 | selu | 1000 | 128 |  mae (loss): 0.0468, mse: 0.0048 |\n",
    "64, None | tanh | 1000 | 128 |  mae (loss): 0.5763, mse: 0.5252 |\n",
    "128, None | tanh | 1000 | 128 |  mae (loss): 0.0583, mse: 0.0069 |\n",
    "256, None | tanh | 1000 | 128 |  mae (loss): 0.0595, mse: 0.0071 |\n",
    "64, 32 | tanh | 1000 | 128 |  mae (loss): 0.0578, mse: 0.0067 |\n",
    "128, 64 | tanh | 1000 | 128 |  mae (loss): 0.0462, mse: 0.0044 |\n",
    "\n",
    "\n",
    "Best result: neurons=128, 64, activation=tanh, batch_size=64, epochs=1000, MAE=0.0446, MSE=0.0040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEv6z0Vpwtrf"
   },
   "source": [
    "### Nestorov Adam optimizer results\n",
    "\n",
    "- Automated Tests:  \n",
    "Nadam(lr=1e-4, beta_1=0.9, beta_2=0.999)  \n",
    "\n",
    "| Neurons | Activation function | Epochs | Batch size | Results on test set|\n",
    "|---------|---------------------|--------|------------|--------------------|\n",
    "64, None | relu | 100 | 16 |  mae (loss): 0.0637, mse: 0.0075 |\n",
    "128, None | relu | 100 | 16 |  mae (loss): 0.0515, mse: 0.0050 |\n",
    "256, None | relu | 100 | 16 |  mae (loss): 0.0600, mse: 0.0062 |\n",
    "64, 32 | relu | 100 | 16 |  mae (loss): 0.0460, mse: 0.0040 |\n",
    "128, 64 | relu | 100 | 16 |  mae (loss): 0.0432, mse: 0.0035 |\n",
    "64, None | selu | 100 | 16 |  mae (loss): 0.0585, mse: 0.0063 |\n",
    "128, None | selu | 100 | 16 |  mae (loss): 0.0686, mse: 0.0086 |\n",
    "256, None | selu | 100 | 16 |  mae (loss): 0.0608, mse: 0.0066 |\n",
    "64, 32 | selu | 100 | 16 |  mae (loss): 0.0483, mse: 0.0044 |\n",
    "128, 64 | selu | 100 | 16 |  mae (loss): 0.0518, mse: 0.0049 |\n",
    "64, None | tanh | 100 | 16 |  mae (loss): 0.0602, mse: 0.0067 |\n",
    "128, None | tanh | 100 | 16 |  mae (loss): 0.0659, mse: 0.0073 |\n",
    "256, None | tanh | 100 | 16 |  mae (loss): 0.0703, mse: 0.0081 |\n",
    "64, 32 | tanh | 100 | 16 |  mae (loss): 0.0532, mse: 0.0053 |\n",
    "128, 64 | tanh | 100 | 16 |  mae (loss): 0.0614, mse: 0.0067 |\n",
    "64, None | relu | 500 | 16 |  mae (loss): 0.0547, mse: 0.0057 |\n",
    "128, None | relu | 500 | 16 |  mae (loss): 0.0537, mse: 0.0052 |\n",
    "256, None | relu | 500 | 16 |  mae (loss): 0.0469, mse: 0.0039 |\n",
    "64, 32 | relu | 500 | 16 |  mae (loss): 0.0510, mse: 0.0049 |\n",
    "128, 64 | relu | 500 | 16 |  mae (loss): 0.0425, mse: 0.0033 |\n",
    "64, None | selu | 500 | 16 |  mae (loss): 0.0570, mse: 0.0061 |\n",
    "128, None | selu | 500 | 16 |  mae (loss): 0.0603, mse: 0.0066 |\n",
    "256, None | selu | 500 | 16 |  mae (loss): 0.0842, mse: 0.0114 |\n",
    "64, 32 | selu | 500 | 16 |  mae (loss): 0.0503, mse: 0.0049 |\n",
    "128, 64 | selu | 500 | 16 |  mae (loss): 0.0489, mse: 0.0046 |\n",
    "64, None | tanh | 500 | 16 |  mae (loss): 0.0607, mse: 0.0070 |\n",
    "128, None | tanh | 500 | 16 |  mae (loss): 0.0807, mse: 0.0098 |\n",
    "256, None | tanh | 500 | 16 |  mae (loss): 0.0705, mse: 0.0088 |\n",
    "64, 32 | tanh | 500 | 16 |  mae (loss): 0.0546, mse: 0.0056 |\n",
    "128, 64 | tanh | 500 | 16 |  mae (loss): 0.0564, mse: 0.0057 |\n",
    "64, None | relu | 1000 | 16 |  mae (loss): 0.0549, mse: 0.0057 |\n",
    "128, None | relu | 1000 | 16 |  mae (loss): 0.0532, mse: 0.0053 |\n",
    "256, None | relu | 1000 | 16 |  mae (loss): 0.0484, mse: 0.0043 |\n",
    "64, 32 | relu | 1000 | 16 |  mae (loss): 0.0474, mse: 0.0043 |\n",
    "128, 64 | relu | 1000 | 16 |  mae (loss): 0.0416, mse: 0.0034 |\n",
    "64, None | selu | 1000 | 16 |  mae (loss): 0.0557, mse: 0.0059 |\n",
    "128, None | selu | 1000 | 16 |  mae (loss): 0.0608, mse: 0.0066 |\n",
    "256, None | selu | 1000 | 16 |  mae (loss): 0.0781, mse: 0.0104 |\n",
    "64, 32 | selu | 1000 | 16 |  mae (loss): 0.0516, mse: 0.0050 |\n",
    "128, 64 | selu | 1000 | 16 |  mae (loss): 0.0508, mse: 0.0048 |\n",
    "64, None | tanh | 1000 | 16 |  mae (loss): 0.0586, mse: 0.0063 |\n",
    "128, None | tanh | 1000 | 16 |  mae (loss): 0.0614, mse: 0.0068 |\n",
    "256, None | tanh | 1000 | 16 |  mae (loss): 0.0786, mse: 0.0110 |\n",
    "64, 32 | tanh | 1000 | 16 |  mae (loss): 0.0570, mse: 0.0059 |\n",
    "128, 64 | tanh | 1000 | 16 |  mae (loss): 0.0505, mse: 0.0046 |\n",
    "64, None | relu | 100 | 32 |  mae (loss): 0.0637, mse: 0.0077 |\n",
    "128, None | relu | 100 | 32 |  mae (loss): 0.0577, mse: 0.0063 |\n",
    "256, None | relu | 100 | 32 |  mae (loss): 0.0544, mse: 0.0054 |\n",
    "64, 32 | relu | 100 | 32 |  mae (loss): 0.0469, mse: 0.0042 |\n",
    "128, 64 | relu | 100 | 32 |  mae (loss): 0.0416, mse: 0.0033 |\n",
    "64, None | selu | 100 | 32 |  mae (loss): 0.0629, mse: 0.0074 |\n",
    "128, None | selu | 100 | 32 |  mae (loss): 0.0572, mse: 0.0060 |\n",
    "256, None | selu | 100 | 32 |  mae (loss): 0.0693, mse: 0.0082 |\n",
    "64, 32 | selu | 100 | 32 |  mae (loss): 0.0504, mse: 0.0048 |\n",
    "128, 64 | selu | 100 | 32 |  mae (loss): 0.0498, mse: 0.0043 |\n",
    "64, None | tanh | 100 | 32 |  mae (loss): 0.0612, mse: 0.0067 |\n",
    "128, None | tanh | 100 | 32 |  mae (loss): 0.0664, mse: 0.0079 |\n",
    "256, None | tanh | 100 | 32 |  mae (loss): 0.0632, mse: 0.0070 |\n",
    "64, 32 | tanh | 100 | 32 |  mae (loss): 0.0582, mse: 0.0060 |\n",
    "128, 64 | tanh | 100 | 32 |  mae (loss): 0.0483, mse: 0.0043 |\n",
    "64, None | relu | 500 | 32 |  mae (loss): 0.0524, mse: 0.0052 |\n",
    "128, None | relu | 500 | 32 |  mae (loss): 0.0466, mse: 0.0041 |\n",
    "256, None | relu | 500 | 32 |  mae (loss): 0.0446, mse: 0.0035 |\n",
    "64, 32 | relu | 500 | 32 |  mae (loss): 0.0430, mse: 0.0037 |\n",
    "128, 64 | relu | 500 | 32 |  mae (loss): 0.0409, mse: 0.0032 |\n",
    "64, None | selu | 500 | 32 |  mae (loss): 0.0575, mse: 0.0064 |\n",
    "128, None | selu | 500 | 32 |  mae (loss): 0.0599, mse: 0.0063 |\n",
    "256, None | selu | 500 | 32 |  mae (loss): 0.0626, mse: 0.0067 |\n",
    "64, 32 | selu | 500 | 32 |  mae (loss): 0.0492, mse: 0.0046 |\n",
    "128, 64 | selu | 500 | 32 |  mae (loss): 0.0540, mse: 0.0052 |\n",
    "64, None | tanh | 500 | 32 |  mae (loss): 0.0580, mse: 0.0062 |\n",
    "128, None | tanh | 500 | 32 |  mae (loss): 0.0700, mse: 0.0083 |\n",
    "256, None | tanh | 500 | 32 |  mae (loss): 0.0798, mse: 0.0102 |\n",
    "64, 32 | tanh | 500 | 32 |  mae (loss): 0.0518, mse: 0.0051 |\n",
    "128, 64 | tanh | 500 | 32 |  mae (loss): 0.0522, mse: 0.0050 |\n",
    "64, None | relu | 1000 | 32 |  mae (loss): 0.0590, mse: 0.0068 |\n",
    "128, None | relu | 1000 | 32 |  mae (loss): 0.0594, mse: 0.0062 |\n",
    "256, None | relu | 1000 | 32 |  mae (loss): 0.0483, mse: 0.0042 |\n",
    "64, 32 | relu | 1000 | 32 |  mae (loss): 0.0435, mse: 0.0036 |\n",
    "128, 64 | relu | 1000 | 32 |  mae (loss): 0.0384, mse: 0.0028 |\n",
    "64, None | selu | 1000 | 32 |  mae (loss): 0.0560, mse: 0.0061 |\n",
    "128, None | selu | 1000 | 32 |  mae (loss): 0.0604, mse: 0.0067 |\n",
    "256, None | selu | 1000 | 32 |  mae (loss): 0.0666, mse: 0.0076 |\n",
    "64, 32 | selu | 1000 | 32 |  mae (loss): 0.0494, mse: 0.0047 |\n",
    "128, 64 | selu | 1000 | 32 |  mae (loss): 0.0465, mse: 0.0041 |\n",
    "64, None | tanh | 1000 | 32 |  mae (loss): 0.0581, mse: 0.0064 |\n",
    "128, None | tanh | 1000 | 32 |  mae (loss): 0.0574, mse: 0.0061 |\n",
    "256, None | tanh | 1000 | 32 |  mae (loss): 0.0782, mse: 0.0102 |\n",
    "64, 32 | tanh | 1000 | 32 |  mae (loss): 0.0495, mse: 0.0046 |\n",
    "128, 64 | tanh | 1000 | 32 |  mae (loss): 0.0542, mse: 0.0052 |\n",
    "64, None | relu | 100 | 64 |  mae (loss): 0.0616, mse: 0.0073 |\n",
    "128, None | relu | 100 | 64 |  mae (loss): 0.0489, mse: 0.0045 |\n",
    "256, None | relu | 100 | 64 |  mae (loss): 0.0469, mse: 0.0040 |\n",
    "64, 32 | relu | 100 | 64 |  mae (loss): 0.0499, mse: 0.0046 |\n",
    "128, 64 | relu | 100 | 64 |  mae (loss): 0.0423, mse: 0.0034 |\n",
    "64, None | selu | 100 | 64 |  mae (loss): 0.0587, mse: 0.0065 |\n",
    "128, None | selu | 100 | 64 |  mae (loss): 0.0634, mse: 0.0074 |\n",
    "256, None | selu | 100 | 64 |  mae (loss): 0.0668, mse: 0.0077 |\n",
    "64, 32 | selu | 100 | 64 |  mae (loss): 0.0555, mse: 0.0059 |\n",
    "128, 64 | selu | 100 | 64 |  mae (loss): 0.0500, mse: 0.0047 |\n",
    "64, None | tanh | 100 | 64 |  mae (loss): 0.0545, mse: 0.0056 |\n",
    "128, None | tanh | 100 | 64 |  mae (loss): 0.0644, mse: 0.0077 |\n",
    "256, None | tanh | 100 | 64 |  mae (loss): 0.0686, mse: 0.0083 |\n",
    "64, 32 | tanh | 100 | 64 |  mae (loss): 0.0502, mse: 0.0049 |\n",
    "128, 64 | tanh | 100 | 64 |  mae (loss): 0.0491, mse: 0.0044 |\n",
    "64, None | relu | 500 | 64 |  mae (loss): 0.0614, mse: 0.0073 |\n",
    "128, None | relu | 500 | 64 |  mae (loss): 0.0527, mse: 0.0054 |\n",
    "256, None | relu | 500 | 64 |  mae (loss): 0.0485, mse: 0.0043 |\n",
    "64, 32 | relu | 500 | 64 |  mae (loss): 0.0458, mse: 0.0040 |\n",
    "128, 64 | relu | 500 | 64 |  mae (loss): 0.0399, mse: 0.0029 |\n",
    "64, None | selu | 500 | 64 |  mae (loss): 0.0565, mse: 0.0062 |\n",
    "128, None | selu | 500 | 64 |  mae (loss): 0.0579, mse: 0.0063 |\n",
    "256, None | selu | 500 | 64 |  mae (loss): 0.0647, mse: 0.0074 |\n",
    "64, 32 | selu | 500 | 64 |  mae (loss): 0.0476, mse: 0.0044 |\n",
    "128, 64 | selu | 500 | 64 |  mae (loss): 0.0510, mse: 0.0046 |\n",
    "64, None | tanh | 500 | 64 |  mae (loss): 0.0588, mse: 0.0066 |\n",
    "128, None | tanh | 500 | 64 |  mae (loss): 0.0729, mse: 0.0090 |\n",
    "256, None | tanh | 500 | 64 |  mae (loss): 0.0717, mse: 0.0089 |\n",
    "64, 32 | tanh | 500 | 64 |  mae (loss): 0.0522, mse: 0.0051 |\n",
    "128, 64 | tanh | 500 | 64 |  mae (loss): 0.0547, mse: 0.0054 |\n",
    "64, None | relu | 1000 | 64 |  mae (loss): 0.0555, mse: 0.0060 |\n",
    "128, None | relu | 1000 | 64 |  mae (loss): 0.0574, mse: 0.0061 |\n",
    "256, None | relu | 1000 | 64 |  mae (loss): 0.0555, mse: 0.0054 |\n",
    "64, 32 | relu | 1000 | 64 |  mae (loss): 0.0486, mse: 0.0045 |\n",
    "128, 64 | relu | 1000 | 64 |  mae (loss): 0.0440, mse: 0.0037 |\n",
    "64, None | selu | 1000 | 64 |  mae (loss): 0.0555, mse: 0.0059 |\n",
    "128, None | selu | 1000 | 64 |  mae (loss): 0.0607, mse: 0.0070 |\n",
    "256, None | selu | 1000 | 64 |  mae (loss): 0.0548, mse: 0.0057 |\n",
    "64, 32 | selu | 1000 | 64 |  mae (loss): 0.0496, mse: 0.0049 |\n",
    "128, 64 | selu | 1000 | 64 |  mae (loss): 0.0469, mse: 0.0042 |\n",
    "64, None | tanh | 1000 | 64 |  mae (loss): 0.0559, mse: 0.0060 |\n",
    "128, None | tanh | 1000 | 64 |  mae (loss): 0.0610, mse: 0.0069 |\n",
    "256, None | tanh | 1000 | 64 |  mae (loss): 0.0679, mse: 0.0082 |\n",
    "64, 32 | tanh | 1000 | 64 |  mae (loss): 0.0539, mse: 0.0055 |\n",
    "128, 64 | tanh | 1000 | 64 |  mae (loss): 0.0520, mse: 0.0047 |\n",
    "64, None | relu | 100 | 128 |  mae (loss): 0.0542, mse: 0.0058 |\n",
    "128, None | relu | 100 | 128 |  mae (loss): 0.0573, mse: 0.0061 |\n",
    "256, None | relu | 100 | 128 |  mae (loss): 0.0560, mse: 0.0057 |\n",
    "64, 32 | relu | 100 | 128 |  mae (loss): 0.0473, mse: 0.0043 |\n",
    "128, 64 | relu | 100 | 128 |  mae (loss): 0.0395, mse: 0.0030 |\n",
    "64, None | selu | 100 | 128 |  mae (loss): 0.0590, mse: 0.0068 |\n",
    "128, None | selu | 100 | 128 |  mae (loss): 0.0740, mse: 0.0095 |\n",
    "256, None | selu | 100 | 128 |  mae (loss): 0.0632, mse: 0.0071 |\n",
    "64, 32 | selu | 100 | 128 |  mae (loss): 0.0488, mse: 0.0048 |\n",
    "128, 64 | selu | 100 | 128 |  mae (loss): 0.0489, mse: 0.0044 |\n",
    "64, None | tanh | 100 | 128 |  mae (loss): 0.0620, mse: 0.0072 |\n",
    "128, None | tanh | 100 | 128 |  mae (loss): 0.0625, mse: 0.0073 |\n",
    "256, None | tanh | 100 | 128 |  mae (loss): 0.0770, mse: 0.0112 |\n",
    "64, 32 | tanh | 100 | 128 |  mae (loss): 0.0557, mse: 0.0057 |\n",
    "128, 64 | tanh | 100 | 128 |  mae (loss): 0.0498, mse: 0.0046 |\n",
    "64, None | relu | 500 | 128 |  mae (loss): 0.0579, mse: 0.0064 |\n",
    "128, None | relu | 500 | 128 |  mae (loss): 0.0583, mse: 0.0065 |\n",
    "256, None | relu | 500 | 128 |  mae (loss): 0.0539, mse: 0.0049 |\n",
    "64, 32 | relu | 500 | 128 |  mae (loss): 0.0458, mse: 0.0040 |\n",
    "128, 64 | relu | 500 | 128 |  mae (loss): 0.0444, mse: 0.0038 |\n",
    "64, None | selu | 500 | 128 |  mae (loss): 0.0598, mse: 0.0068 |\n",
    "128, None | selu | 500 | 128 |  mae (loss): 0.0598, mse: 0.0067 |\n",
    "256, None | selu | 500 | 128 |  mae (loss): 0.0619, mse: 0.0067 |\n",
    "64, 32 | selu | 500 | 128 |  mae (loss): 0.0537, mse: 0.0057 |\n",
    "128, 64 | selu | 500 | 128 |  mae (loss): 0.0495, mse: 0.0046 |\n",
    "64, None | tanh | 500 | 128 |  mae (loss): 0.0594, mse: 0.0067 |\n",
    "128, None | tanh | 500 | 128 |  mae (loss): 0.0579, mse: 0.0064 |\n",
    "256, None | tanh | 500 | 128 |  mae (loss): 0.0759, mse: 0.0095 |\n",
    "64, 32 | tanh | 500 | 128 |  mae (loss): 0.0546, mse: 0.0056 |\n",
    "128, 64 | tanh | 500 | 128 |  mae (loss): 0.0482, mse: 0.0044 |\n",
    "64, None | relu | 1000 | 128 |  mae (loss): 0.0545, mse: 0.0060 |\n",
    "128, None | relu | 1000 | 128 |  mae (loss): 0.0562, mse: 0.0060 |\n",
    "256, None | relu | 1000 | 128 |  mae (loss): 0.0492, mse: 0.0043 |\n",
    "64, 32 | relu | 1000 | 128 |  mae (loss): 0.0432, mse: 0.0036 |\n",
    "128, 64 | relu | 1000 | 128 |  mae (loss): 0.0396, mse: 0.0030 |\n",
    "64, None | selu | 1000 | 128 |  mae (loss): 0.0626, mse: 0.0076 |\n",
    "128, None | selu | 1000 | 128 |  mae (loss): 0.0557, mse: 0.0060 |\n",
    "256, None | selu | 1000 | 128 |  mae (loss): 0.0705, mse: 0.0087 |\n",
    "64, 32 | selu | 1000 | 128 |  mae (loss): 0.0500, mse: 0.0049 |\n",
    "128, 64 | selu | 1000 | 128 |  mae (loss): 0.0484, mse: 0.0045 |\n",
    "64, None | tanh | 1000 | 128 |  mae (loss): 0.0637, mse: 0.0076 |\n",
    "128, None | tanh | 1000 | 128 |  mae (loss): 0.0646, mse: 0.0074 |\n",
    "256, None | tanh | 1000 | 128 |  mae (loss): 0.0693, mse: 0.0085 |\n",
    "64, 32 | tanh | 1000 | 128 |  mae (loss): 0.0495, mse: 0.0047 |\n",
    "128, 64 | tanh | 1000 | 128 |  mae (loss): 0.0524, mse: 0.0050 |\n",
    "\n",
    "\n",
    "Best result: neurons=128, 64, activation=relu, batch_size=32, epochs=1000, MAE=0.0384, MSE=0.0028"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
